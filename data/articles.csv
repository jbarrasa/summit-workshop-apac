url,title,date,author,body
https://dev.to/lirantal/securing-a-nodejs--rethinkdb--tls-setup-on-docker-containers,Securing a Node.js + RethinkDB + TLS setup on Docker containers,2017-08-21T18:41:06Z,Liran Tal,"We use RethinkDB at work across different projects. It isn’t used for any sort of big-data applications, but rather as a NoSQL database, which spices things up with real-time updates, and relational tables support.RethinkDB features an officially supported Node.js driver, as well as a community-maintained driver as well called rethinkdbdash which is promises-based, and provides connection pooling. There is also a database migration tool called rethinkdb-migrate that aids in managing database changes such as schema changes, database seeding, tear up and tear down capabilities.We’re going to use the official RethinkDB docker image from the docker hub and make use of docker-compose.yml to spin it up (later on you can add additional services to this setup).A fair example for docker-compose.yml:The compose file mounts a local tls directory as a mapped volume inside the container. The tls/ directory will contain our cert files, and the compose file is reflecting this.To setup a secure connection we need to facilitate it using certificates so an initial technical step:Important notes:Update the compose file to include a command configuration that starts the RethinkDB process with all the required SSL configurationImportant notes:You’ll notice there isn’t any cluster related configuration but you can add them as well if you need to so they can join the SSL connection: — cluster-tls — cluster-tls-key /tls/key.pem — cluster-tls-cert /tls/cert.pem — cluster-tls-ca /tls/ca.pemThe RethinkDB drivers support an ssl optional object which either sets the certificate using the ca property, or sets the rejectUnauthorized property to accept or reject self-signed certificates when connecting. A snippet for the ssl configuration to pass to the driver:Now that the connection is secured, it only makes sense to connect using a user/password which are not the default.To set it up, update the compose file to also include the — initial-password argument so you can set the default admin user’s password. For example:Of course you need to append this argument to the rest of the command line options in the above compose file.Now, update the Node.js driver settings to use a user and password to connect:Congratulations! You’re now eligible to “Ready for Production stickers.Don’t worry, I already mailed them to your address."
https://dev.to/setevoy/neo4j-running-in-kubernetes-e4p,Neo4j: running in Kubernetes,2020-08-05T14:37:20Z,Arseny Zinchenko,"In the previous post — Neo4j: graph database — run with Docker and Cypher QL examples — we’ve run the Neo4j database with в Docker.The next task is to run it in the Kubernetes cluster.Will use the Neo4j Community Edition, which will be running as a single-node instance as cluster ability for the Neo4j is available in the Enterprise version which is costs about 190.000 USD/year.For the Community Edition, we can apply a Helm chart — neo4j-community.Add its repository:Deploy to a custom namespace, in this example, it’s eks-dev-1-neo4j.With the --set accept its license, and add values for the StorageClass of its PersistentVolume, and add a password for the administrator access:Okay -  it’s deployed, check pods:Check its PersistentVolumeClaim:Check the corresponding PersistentVolume — pay attention, that we’ve created that PVC in the dynamic way, so it has the Reclaim policy set to Delete:Now, run port-forwarding to access the Neo4j server from your workstation:Check connection:The server is ready for work.Originally published at RTFM: Linux, DevOps и системное администрирование."
https://dev.to/divyanshutomar/introduction-to-redis-3m2a,Introduction to Redis,2018-07-08T17:27:05Z,Divyanshu Tomar,"For a high traffic web service, it becomes a necessity for it to leverage some kind of caching mechanism. Caching is a way of storing computed data in memory so that future requests can be fulfilled right away. It also helps in avoiding any round trips to the data layer and computations on the application side if implemented with the right strategy. Redis and Memcached are two most popular memory-based stores available. In this post, we will explore some key concepts of Redis and go through some basic commands. Besides caching, Redis can be also used for other applications where there is a need for fast and frequent access to data.Redis is an in-memory data structure store supporting many data types like strings, hashes, sets, sorted sets, etc. Essentially, it is a key-value store.Every type of value in Redis is stored against a key that is binary safe and it can be anything from an empty string to long hash string. Every application should follow a predetermined schema for naming Redis keys to avoid any naming conflicts.Like every database, Redis contains a server for storing data in memory and clients which will execute commands against a server. For setting up the server on your local machine, I will recommend using Docker as it is easy to get started. If you have Docker daemon running on your machine, run the following command:This will run a Docker container with name local-redis on your localhost with port 6379. It uses the official Redis docker image to run the container.For the client, we can use the redis-cli for executing commands from a console on the Redis server. Open a new tab, and execute the following command to start a redis-cli session connected to local docker Redis server instance:Now we can start executing some basic Redis commands.Setting a value:Syntax: SET <key> <value> Example: SET firstname AlbertRetrieve a value:Syntax: GET <key> Example: GET firstnameCheck whether a key exists:Syntax: EXISTS <key>Deleting a key:A key can be removed along with its associated memory using: DEL <key> This is a synchronous blocking operation.A better way to remove keys will be to unlink them whose associated memory can be collected by a garbage collector later on. UNLINK <key>Setting a time to expire for key:EXPIRE <key> <seconds> PEXPIRE <key> <milliseconds>Setting a key with check for existence and expiry in one go:Syntax: SET <key> <value> <EX seconds>|<PX milliseconds> NX|XXNX - set only when a key does not exist. XX - set only when key already exists. EX - sets expire time for the key in seconds. PX - sets expire time for the key in milliseconds.Example:SET firstname Albert EX 10 NXThis will set the key firstname with string value ""Albert"" with an expiry time of 10 seconds only if the key does not exist.Increment or Decrement an integer value:Redis provides a convenient way to increment or decrement integer values that may be used as counters.Syntax: INCR <key> DECR <key> INCRBY <key> <increment value> DECRBY <key> <decrement value>Example: SET counter 4 INCRBY counter 6counter key will hold the value 4 initially, and after the second command, it will get incremented to 10.All the above mentioned commands just deal with storing and manipulating of string or integer values. There are other data structure values such as hashes, sets, bit arrays, etc. that can be used to solve complex problems.In a real-world application, you can use various programming language specific redis clients available for interacting with your Redis server from the application code.We will be writing a simple Node based application that exposes an endpoint for getting user info against an userid. A JSON file will act as our datastore to keep things as simple as possible.Now, make a redis helper file that forms an instance of the redis client connected to our Redis server. We are also writing some cache helper methods here for our route handlers.In main app file, write a route handler that accepts an userid against which the user info is to be retrieved. Next, form a unique redis key using the userid. This key will always be the same for every request for a given userid. Check for existence of this key in the Redis cache, and return the response if found.Else, we will query the data from our data source and set the response data to Redis cache before sending it back as a response.To have a look at the full code and tinker around with it, you can clone the following repository:An express application that demonstrates how redis can be utilized for caching data so that recurrent requests can be fulfilled right away.Congratulations! You have now learned the basics of Redis. If you'd like to take a deep dive, please have a look at the official redis docs.Thank you for following along and I hope this post would have been useful for you. Do follow me on Twitter to receive updates on such topics."
https://dev.to/zaiste/15-git-commands-you-may-not-know-4a8j,15 Git Commands You May Not Know,2019-03-31T19:44:16Z,Zaiste,"Using Git may be intimidating at times. There are so many commands and details to learn. The documentation, however, while being immense, is still greatly accessible. Once you overcome the initial feeling of being overwhelmed, the things will start to fall into place.Here is a list of 15 Git commands that you may not know yet, but hopefully they will help you out on a journey to master this tool.—-amend allows to append staged changes (e.g. to add a forgotten file) to the previous commit. Adding —-no-edit on top of that will amend the last commit without changing its commit message. If there are no changes, -—amend will allow you to reword the last commit message.For more: git help commit-p (or —patch) allows to interactively select parts of each tracked file to commit. This way each commit contains only related changes.For more: git help addSimilar to git-add , you can use --patch option to interactively select parts of each tracked file to stash.For more: git help stashBy default, when stashing, the untracked files are not included. In order to change that bevahiour and include those files as well you need to use -u parameter. There is also -a (—all) which stashes both untracked and ignored files altogether, which is probably something you usually won’t need.--patch can be also used to selectively discard parts of each tracked file. I aliased this command as git discardFor more: git help checkoutThis command allows you to quickly switch to the previously checked out branch. On a general note - is an alias for the previous branch. It can be used with other commands as well. I aliased checkout to co so, it becomes just git co -If you are sure that all of your local changes can be discarded, you can use . to do it at once. It is, however, a good practice to always use checkout --patch.This command shows all staged changes (those added to the index) in contrast to just git diff which only shows changes in the working directory (without those in the index).For more: git help diffIf you want to rename the currently checked out branch, you can shorten this command to the following form:For more: git help branchIn order to rename a branch remotely, once you renamed your branch locally, you need to first remove that branch remotely and then push the renamed branch again.Rebasing may lead to conflicts, the following command will open all files which need your help to resolve these conflicts.This command will show a log with changes introduced by each commit from the last two weeks.Let's say you committed a file by mistake. You can quickly remove that file from the last commit by combining rm and commit --amend commands:This command will show all branches that contain a particular commit.For more: git help gcAlthough I like CLI a lot, I highly recommend checking Magit to further step up your Git game. It is one of best pieces of software I used.There is, also, a fantastic overview of recommended Git workflows available via help command. Be sure to read it thoroughly!"
https://dev.to/alexjitbit/removing-files-from-mercurial-history-1b15,Removing files from Mercurial history,2019-04-20T16:30:22Z,Alex Yumashev,"Sometimes you need to remove files from Mercurial completely, even from the history. For instance, if you have accidentally stored a sensitive file in the repo (some password, or an access key, or a code-signing certificate etc. etc.) or committed a huge binary. Here's how you remove it:You will need the hg convert command. This command is normally used to convert an SVN/GIT repository to Mercurial, but it can also be used to ""convert"" from Mercurial to Mercurial too.The cool thing about it is the --filemap option that specifies which files should be included in the conversion process and which should not.Add this code to .hgrc to enable the extension:Make sure all your teammates have pushed their local changes to the central repo (if any)Backup your repositoryCreate a ""map.txt"" file:Then run this command:NOTE: You have to use ""forward-slash"" in paths, even on Windows.Then wait and be patient. After a while you will have a new repo at c:\newrepo just without the filesIf you have some central hg-storage you will have to ""strip"" all your changesets and then ""push"" your local repostiroy again.For instance, if you use Bitbucket like us, go to ""Admin - Strip"". Enter ""0"" into the ""Revision"" box and this will remove all commits, but the ""wiki"" and ""issues"" areas will be preserved. Then - push the local repo back to Bitbucket again.There's an important catch though - because the above procedure affects history and re-assigns new ""ids"" to all your changesets, your teammates have to reclone the repo from the central repo again."
https://dev.to/michelemauro/atlassian-sunsetting-mercurial-support-in-bitbucket-2ga9,Atlassian sunsetting mercurial support in BitBucket,2019-08-23T20:51:49Z,michelemauro,"(I didn't expect to start writing here with a text like this. But since I'm looking for longer opinions that those that can be found on twitter, I thought it would be a good place to start, and a good topic to talk about.)The headline came to me via whatsapp from an ex-colleague: Atlassian sunsetting mercurial support in BitBucket. That was really a hard hit.For those who don't know, Mercurial was one of the first really distributed Distributed Version Control System that really took traction: it was started in 2005, and it is still heavily used: Mozilla, Vim and OpenJDK are some high-profile projects that host multiple years of development, and millions of lines of code in hg repositories.I started using Mercurial before git ever became mainstream, and because I was working with some colleagues outside the main office: we needed to collaborate on the same sources and, in that setting, we couldn't reach our central CVS. The ease of use of passing around ""boundles"" of commits, the simple commands and the almost impossibility of losing history made it possible to work for many months without even a central server, just passing files via skype.It was no match for what Git was at that time (early 2007): something Linus whipped up in a couple of weeks, not too stable, still optimized for the Linux use case and adventurous to use outside of it. Mercurial was already stable, well supported on windows, too, with a clear and well-thought model that was easy to understand and use.Twelve years later, git is still making releases with new commands to clarify and simplify some use cases, while Mercurial releases are about technology updates and new features. You can easily find (and probably need) tons of tutorials on how to use git and how to understand its peculiar concepts (like of course the excellent one here on DEV); meanwhile, every colleague I exposed to mercurial never needed any special training nor never lost work because of a detached head or a push in the wrong branch, no matter its previous level of expertise.Finally, in the last three-four years, the ""GitOps"" methods fueled the exponential growth of Git integrations, and we're now stuck in VHS land. Atlassian, of course, has a bottom line to keep in check and finally came to the conclusion that BitBucket is supporting one VCS too many, and the one used by ""1% of the users"" has to go. The irony of BitBucket being the first commercial solution for Mercurial is, of course, only adding sadness to this story.The options that Atlassian proposes are really underwhelming: a yet-to-be automated conversion service, some self-hosted pointers (Heptapod and Kallithea seem to be the most interesting; but they are a far cry from a supported commercial service with more than 10 years of experience), or do-it-yourself. And don't expect to keep PRs, comments, issues, metadata: no export path for them, nor conversion of project structure. When Atlassian pulls the plug in mid-2020, they'll be gone.That's a sad, sad turn of the story that will harm a really good project, a true Betamax of the VCS space.I wish however to thank a lot the original BitBucket team, that in 2008 believed in Mercurial and started its journey. Atlassian, while can be understood as a business entity, is still struggling in executing major moves like these while minimizing bad publicity and juggling worse karma; meme writers will just have more things to poke fun at.Of course, the main lesson here is, for the n-th time, be careful what you put on a cloud service. And you, what do you have on a cloud service? what part of the information about your projects (issues, discussions, decisions, permissions, people) will you be able to offload, reuse and recycle if your current provider pulls the plug on its service or part of it?"
https://dev.to/shirou/back-up-prometheus-records-to-s3-via-kinesis-firehose-54l4,Back up prometheus records to s3 via kinesis firehose,2017-12-23T08:07:19Z,WAKAYAMA shirou,"Because prometheus has not for long term storage, it will be erased at appropriate intervals. For long term data store, it would be preserved as being influxdb. However, AWS S3 is more easy place to manage.AWS Kinesis firehose has came to the Tokyo region in July 2017. By using kinesis firehose, we can automatically save records in S3.So I implemented a remote write adapter integration of prometheus which can sends records to Kinesis.https://github.com/shirou/prometheus_remote_kinesisThis is just for evaluation and not deployed to production, there may be some problems.It is necessary to build with go, but I omit it. It is easy to use multi stage build with Docker.Of course, you should set AWS credentials.I also put it in the docker hub.https://hub.docker.com/r/shirou/prometheus_remote_kinesis/It should start with such feeling.Set the remote write setting of prometheus.yml as follows. It is important to add a - before the url to make it a sequence.The settings of kinesis and kinesis forehose are omitted.The setting is over with the above. As time goes on the logs are generated more and more in s3.The data sent to kinesis was made into JSON format like this.In Timeseries of prometheus, multiple samples can be stored for one records. However, since I do not want to increase the hierarchy much, I flatten it so that I create one record for each sample. As it was impossible to truly label, it is on the map. In addition, it assumes use from Athena or S3 SELECT, and it makes it with new line (JSON-LD). I tried to send it to kinesis with gzip compressed, but I removed currently because my t2.small uses a CPU too much. In addition, it sends it by PutRecords by every 500 records. As it will be buffered, it may be lost if remoe_kinesis die. Graceful shutdown is implemented, though.Since the write request from prometheus comes with snappy compression + protobuf, it may be the fastest way to transfer to kinesis with its byte sequence as it is, but this will be difficult to handle it later .I created remote storage integration to save to s3 via AWS Kinesis for long term log preset of prometheus.I have not deployed to production environment, so there may be problems. Another problem is reading from S3. But it is just a plain JSON format, I think it is easy to convert if necessary. Although it is possible to read directory from promehteus with remote read storage integration, but it probably not good performance.Oh, AlpacaJapan is recruiting highly acclaimed people who will do this around. Please contact Twitter @r_rudi"
https://dev.to/ionic/farewell-phonegap-reflections-on-my-hybrid-app-development-journey-10dh,"Farewell, PhoneGap: Reflections on my Hybrid App Development Journey",2020-08-13T12:13:56Z,Ionic,"Adobe has officially announced the shutdown of PhoneGap and PhoneGap Build.In the context of the hybrid app development world, this is the end of an era. It's certainly the case for me: it sped up my transition from .NET to web development, and ultimately led to me landing a wonderful role at Ionic. A heartfelt thanks to the team(s) at Adobe and those in the community who supported me along the way!PhoneGap has had such a positive impact on my career. Here's my hybrid app development journey.It was 2010, and I had just bought my first smartphone - a clunky Android device. I was .NET developer building tools and WinForm desktop apps for a SaaS company. That was fun, but my work was only used by a handful of corporate clients. This Android phone had potential - I could build an ""app"" and reach anyone in the world via the app marketplaces? Sign me up!I learned Java in college, so Android development was the obvious choice to learn. I bought a beginner Android book, ""Hello, Android"", and got to work. The dev experience was brutal to say the least. Between confusing Eclipse IDE errors and trying to understand the ins and outs of mobile development, I almost gave up multiple times.I pushed through, and in February 2011 released the app. Frustrated that Netflix movies would expire and be removed from my instant queue without notice, I explored my options. I discovered that Netflix had an open API, and while not used on the site, every movie was assigned a ""movie availability"" (expiration) date!FixMyQ was born: it displayed each movie in your Instant Queue along with its expiration date. Optionally, with one button tap, you could rearrange your entire queue by the movies expiring next. In practice, after deciding to watch something on Netflix, you could pull up my app first then pick based on what was expiring soonest:Despite being super ugly (ha), the app worked quite well and was decently popular.The app was doing well, but I was missing out on a huge audience: iOS users. I quickly realized that targeting iOS meant that I had to completely rewrite the app - yikes! Fortunately, there was another way: PhoneGap.Through my day job work and attending developer conferences, I noticed this thing called ""JavaScript"" was skyrocketing in popularity. I began to actively seek out opportunities at work to use it - landing on ASP.NET MVC, jQuery, and Knockout.js. I don't remember exactly how I found PhoneGap, but I loved the idea of ""write once, run everywhere"": targeting web, iOS, and Android with one code base.Additionally, their beliefs, goals, and philosophy really struck a chord. The team recognized that the web was not a first class development platform, but they fully believed it could be, laying out a strong vision for its future.What really stood out at the time (and still does) was this line:""The ultimate purpose of PhoneGap is to cease to exist.""To this day, I've yet to see another project put that front and center! It made sense, though: they were committed to the ""standardization of the web as a platform.""I was convinced and started building FixMyQ for iOS using PhoneGap 1.2.0. Unfortunately, I didn't get very far: Netflix deprecated then eventually shut down their open API - effectively killing the app. It was a great first mobile app project though and made for a fun retrospective.(Not so) hilariously, in 2020 Netflix still acts like movies don't expire. C'mon, Netflix!Despite shutting down my first app, I was excited by PhoneGap's potential and got to work right away on a new app idea. Work had just bought everyone a Fitbit device. I was also in the WeightWatchers program, so I wondered what it would take to integrate them together. A few months later, Fitwatchr was born and thanks to PhoneGap, I created iOS, Android, and Windows Phone apps all from one code base:Besides improving my web development skills, Fitwatchr was my first foray into becoming somewhat of an entrepreneur: in order to improve app sales, I learned so much about marketing, sales, and product development, ultimately earning thousands of dollars over a ~5 year timespan. As the app started making waves, I partnered with my good friend David Lapekas for design and marketing help - he was absolutely critical to my apps' success.You might say I was ""hooked on hybrid!""My next app scratched another itch. I love craft beer and had gotten really into tracking beer tastings with Untappd (another PhoneGap/Cordova - and later, Ionic Framework - app!). Their app was great, but didn't work well in offline scenarios (such as beer festivals or inside crowded brewery tasting rooms) where cell service was weak or wifi non-existent. With BeerSwift, you can queue up the beers you're drinking, rate them, then check them all into Untappd with one button tap (once you're back online):These apps were so much fun to build. I worked on them during the days of Angular 1, but was honestly scared off by how complex it seemed. So instead, I opted for a simpler stack: Vanilla HTML/CSS/JavaScript paired with jQuery, KendoUI Mobile for UI components, and Knockout.js for declarative bindings and automatic UI refresh.As you can tell from those screenshots, the apps look much better than my original Android app, but the UI still has some rough edges. Someday I'll rewrite them using Ionic Framework UI components...While PhoneGap makes it easy to create an app that runs on all platforms, in practice managing each platform is challenging, especially as a solo developer. There are nuances to each one as well as headaches with security profiles and certificates (cough cough iOS!). Enter Adobe's PhoneGap Build service, which let you build your PhoneGap-based Android, iOS, and Windows Phone apps in the cloud. It was incredibly successful as one of the early attempts at Mobile CI/CD since you could avoid wrestling with the challenges of native IDEs and tooling. Everyone in the PhoneGap community embraced it: solo devs, teams, and large companies.After gaining lots of experience with the service, I began sharing various tips and tricks on my personal blog. I'm particularly proud of the ""Cut Your PhoneGap Build App Size In Half With This One Weird Trick!"" post - one of my first attempts at ""marketing."" 😂It was rewarding to share what I'd learned with the community. I kept plugging away on both app development and blogging. From there, I decided to give public speaking a try, presenting a talk on hybrid app development at That Conference 2014.By 2015, hybrid app development had become far less niche and I had built up a lot of experience with several successful apps under my belt. I looked for my next challenge and settled on creating a video course on PhoneGap Build. With only a small blog audience, I turned to Pluralsight. I was a long-time fan - they are known for their high quality courses and popular authors. After a brief audition, I was in! You can read about that 2 year journey (yeah) here. It was incredibly challenging with lots of ups and downs, but in the end, I pulled it off.So unbelievably stoked! A long journey comes to an end. My @pluralsight course has been released! Cheers to a wonderful editorial team and @nursingaround 's support. #PhoneGap #Cordova #MobileDevA post shared by Matt Netkow (@dotnetkow ) on Jul 11, 2017 at 8:53pm PDTThe Pluralsight course was not a major hit by any means, but it was definitely a personal success: I learned basic video editing and production, and improved my writing and speaking skills along the way - all skills I would eventually use regularly in my DevRel role at Ionic.At some point during the development of my PhoneGap apps, I became frustrated trying to create the variety of icons and splash screens. Besides the act of creating them (I'm certainly no designer!), generating them for each platform and dimension was tedious. I'm not entirely sure, but I believe this was the first time I learned about Ionic: I stumbled upon a blog post of theirs on automating icon/splash screen generation.I created an Ionic account just to generate those images for free with the Ionic CLI (they were originally built in the cloud). Thanks, Ionic! 😬Little did I know where I'd end up someday...As part of the efforts to promote my PhoneGap Build Pluralsight course, I reached out to the PhoneGap team and asked about writing a post for the official blog. They graciously accepted, no doubt largely due to my course and personal PhoneGap blog posts, so I wrote ""Hybrid Mobile Apps are Overtaking Native."" This was a fun one: I covered the most popular concerns about hybrid app development from a fresh 2017 perspective: performance, design, frameworks, and tooling.By then, I was a regular reader of the Ionic blog and used (borrowed?) an image of theirs for the post (Thanks again, Ionic!). It was well received and led to a bunch of native developers leaving many ""spirited"" comments. Ha!Later, after the iPhone X was released, I struggled to update my PhoneGap apps to support the infamous ""notch."" I eventually figured out a general solution then wrote another guest post for the PhoneGap blog. To date, ""Displaying a PhoneGap App Correctly on the iPhone X"" is my highest viewed piece of writing ever with over 223,000 views (the notch still confuses developers to this day!).My final post for the PhoneGap blog, ""PhoneGap Devs: It’s Time to Embrace a UI Framework"" was a clear call to action to the community: pick a UI framework so that you can focus on building your app instead of dealing with various mobile issues (like the iPhone notch!). By that time, I was working for Ionic so naturally the article focused on the Ionic Framework.Huge thanks to the PhoneGap team for allowing me to guest on the blog!By the time 2018 rolled around, I was even deeper into web development, working regularly with Angular 2 and .NET Core at my day job. Angular, while initially challenging to learn, was a breathe of fresh air compared to my now-aging ""PhoneGap stack.""One night, I saw a tweet from the Ionic team:Calling all product champions! Do you have a technical background, but enjoy being the face and voice of the product, rather than sitting behind the scenes? Ionic is looking for a Senior Product Evangelist to join the team! Position can be remote! https://t.co/lHQo6OHrcJThe timing was simply incredible: at that moment I was taking a break from packing up my apartment. I planned to move back to Madison, Wisconsin that Summer, where Ionic headquarters is located. Leveraging my PhoneGap guest blog posts, Pluralsight course, and hybrid app experience, I landed the role as a Product Evangelist/Developer Advocate. See the full story here.When I started building PhoneGap apps, I had no idea what it would lead to. Hybrid app development has been such an incredibly rewarding career path. After years of hard work, some luck along the way, and support from an amazing community, I'm grateful to work on hybrid app development fulltime now, and for such an awesome company as Ionic.So as you can see, PhoneGap changed my life for the better. I owe a lot of my career to this amazing technology and the people that built it. But enough about me. 😀Did PhoneGap succeed in its mission to make the web platform a first-class citizen?""We believe in a web open to everyone to participate however they will. No locked doors. No walls.""Broadly speaking, PhoneGap absolutely succeeded: as pioneers of hybrid app development, they ""solved"" cross-platform app development challenge while also being incredibly influential in making the web a first-class development platform.In the time since it was created - over 12 years ago - we've seen the web platform explode in popularity: the vast majority of developers are web developers and many folks new to software development learn web dev first since it's so accessible and open.Sure, the web platform isn't perfect, but it's come a long way and will continue to evolve. It has matured a lot over the past few years, from modern JavaScript (ES6 and beyond) to package managers like npm, to built-in cross-platform browser APIs that provide rich user experiences, to the rise of Progressive Web Apps (PWAs) that fill the ""gap"" in ""PhoneGap.""Now, all of us at Ionic are ready to carry the torch as the modern leader of cross-platform hybrid app development. Our native runtime tool Capacitor, as a spiritual successor to PhoneGap, offers a modern, web-first approach to hybrid and is backward compatible with PhoneGap.Thank you to Adobe and the PhoneGap team for their hard work over the years and helping so many developers embrace web development. Long live the web platform!"
https://dev.to/rootsami/rancher-kubernetes-on-openstack-using-terraform-1ild,Rancher Kubernetes on Openstack using Terraform,2020-05-27T03:33:00Z,Sami Alhaddad,"In this article we will walk through creating complete infrastructure pieces on OpenStack that are needed to have a fully provisioned Kubernetes cluster using Terraform and Rancher2. In addition to integration with cloud-provider-openstack and cinder-csi-pluginRKE configuration can be adjusted and customized in rancher2.tf, you can check the provider documentation at rancher_cluster NOTE: It is really important to keep kubelet extra_args for the external cloudprovider in order to integrate with cloud-provider-openstackRun terraform init to initialize a working directory containing Terraform configuration files.To apply the creation of the environment, Run terraform apply --auto-approve and wait for the output after all resources finish the creationUp to this point, use the rancher_url from above output and login to rancher instance with username admin and password defined in rancher_admin_password. Wait for all kubernetes nodes to be discovered, registered, and active.As you may notice, that all the nodes have a taint node.cloudprovider.kubernetes.io/uninitialized. The usage of --cloud-provider=external flag to the kubelet makes it waiting for the clouder-provider to start the initialization. This marks the node as needing a second initialization from an external controller before it can be scheduled work.Up to this point, openstack-cloud-controller-manager and cinder-csi-plugin have been deployed, and they're able to obtain valuable information such as External IP addresses and Zone info.Also, as shown in the nodes tab, All nodes are active and labeled by openstack zones.When it comes to scalability with IaaC (infrastructure-as-a-code), it becomes so easy to obtain any desired state in less consumed efforts and time. All you have to do is to change the number of nodes count_master or count_worker_nodes and run terraform apply again For example, let's increase the number of count_worker_nodes by 1 A few minutes later, after refreshing states and applying updates:Couple of minutes for the new node to be registeredNOTE: Scaling down the cluster could be made by decreasing the number of nodes in terrafrom.tfvars. Node gets deleted, moreover cloud-provider-openstack detects that and removes it from the clusterTo clean up all resources created by this terraform, Just run terraform destroyTerraform manifests to create e2e production-grade Kubernetes cluster on top of cloud providersThis repo is intended to be for creating complete infrastructure pieces on OpenStack that are needed to have a fully provisioned Kubernetes cluster using Terraform and Rancher2. In addition to integration with cloud-provider-openstack"
https://dev.to/jignesh_simform/comparing-mongodb--mysql-bfa,Comparing MongoDB & MySQL,2017-12-05T12:19:07Z,Jignesh Solanki,"Imagine finding a DBMS that aligns with tech goals of your organization. Pretty exciting, right?Relational databases held the lead for quite a time. Choices were quite obvious: MySQL, Oracle or MS SQL, to mention a few. Though times have changed pretty much with the demand for more diversity and scalability, haven't they?There are many alternatives in the market to choose from, though I don’t want you to get all confused again. So how about a faceoff between two dominant solutions that are close in popularity?MongoDB vs MySQL?Both of these are some of the most popular open-source database software.On that note, let’s get started.One of the best things about MongoDB is that there are no restrictions on schema design. You can just drop a couple of documents within a collection and it isn’t necessary to have any relations between those documents. The only restriction with this is supported data structures.But due to the absence of joins and transactions (which we will discuss later), you need to frequently optimize your schema based on how the application will be accessing the data.Before you can store anything in MySQL, you need to clearly define tables and columns, and every row in the table should have the same column.And because of this, there isn’t much space for flexibility in the manner of storing data if you follow normalization.For example, if you run a bank, its information can be added to the table named ‘account’ as follows:This is how MySQL stores the data. As you can see, the table design is quite rigid and it is not easily changeable. MongoDB stores the data in the JSON type manner as described below:Such documents can be stored in a collection as well.MongoDB creates schemaless documents which can store any information you want though it may cause problems with data consistency. MySQL creates a strict schema-template and hence it is bound to make mistakes.MongoDB uses an unstructured query language. To build a query in JSON documents, you need to specify a document with properties you wish the results to match.It is typically executed using a very rich set of operators that are linked to each other using JSON. MongoDB treats each property as having an implicit boolean AND. It natively supports boolean OR queries, but you must use a special operator ($or) to achieve it.MySQL uses the structured query language SQL to communicate with the database. Despite its simplicity, it is indeed a very powerful language which consists mainly of two parts: data definition language (DDL) and data manipulation language (DML).Let’s have a quick comparison.MongoDB doesn’t support JOIN — at least, it has no equivalent. On the contrary, it supports multi-dimensional data types such as arrays and even other documents. The placement of one document inside another is known as embedding.One of the best parts about MySQL is the JOIN operations. To put it in simple terms, JOIN makes the relational database relational. JOIN allows the user to link data from two or more tables in a single query with the help of single SELECT command.For example, we can easily obtain related data in multiple tables using a single SQL statement.This should provide you with an account number, first name, and the respective branch.One single main benefit it has over MySQL is its ability to handle large unstructured data. It is magically faster because it allows users to query in a different manner that is more sensitive to workload.Developers note that MySQL is quite slower in comparison to MongoDB when it comes to dealing with large databases. It is unable to cope with large and unstructured amounts of data.As such, there is no “standard” benchmark that can help you with the best database to use for your needs. Only your demands, your data, and infrastructure can tell you what you need to know.Let’s look at a general example to understand the speed of MySQL and MongoDB in accordance with various functions.Measurements have been performed in the following cases:MySQL 5.7.9MongoDB 3.2.0Each of these has been tested on a separate m4.xlarge Amazon instance with Ubuntu 14.4 x64 and default configurations; all tests were performed for 1,000,000 records.It is evident from the above graph that MongoDB takes way more lesser time than MySQL for same commands.MongoDB uses a role-based access control with a flexible set of privileges. Its security features include authentication, auditing, and authorization.Moreover, it is also possible to use Transport Layer Security (TLS) and Secure Sockets Layer (SSL) for encryption purposes. This ensures that it is only accessible and readable by the intended client.MySQL uses a privilege-based security model. This means it authenticates a user and facilitates it with user privileges on a particular database such as CREATE, SELECT, INSERT, UPDATE, and so on.But it fails to explain why a given user is denied specific access. On the transport layer, it uses encrypted connections between clients and the server using SSL.When to Use MongoDB or MySQL? This infographic explains when you'd use MongoDB over MySQL and vice versa.To answer the question, “Why I should use X over Y?” you need to take into consideration your project goals and many other things.MySQL is highly organized for its flexibility, high performance, reliable data protection, and ease of managing data. Proper data indexing can resolve your issue with performance, facilitate interaction and ensure robustness.But if your data is not structured and complex to handle, or if predefining your schema is not coming easy for you, you should better opt for MongoDB. What’s more, if you're required to handle a large volume of data and store it as documents, MongoDB will help you a lot!The result of the faceoff: One isn’t necessarily better than the other. MongoDB and MySQL both serve in different niches.We have published an updated version of this post here MongoDB vs MySQL: A Comparative Study on Databases. If you've more suggestions up your sleeve, kindly comment."
https://dev.to/nipeshkc7/dynamodb-the-basics-360g,DynamoDB: the basics,2020-06-02T04:09:36Z,Arpan Kc,"Recently I've started looking at Serverless architecture inorder to move away from the monolithic architecture. While exploring AWS free tier, Amazon DynamoDB stood out to me. As a NOSQL database with free 25 GB of storage, I could use it for my personal projects basically for free.So I've been reading up on AWS docs on the basics of DynamoDB, below listed are some of the basic terminology I have gotten to know about DynamoDB:Strange name, given DynamoDB is NoSQL, However, it serves as a good mental model, and helps understand how data is grouped together. These 'tables' consists of a collection of 'items'.Items can be thought of as 'rows' in the tables, But unlike traditional tables, it does not have or need a specific schema. Aside from the primary key, it can contain any number of 'key-value' relations, and can also store nested objects upto 32 levels deep.This is a simple one, Primary key uniquely identifies the 'item' in the table, it can be a single attribute or a combination of attributes.The image above is from AWS docsIn this specific case, 'People' is the table, which consists of a collection of items, And 'PersonID' is the primary key which uniquely identifies the item. Here, besides 'PersonID', other key value attributes can be anything, even nested items.So AWS DynamoDB is an awesome free data storage option, however NoSQL Databases has its own use cases as does Relational Databases. So it is important to figure out what kind of storage option is best for your specific project before going forward.Thanks for reading.P.S. Please follow me on twitter @Nipeshkc7"
https://dev.to/heroku/postgres-is-underrated-it-handles-more-than-you-think-4ff3,Postgres Is Underrated—It Handles More than You Think,2019-10-09T15:04:20Z,Heroku,"Thinking about scaling beyond your Postgres cluster and adding another data store like Redis or Elasticsearch? Before adopting a more complex infrastructure, take a minute and think again. It’s quite possible to get more out of an existing Postgres database. It can scale for heavy loads and offers powerful features which are not obvious at first sight. For example, its possible to enable in-memory caching, text search, specialized indexing, and key-value storage.After reading this article, you may want to list down the features you want from your data store and check if Postgres will be a good fit for them. It’s powerful enough for most applications.As Fred Brooks put it in The Mythical Man-Month: ""The programmer, like the poet, works only slightly removed from pure thought-stuff. [They] build castles in the air, from air, creating by exertion of the imagination.""Adding more pieces to those castles, and getting lost in the design, is endlessly fascinating; however, in the real world, building more castles in the air can get in your way. The same holds true for the latest hype in data stores. There are several advantages to choosing boring technology:Although it can be managed by thoughtful design, adding multiple datastores does increase complexity. Before exploring adding additional datastores, it's worth investigating what additional features your existing datastores can offer you.Many people are unaware that Postgres offers way more than just a SQL database. If you already have Postgres in your stack, why add more pieces when Postgres can do the job?There’s a misconception that Postgres reads and writes from disk on every query, especially when users compare it with purely in-memory data stores like Redis.Actually, Postgres has a beautifully designed caching system with pages, usage counts, and transaction logs. Most of your queries will not need to access the disk, especially if they refer to the same data over and over again, as many queries tend to do.The shared_buffer configuration parameter in the Postgres configuration file determines how much memory it will use for caching data. Typically it should be set to 25% to 40% of the total memory. That’s because Postgres also uses the operating system cache for its operation. With more memory, most recurring queries referring the same data set will not need to access the disk. Here is how you can set this parameter in the Postgres CLI:Managed database services like Heroku offer several plans where RAM (and hence cache) is a major differentiator. The free hobby version does not offer dedicated resources like RAM. Upgrade when you’re ready for production loads so you can make better use of caching.You can also use some of the more advanced caching tools. For example, check the pg_buffercache view to see what’s occupying the shared buffer cache of your instance. Another tool to use is the pg_prewarm function which comes as part of the base installation. This function enables DBAs to load table data into either the operating system cache or the Postgres buffer cache. The process can be manual or automated. If you know the nature of your database queries, this can greatly improve application performance.For the really brave at heart, refer to this article for an in-depth description of Postgres caching.Elasticsearch is excellent, but many use cases can get along just fine with Postgres for text searching. Postgres has a special data type, tsvector, and a set of functions, like to_tsvector and to_tsquery, to search quickly through text. tsvector represents a document optimized for text search by sorting terms and normalizing variants. Here is an example of the to_tsquery function:You can sort your results by relevance depending on how often and which fields your query appeared in the results. For example, you can make the title more relevant than the body. Check the Postgres documentation for details.Postgres provides a powerful server-side function environment in multiple programming languages.Try to pre-process as much data as you can on the Postgres server with server-side functions. That way, you can cut down on the latency that comes from passing too much data back and forth between your application servers and your database. This approach is particularly useful for large aggregations and joins.What’s even better is your development team can use its existing skill set for writing Postgres code. Other than the default PL/pgSQL (Postgres’ native procedural language), Postgres functions and triggers can be written in PL/Python, PL/Perl, PL/V8 (JavaScript extension for Postgres) and PL/R.Here is an example of creating a PL/Python function for checking string lengths:Extensions are to Postgres what plug-ins mean in many applications. Suitable use of Postgres extensions can also mean you don’t have to work with other data stores for extra functionality. There are many extensions available and listed on the main Postgres website.PostGIS is a specialized extension for Postgres used for geospatial data manipulation and running location queries in SQL. It’s widely popular among GIS application developers who use Postgres. A great beginner’s guide to using PostGIS can be found here.The code snippet below shows how we are adding the PostGIS extension to the current database. From the OS, we run these commands to install the package (assuming you are using Ubuntu):After that, log in to your Postgres instance and install the extension:If you want to check what extensions you have in the current database, run this command:The Postgres hstore extension allows storing and searching simple key-value pairs. This tutorial provides an excellent overview of how to work with hstore data type.There are two native data types for storing semi-structured data in Postgres: JSON and XML. The JSON data type can host both native JSON and its binary form (JSONB). The latter can significantly improve query performance when it is searched. As you can see below, it can convert JSON strings to native JSON objects:If you’re considering switching off Postgres due to performance reasons, first see how far you can get with the optimizations it offers. Here we'll assume you've done the basics, like creating appropriate indexes. Postgres offers many advanced features, and while the changes are small they can make a big difference, especially if it keeps you from complicating your infrastructure.Avoid unnecessary indexes. Use multi-column indexes sparingly. Too many indexes take up extra memory that crowd out better uses of the Postgres cache, which is crucial for performance.Using a tool like EXPLAIN ANALYZE might surprise you by how often the query planer actually chooses sequential table scans. Since much of your table’s row data is already cached, oftentimes these elaborate indexes aren’t even used.That said, if you do find slow queries, the first and most obvious solution is to see if the table is missing an index. Indexes are vital, but you have to use them correctly.A partial index can save space by specifying which values get indexed. For example, you want to order by a user’s signup date, but only care about the users who have signed up:Choosing the right index for your data can improve performance. Here are some common index types and when you should use each one.There are legitimate cases for adding another datastore beyond Postgres.Some data stores give you data types that you just can’t get on Postgres. For example, the linked list, bitmaps, and HyperLogLog functions in Redis are not available on Postgres.At a previous startup, we had to implement a frequency cap, which is a counter for unique users on a website based on session data (like cookies). There might be millions or tens of millions of users visiting a website. Frequency capping means you only show each user your ad once per day.Redis has a HyperLogLog data type that is perfect for a frequency cap. It approximates set membership with a very small error rate, in exchange for O(1) time and a very small memory footprint. PFADD adds an element to a HyperLogLog set. It returns 1 if your element is not in the set already, and 0 if it is in the set.If you’re in a situation with many pub-sub events, jobs, and dozens of workers to coordinate, you may need a more specialized solution like Apache Kafka. LinkedIn engineers originally developed Kafka to handle new user events like clicks, invitations, and messages, and allow different workers to handle message passing and jobs to process the data.If you have a real-time application under heavy load with more than ten searches going on at a time, and you need features like autocomplete, then you may benefit more from a specialized text solution like Elasticsearch.Redis, Elasticsearch, and Kafka are powerful, but sometimes adding them does more harm than good. You may be able to get the capabilities you need with Postgres by taking advantage of the lesser-known features we’ve covered here. Ensuring that you are getting the most out of Postgres can save you time and help you avoid added complexity and risks.To save even more time and headaches, consider using a managed service like Heroku Postgres. Scaling up is a simple matter of adding additional follower replicas, high availability can be turned on with a single click, and Heroku operates it for you. If you really need to expand beyond Postgres, the other data stores that we mentioned above, such as Redis, Apache Kafka and Elasticsearch, can all be easily provisioned on Heroku. Go ahead and build your castles in the air―but anchor them to a reliable foundation, so you can dream about a better product and customer experience.For more information on Postgres, listen to Cloud Database Workloads with Jon Daniel on Software Engineering Daily."
https://dev.to/dmfay/the-ultimate-postgres-vs-mysql-blog-post-1l5f,The Ultimate Postgres vs MySQL Blog Post,2018-04-11T17:48:14Z,Dian Fay,"I should probably say up front that I love working with Postgres and could die happy without ever seeing a mysql> prompt again. This is not an unbiased comparison -- but those are no fun anyway.The scenario: two applications, using Massive.js to store and retrieve data. Massive is closely coupled to Postgres by design. Specializing lets it take advantage of features which only exist in some or no other relational databases to streamline data access in a lighter, more ""JavaScripty"" way than a more traditional object-relational mapper. It's great for getting things done, since the basics are easy and for the complicated stuff where you'd be writing SQL anyway.... you write SQL, you store it in one central place for reuse, and the API makes running it simple.Where Massive is less useful is if you have to support another RDBMS. This is, ideally, something you know about up front. Anyway: things happen, and sometimes you find yourself having to answer the question ""what's it going to look like if we need to run these applications with light but tightly coupled data layers on MySQL?""Not good, was the obvious answer, but less immediately obvious was how not good. I knew there were some things Postgres did that MySQL didn't, but I also knew there were a ton of things I'd just never tried in the latter. So as I got to work on this, I started keeping notes. Here's everything I found.Now that we're all basically over the collective hallucination of a ""schemaless"" future, arguably the most important aspect of data storage is how information is modeled in a database. Postgres and MySQL are both relational databases, grouping records in strictly-defined tables. But there's a lot of room for variation within that theme.First things first: ""schema"" doesn't always mean the same thing. To MySQL, ""schema"" is synonymous with ""database"". For Postgres, a ""schema"" is a namespace within a database, which allows you to group tables, views, and functions together without having to break them apart into different databases.MySQL's simplicity in this respect is ameliorated by its offering cross-database queries:With Postgres, you can work across schemas, but if you need to query information in a different database, that's a job for...Foreign data wrappers let Postgres talk to practically anything that represents information as discrete records. You can create a ""foreign table"" in a Postgres database and SELECT or JOIN it like any other table -- only under the hood, it's actually reading a CSV, talking to another DBMS, or even querying a REST API. It's a powerful enough feature that NoSQL stalwart MongoDB sneakily built their BI Connector on top of Postgres with foreign data wrappers. You don't even need to know C to write a new FDW when Multicorn lets you do it in Python!Oracle and SQL Server both have some functionality for registering external data sources, but Postgres' offering is the most extensible I'm aware of. MySQL, besides the inter-database query support mentioned above, has nothing.Inheritance is more commonly thought of as an attribute of object-oriented programming languages rather than databases, but Postgres is technically an ORDBMS or object-relational database management system. So you can have a table cities with columns name and population, and a table capitals which inherits the definition of cities but adds an of_country column only relevant, of course, for capital cities. If you SELECT from cities, you get rows from capitals -- they're cities too! You can of course SELECT name FROM ONLY cities to exclude the capitals. This is something of a niche feature, but when you have the right use case it really shines.MySQL, being a traditional RDBMS, doesn't do this.Materialized views are like regular views, except the results of the specifying query are physically stored ('materialized') and must be explicitly refreshed. This allows database developers to cache the results of slower queries when the results don't have to be realtime.Oracle has materialized views, and SQL Server's indexed views are similar, but MySQL has no materialized view support.Constraints in general ensure that invalid data is not stored. The most common constraint is NOT NULL, which prevents records without a value for the non-nullable column from being inserted or updated. Foreign key constraints do likewise when a reference to a record in another table is invalid. Check constraints are the most flexible, and allow validation of any predicate you could put in a WHERE clause -- for example, asserting that prices have to be positive numbers, or that US zip codes have to be five digits.Per the MySQL docs: the CHECK clause is parsed but ignored by all storage engines.Postgres and MySQL both have a JSON column type (MySQL replacement MariaDB does too, but it's currently just an alias for LONGTEXT) and functions for building, processing, and querying JSON fields. Postgres actually goes a step further by offering a JSONB type which processes input data into a binary format. This means it's a little bit slower to write, but much faster to query.It also means you can index the binary data. A GIN or Generalized INverted index allows queries checking for the existence of specific keys or key-value pairs to avoid scanning every single record for matches. This is huge if you run queries which dig into JSON fields in the WHERE clause.DEFAULT is a useful specification for columns in a CREATE TABLE statement. At the simplest level, this could be used to baseline a boolean field to true or false if the INSERT statement doesn't give an explicit value. But you can do more than set a scalar value: a timestamp can default to now(), a UUID to any of a variety of UUID-generating functions, any other field to the value returned by whatever function you care to write -- the sky's the limit!Unless you're using MySQL, in which case the only function you can reference in a DEFAULT clause is now().Layout's only part of the story, though. Equally important is the difference in type support. The benefit of a robust type system is in enabling database architects to represent information with the greatest accuracy possible. If a value is difficult or impossible to represent with built-in types, it's harder for developers to work with in turn, and if compromises have to be made to cut the data to fit then they can affect entire applications. Some types can even affect the overall database design, such as arrays and enumerations. In general, the more options you have the better.Postgres has a UUID type. MySQL does not. If you want to store a UUID in MySQL, your options are CHAR, if you want values to be as human-readable as UUIDs ever are, or BINARY, if you want it to be faster but more difficult to work with manually. Postgres also generates more types of UUIDs.Boolean seems like a pretty basic type to have! However, MySQL's boolean is actualy an alias for TINYINT(1). This is why query results show 0 or 1 instead of true or false. It's also why you can set the value of an ostensibly boolean field to 2. Try it!Postgres: has proper booleans.MySQL isn't alone in aliasing standard types in strange ways, however. CHAR, VARCHAR, and TEXT types in Postgres are all aliased representations of the same structure -- the only distinction is that length constraints will be enforced if specified. The documentation notes that this is actually slower, and recommends that unbounded text simply be defined as the TEXT type instead of given an arbitrary maximum length.What's happening here is that Postgres uses a data structure called a varlena, or VAriable LENgth Array, to store the information. A varlena's first four bytes store the length of the value, making it easy for the database to pick the whole thing out of storage. TEXT is only one of the types that uses this structure, but it's easily the most commonly encountered.If a varlena is longer than would fit inline, the database uses a system called TOAST (""The Oversized Attribute Storage Technique"") to offload it to extended storage transparently. Queries with predicates involving a TOASTable field might not be all that performant with large tables unless designed and indexed carefully, but when the database is returning records it's easy enough to follow the TOAST pointer that the overhead is barely noticeable for most cases.The upshot of all this, as far as most people are concerned, is this: with Postgres, you only have to worry about establishing a length constraint on fields that have a reason for a length constraint. If there's no clear requirement to limit how much information can go into a field, you don't have to pick an arbitrary number and try to match it up with your page size.Non-scalar values in records! Madness! Dogs and cats living together! Anyone who's worked with JSON, XML, YAML, or even HTML understands that information isn't always flat. Relational architectures have traditionally mandated breaking out any vectors, let alone even more complex values, into new tables. Sometimes that's useful, but often enough it adds complexity to no real purpose. Inlining arrays makes many tasks -- such as tagging records -- much easier.Postgres has arrays, as does Oracle; MySQL and SQL Server don't.If the built-in types aren't sufficient, you can always add your own. Custom types let you define a value to be exactly what you want. Domains are a related concept: types (custom or built-in) which enforce constraints on values. You might for example create a domain to represent a zip code as a TEXT value which uses regular expressions in a CHECK clause to ensure that values consist of five digits, optionally followed by a dash and four more digits.If you're using Postgres, that is. Oracle and SQL Server both offer some custom type functionality, but MySQL has nothing. You can't even use table-level CHECK constraints because the engine simply ignores them.Enumerations don't get enough love. If I had a dollar for every INT -- or worse, VARCHAR -- field I've seen representing one of a fixed set of potential values, I probably still couldn't retire but I could at least have a pretty nice evening out. There are drawbacks to using enums, to be sure: adding new values requires DDL, and you can't remove values at all. But appropriate use cases for them are still reasonably common.MySQL and Postgres both offer enums. The critical distinction is that Postgres' enums are proper reusable types. MySQL's enums are more like the otherwise-ignored CHECK constraints and specify a valid value list for a single column in a single table. Possible improvement on allowing a boolean column to contain -100?So that's data modeling covered. There's an entire other half to go: actually working with the information being stored. SQL itself is divided in two parts, the ""data definition language"" which defines the structure of a database and the ""data manipulation language"". This latter comprises the SELECT, INSERT, and other statements most people think of when they hear the name ""SQL"". And just as with modeling, there are substantial differences between Postgres and MySQL in querying.Autogenerating primary keys takes a huge headache out of storing data. But there's one catch: when you insert a new record into a table, you don't know what its primary key value got set to. Most relational databases will tell you what the last autogenerated key was if you call a special function; some, like SQL Server, even let you filter down to the single table you're interested in.Postgres goes above and beyond with the RETURNING clause. Any write statement -- INSERT, UPDATE, DELETE -- can end with a RETURNING [column-list], which acts as a SELECT on the affected records. RETURNING * gives you the entire recordset from whatever you just did, or you can restrict what you're interested in to certain columns.That means you can do this:With MySQL, you're stuck with calling LAST_INSERT_ID() after you add a new record. If you added multiple, LAST_INSERT_ID only gives you the earliest new id, leaving you to work out the rest yourself. And of course, this is only good for integer primary keys.MySQL also has no counterpart to this functionality for UPDATEs and DELETEs. Competitor MariaDB supports RETURNING on DELETE, but not on any other kind of statement.Common Table Expressions or CTEs allow complex queries to be broken up and assembled from self-contained parts. You might write this:In the first query, we aggregate visit counts; in the second, we use DISTINCT ON on the results of the first to filter out all but the most popular pages; finally, we join both of our intermediary results to provide the output we're looking for. CTEs are a really readable way to factor query logic out, and they let you do some things in one statement that you can't otherwise.MySQL does have CTEs! However: thanks to the RETURNING clause, Postgres can write records in a CTE and operate on the results. This is huge for application logic. This next query writes a record in a CTE, then adds a corresponding entry to a junction table -- all in the same transaction.Sometimes a query needs to treat a value as if it has a different type, whether to store it or to operate on it somehow. Postgres even lets you define additional conversions between types with CREATE CAST.MySQL supports casting values to binary, char/nchar, date/datetime/time, decimal, JSON, and signed and unsigned integers. Absent from this list: tinyints, which, since booleans are actually tinyints, means you're stuck with conditionals when you need to coerce a value to true or false for storage in a ""boolean"" column.A lateral join is fundamentally similar to a correlated subquery, in that it executes for each row of the current result set. However, a correlated subquery is limited to returning a single value for a SELECT list or WHERE clause; subqueries in the FROM clause run in isolation. A lateral join can refer back to information in the rest of the result set:It can also invoke table functions like unnest which return multiple rows and columns:Oracle and SQL Server offer similar functionality with the LATERAL keyword in the former, and CROSS APPLY/OUTER APPLY. MySQL does not.Functions! Procedures, if you believe in making that distinction! They're great! You can declare variadic arguments -- ""varargs"" or ""rest parameters"" in other languages -- to pull an arbitrary number of arguments into a single collection named for the final argument.In Postgres.A handful of useful operations which allow more expressive WHERE clauses with Postgres:That's it for the architecture and query language feature gaps I discovered. I did run into a couple other things that bear mentioning, however:MySQL doesn't care about dependencies among database objects. You can tell it to drop a table a view or proc depends on and it will go right ahead and drop it. You'll have no idea something's gone wrong until the next time you try to invoke the view or proc. Postgres saves you from yourself, unless you're really sure and drop your dependents too with CASCADE.Just the mention of triggers is probably putting some people off their lunch. They're not that bad, honest (well, they can be, but it's not like it's their fault). Anyway, point is: sometimes you want to write a trigger that modifies other rows in the table it's being activated from.Well, you can't in MySQL.This may have exhausted me, but I'm pretty sure it's still not an exhaustive list of the feature gaps between Postgres and MySQL. I did cop to my preference up front, but having spent six weeks putting the effort into converting the comparison is pretty damning. I think there could still be reasons to pick MySQL -- but I'm not sure they could be technical."
https://dev.to/kiwicopple/loading-json-into-postgres-2l28,Loading JSON into Postgres,2020-04-16T14:00:03Z,Copple,"Today I had to load some JSON data into Postgres.Postgres' COPY command it expects one JSON object per line rather than a full array.For example, instead of a JSON array:It needs to be this format:It took me a surprisingly long time to get the data into Postgres, but the solution was fairly simple.Here's how I did it.This is done with one command:ExplanationFrom here it's easiest to ingest the data into a JSONB column.20 seconds of reading, and 1 hour of my time. To get the data out of the table now you can use any of Postgres' amazing JSON support. For example:Enjoy."
https://dev.to/condenastitaly/when-food-meets-ai-the-smart-recipe-project-b3g,When Food meets AI: the Smart Recipe Project,2020-08-07T10:05:49Z,Condé Nast Italy,"Did you ever try a Maritozzo?In the past post, we converted the recipe data, stored in JSON files, into RDF triples. In this post, we show you:To query the graph, we use SPARQL. SPARQL is an RDF query language, namely a semantic query language for databases, able to retrieve and manipulate data stored or viewed in the RDF format.We followed the described procedure to load the RDF triples on the Amazon Neptune service. We used an Amazon Simple Storage Service, the Amazon S3 bucket. Firstly we created an S3 bucket; then we uploaded the data. In this first phase, we loaded the RDF data to build the first level of the graph (see the previous article).In the case we want to add a few recipes at the time, we can alternatively use the SPARQL statement INSERT DATA :Once the recipes have been loaded, we checked whether there are recipes not yet processed by the extractor and classifier services. This means to check which recipes have not i) food entity chunks extracted (the bnode in the graph, see the previous article); ii) ingredients classified.This is the SPARQL query to check whether bnodes exist in the graph (through the statement FILTER NOT EXISTS), which is equivalent to say “return all the recipes without bnodes”:Now the graph is on Amazon Neptune. Let’s have fun of these connections, extracting knowledge from the graph:With the above query we interrogate the graph to know 1) whether there are recipes containing the ingredient “butter” and 2) which are these recipes. The WHERE statement navigates the graph following the pattern described in the triples to arrive at the query result. In this case, the output is the id of the recipes which have the ingredients ”butter”. We can query the graph to return recipes containing more than one ingredient or all the recipes containing some ingredients and not others:With this last article, we conclude illustrating the main stages of the Smart Recipe Project, this innovative and amazing project involving on one side the global company Condé Nast, and on the other the IT company RES.We have in mind some possible interesting applications for the resources we developed under the Smart Recipe Project like:personalization of contents, personalized recipe searchers, newsletter; recommendation systems for food items, recipes, and menus, which integrate, where needed, dietary restrictions; virtual assistants, able to guide you in planning and cooking meals; smart cooking devices, and much more.As always, go on Medium to read the complete article.When Food meets AI: the Smart Recipe Project a series of 6 amazing articlesTable of contentsPart 1: Cleaning and manipulating food data Part 1: A smart method for tagging your datasets Part 2: NER for all tastes: extracting information from cooking recipes Part 2: Neither fish nor fowl? Classify it with the Smart Ingredient Classifier Part 3: FoodGraph: a graph database to connect recipes and food data Part 3. FoodGraph: Loading data and Querying the graph with SPARQL"
https://dev.to/adamcowley/building-a-modern-web-application-with-neo4j-and-nestjs-38ih,Building a Modern Web Application with Neo4j and NestJS,2020-07-02T08:52:46Z,Adam Cowley,"This article is the introduction to a series of articles and a Twitch stream on the Neo4j Twitch channel where I build an application on top of Neo4j with NestJS and a yet-to-be-decided Front End. This week I built a Module and Service for interacting with Neo4j.TL;DR: I've pushed the code to Github and created a Neo4j module for NestJS to save you some time.Over the past few weeks I have been spending an hour live streaming something that I have found interesting that week, but from this week I thought I would change things up and start to build out a project on Neo4j.If you're subscribed to this channel then you are likely familiar with Neo4j, but if not then Neo4j is the world's leading Graph Database. Rather than tables or documents, Neo4j stores it's data in Nodes - those nodes are categorised by labels and contain properties as key/value pairs. Those Nodes are connected together by relationships, which are categorised by a type and can also contain properties as key/value pairs.What sets Neo4j apart from other databases is it's ability to query connected datasets. Where traditional databases build up joins between records at read time, Neo4j stores the dataNeo4j is schema-optional - meaning that you can enforce a schema on your database if necessary by adding unique or exists constraints on Nodes and Relationships.I've been experimenting with Typescript for a while now, and the more I use it the more I like it.Typescript is essentially Javascript but with additional static typing. Under the hood, it compiles down to plain Javascript but it improves the developer experience a lot, and allows you to identify problems in real-time as you are writing your code.By far the best framework I have seen that supports typescript is NestJS. NestJS is an opinionated framework for building server-side applications. It also includes modern features you'd expect in a modern framework like Spring Boot or Laravel - mainly Dependency Injection.Nest comes with a CLI with many helpers for starting and developing a project. You can install it by running:Once it's installed, you can use the new or n command to create a new project.After selecting the package manager of your choice, the CLI command will generate a new project and install any dependencies. Once it's done, you can cd into the directory and then run npm run start:dev to fire up the development server.In the generated src/ folder, you'll see:Functionality in Nest is grouped into modules, the official documentation uses Cats as it's example. Modules are a way of grouping related functionality together. In the Cats example, the module provides a CatsService which handles the applications interactions with Cats, and a Cats controller which registers routes which define how the Cats are accessed.Module classes are defined by a @Module annotation, which in turn defines which child modules are imported into module, any controllers that are defined in the module, and any classes that are exported from the module and made available for dependency injection.Take the annotation on the Cats example in the documentation, this is saying that the CatsModule registers a single controller CatsController and provides the CatsService.The CatService is registered with the Nest instance and can then be injected into any class.Classes annoted with @Injectable() are automatically injected into a class using some under-the-hood Nest ""magic"". For example, by defining the CatsService in the constructor for the CatsController, Nest will automatically resolve this dependency and inject it to the class without any additional code.This is identical to how things work in more mature frameworks like Spring and Laravel.Dependency Injection is a software technique where a class will be ""injected"" with instances of other classes that it depends on. This makes the testing process easier where instead of instantiating classes. It also promotes the principles of DRY - don't repeat yourself and SOLID. Each class should have a single responsibility - for example a User service should only be concerned with acting on a User's record, not be concerned with how that record is persisted to a database.In order to use Neo4j in services across the application, we can define a Neo4jService for interacting with the graph through the JavaScript driver. This service should provide the ability to interact with Neo4j but without the service itself needing to know any of the internals. This service should be wrapped in a module which can be registered in the application.The first step is to install the Neo4j Driver.Then, we can use CLI to generate a new module with the name Neo4j.The command will create a neo4j/ folder with it's own module. Next, we can use the CLI to generate the Service:This command will generate neo4j.service.ts and append it to the providers array in the module so it can be injected into any application that uses the module.By default, these modules are registerd as static modules. In order to add configuration to the driver, we'll have add a static method which accepts the user's Neo4j credentials and returns a DynamicModule.The first thing to do is generate an interface that will define the details allowed when instantiating the module.The driver takes a connection string and an authentication method. I like to split up the connection string into parts, this way we can validate the scheme.The scheme (or protocol) at the start of the URI should be a string, and one of the following options:The host should be a string, port should either be a number or a string, then username, password should be a string. The database should be an optional string, if the driver connects to a 3.x version of Neo4j then this isn't a valid option and if none is supplied then the driver will connect to the default database (as defined in neo4j.conf - dbms.default_database).Next, for the static method which registers the dynamic module. The documentation recommends using the naming convention of forRoot or register. The function should return a DynamicModule - this is basically an object that contains metadata about the module.The module property should return the Type of the module - in this case Neo4jModule. This module will provide the Neo4jService so we can add the class to the provides array.Because we are providing a configuration object, we'll need to register it as a provider so that it can be injected into the Neo4jService. For providers that are not defined globally, we can define a unique reference to the provider and assign it to a variable. We will use this later on when injecting the config into the service. The useValue property instructs Nest to use the config value provided as the first argument.If the user supplies incorrect credentials, we don't want the application to start. We can create an instance of the Driver and verify the connectivity using an Asynchronous provider. An async provider is basically a function that given a set of configuration parameters, returns an instance of the module that is configured at runtime.In a new file neo4j.utils.ts, create an async function to create an instance of the driver and call the verifyConnectivity() to verify that the connection has been successful. If this function throws an Error, the application will not start.The function accepts the Neo4jConfig object as the only argument. Because this has already been defined as a provider, we can define it in the injects array when defining it as a provider.Now that the driver has been defined, it can be injected into any class in it's own right by using the @Inject() annotation. But in this case, we will add some useful methods to the Neo4jService to make it easier to read from and write to Neo4j. Because we have defined NEO4J_DRIVER in the provides array for the dynamic module, we can pass the NEO4J_DRIVER as a single parameter to the @Inject directive in the constructor.Each Cypher query run against Neo4j takes place through a Session, so it makes sense to expose this as an option from the service. The default access mode of the session allows the Driver to route the query to the right member of a Causal Cluster - this can be either READ or WRITE. There is also an optional parameter for the database when using multi-tenancy in Neo4j 4.0. As I mentioned earlier, if none is supplied then the query is run against the default database.So the user doesn't need to worry about the specifics of read or write transactions, we should create a method for each mode - both with an optional parameter for the database. There is also a database specified in the Neo4jConfig object, so we should fall back to this if none is explicitly specified.These methods make use of NEO4J_CONFIG and NEO4J_DRIVER which were injected into the constructor.So with that in mind, it would be useful to create a method to read data from Neo4j. The driver accepts parameterised queries as a string (eg. queries with literal variables replaced with parameters - $myParam) and an object of parameters so these will be the arguments for the query. Optionally, we may want to specify which database this query is run against so it makes sense to include that as an optional third parameter. The query then returns a Result statement which includes the result and some additional statistics.Over the course of the application, this will save us a few lines of code. The same can be done for a write query:Now we have a service that is registered in the main application through the Neo4jModule that can be injected into any class in the application. So as an example, let's modify the Controller that was generated in the initial command. By default, the route at '/' returns a hello world message, but instead let's use it to return the number of Nodes in the database.To do this, we should first inject the Neo4jService into the controller:Now, we can modify the getHello method to return a string. The constructor will automatically assign the Neo4jService to the class so it is accessible through this.neo4jService. From there we can use the .read() method that we've just created to execute a query against the database.Navigating in the browser to http://localhost:3000 should now show a message including the number of nodes in the database.Tune in to the Neo4j Twitch channel Tuesdays at 13:00BST, 14:00CEST for the next episode."
https://dev.to/codaelux/running-dynamodb-offline-4k1b,How to run DynamoDB Offline,2020-03-23T21:48:31Z,Joseph Peculiar,"I am assuming you are already familiar with the Serverless Framework so I'll jump right into itAmazon DynamoDB:- is a fully managed proprietary NoSQL database service that supports key-value and document data structures designed to deliver fast and predictable performance.The Serverless Framework:- enables developers to deploy backend applications as independent functions that will be deployed to AWS Lambda.let's kick off by configuring AWS if not already configured doing this you will have to download the AWS CLI for Windows, macOS, or LinuxThe serverless framework will need us to configure access to AWS - This is accomplished by runningInstall Serverless globally if you haven't installed it alreadyNow let's setup up the serverless-offline pluginInstall serverless-dynamodb-local pluginUpdate your serverless.yml file - add this to the bottom of your serverless.yml fileInitialize DynamoDB - this initializes DynamoDB offline in your project folderRun serverless:-Be sure DynamoDB is running on port 8000 because serverless-offline will use that port as defaultyou should see the output belowKeep this console running.Now let's setup dynamodb-admin: this gives us a good GUI to view our DynamoDB tablesNow runWe should have our table displayedConfiguring serverless.yml for DynamoDBI published a great cheat sheet with all commands needed for setting up DynamoDB offline you can find it herehttps://github.com/codaelux/DynamoDB-offline-doc"
https://dev.to/arthurolga/newsql-an-implementation-with-google-spanner-2a86,NewSQL: An Implementation with Google Spanner,2020-08-10T16:01:25Z,arthurolga,"What is actually NewSQL? You’ve probably already dealt with SQL and NoSQL databases. Each with its own advantages and disadvantages. In this article, I’m going to present the main characteristics of NewSQL databases and a simple implementation using Google Spanner.Firstly, a little recap about the main topics concerning relational e and no-relational databases.SQL is known for being sturdy and organized, dealing with a set of properties called ACID (Atomicity, Consistency, Isolation, Durability), which is the main reason it is so popular, bringing a lot of these sturdiness. But this also means that it only scales vertically, so large companies might need a single powerful machine to support all their requests.NoSQL, on the other hand, is very flexible, without the need for the relational structure of the regular SQL, being able to support non-structured forms of data. This type of database is known for being able to scale horizontally, which means that companies can build database servers close the global clients. But, these non-relational databases don’t support ACID transactions, which means they cannot provide the same consistency as the regular relational.NewSQL comes with the idea of bringing the major advantages in each SQL and NoSQL to the same service. These databases use various techniques to provide these functionalities, as such:Partitioning/Sharding: In order to be able to scale horizontally, NewSQL uses a system of dividing itself in various shards/nodes/partitions. Differently form the fragmentation of the NoSQL databases, the shards run partial parts of the whole databases, although the whole works as a distributed application.Main Memory Storage: This characteristic provides the ability for the database to run on the memory, instead of a hard disc or flash memory. This provides the NewSQL a lot of leverage in terms of speed.Replication and Consistency: NewSQL replicates itself through transparent nodes using the Paxos or Raft consensus algorithm. They are families of complex protocols able to assure consensus in a network of unreliable processes.Concurrency Control: The NewSQL applies Multi-Version Concurrency Control (MVCC), Timestamp methods and T/O to assure the access is granted to the necessary nodes given a certain operation. This together with the consensus algorithm provides the ability of the database to support ACID transactions.This all makes NewSQL databases capable of OLTP, unlike any other databases, considering even SQL supports it partially.One of the best ways to implement a NewSQL database is using Google's DBaaS solution, called Spanner. Which can start a instance ready in minutes.But this all comes with a cost, a large cost. A simple Google Spanner Database with two nodes can cost about 2.00 USD per hour, a lot more than regular services. The main reason this service costs that much is because of the in-memory implementation, which is a lot pricier than a normal hard disc. Also, as the transparent shards work with redundancy, the space needed to store data can sometimes be very large to provide the consistency needed.We are going to build a Google Spanner Instance running with two nodes. Be careful as this will result in charges of around 2.00 USD per hour, so you can delete it after this tutorial. For this, we're going to need Python 2.7 and a Google Cloud account.Supposing you already have a Google Account with a credit card. Enter in your Google Cloud console, specifically at the Projects page: https://console.cloud.google.com/cloud-resource-managerClick on the ""Create Project"" button. We are going to create a new project for this tutorial, but you can also use one of your own.Choose a Project Name that suits you, but you are going to have to remember it for the later parts. Click on ""Create"" and wait for your project to be ready.Now, activate de Google Spanner API on your project: https://console.cloud.google.com/flows/enableapi?apiid=spanner.googleapis.comWe are going to deploy the instance using the CLI. You can also do this using the Google Cloud Console on the web and switch to the command line at any moment.Open your terminal and check if you have Python 2.7 installed and on your path:You can install it using Homebrew with:Alternatively you can install using the interactive installer bellow.Run:Reboot the shell:Use the GUI installer for windows, available at: https://cloud.google.com/sdk/downloads#windowsRun on the terminal:Chrome will open a window for you to login with your Google Account.Run to show all available projects:The project you just created should appear like this:Now you want to select this project as default. If you choose not to, you will have to pass a project parameter at every command.Now we are going to run our instances with two nodes. I'm choosing us-east1 as the region because it is the cheapest by the time I'm writing this tutorial.To see the instance you have created, run:Which should return as such:The instance is identified by test-instance, and the display name is My Instance.Now we are going to build a database and a table. As I've mentioned earlier, we can straight up use SQL language to work with this Database, so we're going to build some simples queries.To create a database called classroom:We are going to build a simples student table. For this, we are going to run a DDL command with the content of the query being:To run this on the Google CLI, we are going to run a command on the directory of this file with:You can use gcloud spanner databases ddl update DATABASE every time you want to alter the database schema.Now, to add run a DML we are going to use execute-sql:To add some more rows:Now we can see the table with:Which should returnNow to delete this expensive test instance:As you've seen, running commands on the CLI can be useful, but to build a real service, like a REST application, you need to be able to run these commands in any sort of language. So we're going to run a query using Python 3.7,** **but you can choose from a variety of different languages, such as C#, NodeJS or Ruby.Now we are going to run a query using a program. First You should have Python3 and pip installed.Run:Now create a file name **test-spanner.py **with the inicialization of the Client and Instance and the run of a simple Select query:Running this with python3 test-spanner.py should print on the terminal something like:This concludes this Tutorial, entirely based on the Google Spanner Documentation. Which is awesome! If you want to continue using this database, you should really check it out. Thanks for checking this out!Google Spanner Documentation: https://cloud.google.com/spanner/docs/"
https://dev.to/javinpaul/5-best-courses-to-learn-apache-kafka-in-2020-584h,5 Best courses to learn Apache Kafka for Java Programmers,2019-12-21T04:50:27Z,javinpaul,"Disclosure: This post includes affiliate links; I may receive compensation if you purchase products or services from the different links provided in this article.You might have heard about Apache Kafka, the next generation big data messaging system which is handling billions of messages per day for companies like LinkedIn, Uber, Airbnb, and Twitter.In the past, I have shared some awesome courses on Big Data, Spark, and Hadoop and many of my readers requested that I share similar suggestions for Apache Kafka. If you are interested in Apache Kafka and looking for some awesome courses to learn online then you have come to the right place.In this article, I am going to share some of the best Apache Kafka courses to learn online. If you know about big data then there is a good chance that you have also heard about Apache Kafka, the software which can handle data feeds for any organization. It's a distributed streaming platform developed by the Apache Foundation for building real-time data pipelines.The biggest advantage of Apache Kafka is its speed and horizontal scalability, which means you can increase capacity and throughput by adding more hardware. This makes it ideal for companies dealing with big data.Apache Kafka was designed to address large-scale data movement problems and has enabled hundreds of companies to achieve successes which were not otherwise possible with existing messaging systems.You might not know that Apache Kafka is written in Scala and Java and it aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.This is a great course to start learning Apache Kafka from Scratch. In this course, instructor Ryan Plant will teach you everything about the architecture of Apache Kafka which enabled it to scale and perform so reliably.After the initial overview, the course moves to explain individual components of Apache Kafka, like Kafka Streams for processing real-time data feeds and you how to develop Apache Kafka solutions in Java.After this course, you should have all the necessary knowledge to build your own, next-generation big data solutions with Apache Kafka.Here is the link to join the course --- Getting Started with Apache KafkaBy the way, you need a Pluralsight membership to access this course, which cost around $29 per month. But, if you want, you can get access to this course for a fee by signing up for a 10-day free trial.This is another good course to learn Apache Kafka from ground zero. It's an ideal course for both developers and architects who want to learn the fundamentals of Apache Kafka.In this course, instructor Stephane Maarek, author of a series of Apache Kafka courses on Udemy will teach you everything about the Apache Kafka ecosystem from its architecture and core concepts to operations.The course is also hands-on as you will start a personal Kafka Cluster for development purposes and create and configure topics for reading and writing data.You will also learn to integrate Apache Kafta with popular programming and big data frameworks like Spark, Akka, Scala, and Apache NiFi.Here is the link to Sign up ---Learn Apache Kafka for BeginnersThis is another awesome course on Apache Kafka by Stephane Maarek. This course is focused on Kafka Streams, a client-side library for building microservices, where input and output data are stored in a Kafka cluster.In this course, you will learn how to use the Kafka Streams API with hands-on examples in Java 8. Though, before attempting this course you should have a good understanding of both Java 8 and Apache Kafka.You will also learn about KStream and KTable, simple and advanced operations, and Exactly Once Semantics, or EOS, like how Kafka enables EOS and how to activate it in Kafka Streams.Here is the link to Sign up --- Apache Kafka Streams for Data ProcessingThis course is part of the Big Data Hadoop Architect master's program in SimpliLearn and it will teach you everything about Apache Kafka you want to know. It's a certification course so it covers a variety of topics.In this Apache Kafka training course, you will learn about Kafka architecture, installation, interfaces, and configuration.The course starts with an overview of big data and then explains ZooKeeper and Apache Kafka from the introduction to installation.Here is the link to Sign up --- Apache Kafka Certification TrainingThis is the third course in the Apache Kafka series by Stephane Marek on Udemy. In this course, you will learn about Kafka Cluster Setup and Administration.You will set up a ZooKeeper and Kafka cluster on AWS and learn how to deploy Kafka in production. You will also set up a ZooKeeper Cluster and understand its role in Kafka.This is an ideal course for system Administrators or Architects who want to learn how to set up a Kafka Cluster on multiple serversHere is the link to Sign up --- Apache Kafka Cluster Setup and AdministrationThat's all about some of the best courses to learn Apache Kafka for Java developers. Apache Kafka is a groundbreaking technology and power more than 2000+ companies for their high speed messaging need and a good knowledge of Apache Kafka will go a long way to boost your career. I strongly recommend experienced Java developer, tech lead and solution architect to learn and understand Apache Kafka.Other Programming Resources you may like:Thanks for reading this course so far. If you like these Apache Kafka online training courses and certification then please share with your friends and colleagues. If you have any questions or feedback then please drop a note."
https://dev.to/subhransu/realtime-chat-app-using-kafka-springboot-reactjs-and-websockets-lc,"Realtime Chat app using Kafka, SpringBoot, ReactJS, and WebSockets",2020-04-25T23:17:22Z,Subhransu Maharana,"In this tutorial, we would be building a simple real-time chat application that demonstrates how to use Kafka as a message broker along with Java, SpringBoot as Backend, and ReactJS on the front-end.This project is just for learning purposes. It doesn't contain a production-ready code.Apache Kafka is a widely popular distributed messaging system that provides a fast, distributed, highly scalable, highly available, publish-subscribe messaging system.In turn, this solves part of a much harder problem:Communication and integration between components of large software systems.Before starting the project, We need to download Zookeeper and Kafka.You can download Kafka from here.Extract the contents of the compressed file into a folder of your preference. Inside the Kafka directory, go to the bin folder. Here you’ll find many bash scripts that will be useful for running a Kafka application.If you are using Windows, you also have the same scripts inside the windows folder. This tutorial uses Linux commands, but you just need to use the equivalent Windows version if you’re running a Microsoft OS.Zookeeper is basically to manage the Kafka cluster. It comes bundled with the downloaded Kafka directory. So, we need not download it separately.To start the zookeeper, go to the bin directory and enter the below command.Next, To start the Kafka broker, run the below command in the same directoryMake sure the zookeeper is running before starting Kafka because Kafka receives information such as Offset information kept in the partitions from Zookeeper.After running Zookeeper and Apache Kafka respectively, We can create a Topic and send and receive data as Producer and Consumer.Here we are creating a topic kafka-chat to handle chat messages. We would be using this topic later in the chat application.Now, Let's write some code.We would be developing the backend in Spring Boot. So, download a fresh Spring Boot Project using Spring Initializer with the following details.Since Apache Kafka cannot send the Consumer Messages instantly to the client with Classical GET and POST operations. I performed these operations using WebSockets which provide full-duplex bidirectional communication, which means that information can flow from the client to the server and also in the opposite direction simultaneously. It is widely used in chat applications.First lets create a Message Modal which would hold the message content. Message.javaFirst, we would have to write a Config class for the Producer.ProducerConfiguration.javaThis class creates a ProducerFactory which knows how to create producers based on the configurations we provided.We also declared a KafkaTemplate bean to perform high-level operations on your producer. In other words, the template can do operations such as sending a message to a topic and efficiently hides under-the-hood details from you.In producerConfigurations method, we need to perform the following tasks:The next step is to create an endpoint to send the messages to the Kafka topic. Create the following controller class for that.As you can see the endpoint is quite simple. When we do POST request to /api/send it Injects the KafkaTemplate configured earlier and sends a message to the kafka-chat topic which we created earlier.Let's test everything we build until now. Run the main method inside KafakaJavaApp.java class. To run from the command line, execute the following commandYour server should be running on port 8080 and you can make API requests against it! You can use postman to do a POST request as shown below.But how do you know the command successfully sent a message to the topic? Right now, you don’t consume messages inside your app, which means you cannot be sure!Fortunately, there is an easy way to create a consumer to test right away. Inside the bin folder of your Kafka directory, run the following command:Hit http://localhost:8080/api/send again to see the message in the terminal running the Kafka consumerNow let's achieve the same functionality using the Java Code. For that, we would need to build a Consumer or Listener in Java.Similar to ProducerConfig.java we need to have a Consumer Config to enable the consumer to find the broker.ListenerConfig.javaIn Consumer Config, similar to Producer Config we are setting the deserializer for key and value. Additionally we need to setMessageListener.javaIn this class, the @KafkaListener annotated the method that will listen for the Kafka queue messages, and template.convertAndSend will convert the message and send that to WebSocket topic.Next, we need to configure the Websocket to send the Message to the client system.WebSocketConfig.javaNext add the below MessageMapping in the ChatController.javaThis would broadcast the Message all the client who have subscribed to this topic.Next, let's move on to developing the UI part.We would create a simple chat page with a list of messages and a text field at the bottom of the page to send the messages to Kafka backend.We will use Create React App to quickstart the app.Install dependenciesYou can refer documentation of material-ui here.Copy the CSS styleCopy the css style from here paste it in the App.css file.Next, add the below changes to App.jsApp.jsHere we are using SocketJsCLient from react-stomp to connect to the WebSocket.Alternatively, you can also use SockJS from sockjs-client to create a stompclient and connect to the WebSocket.Next, we need to create Messages Child Component which would show the list of messages.LoginForm.jsOpen the application in multiple windows and send a message in one window. All the other browser window should show the sent messages.we are using SockJS to listen to the messages, which are sent from the server-side WebSocket.You can find the complete source code in my Github page."
https://dev.to/offlineprogrammer/collect-analytics-data-for-your-app-using-aws-amplify-4ifp,Collect analytics data for your App using AWS Amplify,2020-08-09T21:35:31Z,Offline Programmer,"Wouldn’t be great to get some insights on the behavior of your App users. This will allow you implement data driven features to drive customer adoption, engagement, and retention.This is where AWS Amplify can help, in this post I will show you how you can integrate your Android App with AWS Amplify to achieve this. Let’s get started.Check the full code on github and follow me on Twitter for more tips about #coding, #learning, #technology, #Java, #JavaScript, #Autism, #Parenting...etc.Check my Apps on Google Play"
https://dev.to/guthakiran/building-a-cluster-using-elasticsearch-kibana-zookeeper-kafka-and-rsyslog-17ja,"Building a Cluster Using Elasticsearch, Kibana, Zookeeper, Kafka and Rsyslog",2018-06-25T12:09:02Z,Kiran Gutha,"Here in this tutorial you will learn about how to build a cluster by using elasticsearch, kibana, zookeeper, kafka and rsyslog. Developers can face many difficult situation when building a cluster, here we clearly explained step by step procedure to create a cluster.Environmental difficulties: 　　1. The developer cannot log in to the server 　　2. Each system has a log, the log data is scattered and difficult to find 　　3 large amount of log data, the query is busy, can not be real-timeEnvironmental requirements: 　　1. The log needs to be standardizedRequired software for building this cluster: logstash-2.0.0.tar.gz Elasticsearch-2.1.0.tar.gz Jdk-8u25-x64.rpm Kafka_2.11-0.11.0.1.tgz Kibana-4.3.2-linux-x64.tar.gzThe above software can be downloaded from the official website: https://www.elastic.co/downloadsClose the firewall and close selinux (production environment is turned off or on as needed) Synchronize server time, select public network ntpd server or self-built ntpd server[root@es1 ~]# crontab -l # To facilitate the direct use of the public network server*/5 * * * * /usr/bin/rdate -s time-b.nist.gov &>/dev/nullThe elk runtime needs the jvm environment. The 2.x version needs the oracale JDK 1.7 or open-jdk1.7.0 version. The 5.X version requires the oracale JDK 1.8 or the open-jdk1.8.0 version. The multi-node JDK version ensures that it always contains minor versions. No., otherwise it may be an error when joining the cluster, which is why the younger brother did not use yum to install the JDK.[root@es1 ~]# rpm -ivh jdk-8u25- x64.rpm # Because 5.X version requires 1.8, in order to upgrade later to install 1.8 directly Preparing... ########################################### [100%] 1:jdk1.8.0_131 ########################################### [100%][root@es1 ~]# cat /etc/profile.d/ java.sh #Edit the Java environment configuration file export JAVA_HOME=/usr/java/latest export CLASSPATH=$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH [root@es1 ~]# . /etc/profile.d/java.sh [root@es1 ~]# java - version #Confirm Configuration java version ""1.8.0_131"" Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)[root@es1 ~]# tar xf elasticsearch-2.1.0.tar.gz -C /usr/local/ [root@es1 ~]# cd /usr/local/ [root@es1 local]# ln -sv elasticsearch-2.1.0 elasticsearch ""elasticsearch"" -> ""elasticsearch-2.1.0"" [root@es1 local]# cd elasticsearch [root @ es1 elasticsearch] # vim config / elasticsearch.yml [root@es1 elasticsearch]# grep ""^[a-Z]"" config/elasticsearch.yml Cluster.name: pwb - cluster #Cluster name, which must be configured in the same cluster Node.name: pwb - node1 # Cluster node name, unique within the cluster Path.data: /Data/es/ data #data directory Path.logs: /Data/es/ logs #log directory bootstrap.mlockall: true network.host: 10.1.1.243 http.port: 9200 discovery.zen.ping.unicast.hosts: [""10.1.1.243"", ""10.1.1.244""] discovery.zen.minimum_master_nodes: 1[root@es1 elasticsearch]# mkdir -pv /Data/es/{data,logs} Mkdir: created directory "" /Data "" mkdir: created directory "" /Data/es "" mkdir: created directory "" /Data/es/data "" mkdir: created directory "" /Data/es/logsCreate a normal user and suggest adding the appropriate sudo permissions [root@es1 elasticsearch]# useradd elasticsearch [root@es1 elasticsearch]# chown -R elasticsearch:elasticsearch /Data/es/ [root@es1 elasticsearch]# chown -R elasticsearch:elasticsearch /usr/local/elasticsearch-2.1.0/[root@es1 elasticsearch]# echo ""elasticsearch hard nofile 65536"" >> /etc/security/limits.conf [root@es1 elasticsearch]# echo ""elasticsearch soft nofile 65536"" >> /etc/security/limits.conf [root@es1 elasticsearch]# sed -i 's/1024/2048/g' /etc/security/limits.d/90-nproc.conf [root@es1 elasticsearch]# echo ""vm.max_map_count=262144 "" >> /etc/sysctl.conf [root@es1 elasticsearch]# sysctl -p[root@es1 elasticsearch]# grep "" ES_HEAP_SIZE= "" "" bin/ elasticsearch # Set elasticsearch memory size, in principle, the bigger the better, but do not exceed 32G Export ES_HEAP_SIZE =100m # The test environment has limited memoryThe configuration of other nodes is the same as that of the other nodes.Network.host: 10.1 . 1.243 # Local IP AddressNode.name: pwb -node1 # The assigned node name[root@es1 elasticsearch]# su - elasticsearch [elasticsearch@es1 ~]$ cd /usr/local/elasticsearch [elasticsearch@es1 elasticsearch]$ bin/elasticsearch&Through the output you can see the service startup and add other nodes in the cluster through auto discovery.Check if the service is normal[root@es1 ~]# netstat -tnlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 944/sshd Tcp 0 0 ::ffff: 10.1 . 1.243 : 9300 :::* LISTEN 3722 / java Communication between # ES nodes tcp 0 0 :::22 :::* LISTEN 944/sshd Tcp 0 0 ::ffff: 10.1 . 1.243 : 9200 :::* LISTEN 3722 / java # ES node and external communication use [root@es1 ~]# curl http: // 10.1.1.243:9200/ # If the following message appears, the installation and configuration is successful { "" name "" : "" pwb-node1 "" , "" cluster_name "" : "" pwb-cluster "" , "" version "" : { "" number "" : "" 2.1.0 "" , "" build_hash "" : "" 72cd1f1a3eee09505e036106146dc1949dc5dc87 "" , ""build_timestamp"" : ""2015-11-18T22:40:03Z"", ""build_snapshot"" : false, ""lucene_version"" : ""5.3.1"" }, ""tagline"" : ""You Know, for Search"" }[root@es1 ~]# /usr/local/elasticsearch/bin/plugin install mobz/elasticsearch-headAfter the installation is complete, visit the URL http://10.1.1.243:9200/_plugin/head/. Since there is no data in the cluster for the moment, the display is empty (the five-pointed star indicates the master node, and the dot indicates the data node).Other commonly used plug-in installation methods (not demonstrated here, are interested in their own installation)./bin/plugin install lukas-vlcek/bigdesk # 2 .0 version change commands . /bin/plugin install hlstudio/bigdesk # 2 .0 or later Use this command to install , / bin / plugin install lmenezes / elasticsearch-head / versionLogstash needs to rely on the java environment, so here we still need to install the JVM, this step is omitted[root@logstash1 ~]# tar xf logstash-2.0.0.tar.gz -C /usr/local/ [root@logstash1 ~]# cd /usr/local/ [root@logstash1 local]# ln -sv logstash-2.0.0 logstash ""logstash"" -> ""logstash-2.0.0"" [root@logstash1 local]# cd logstash [root@logstash1 logstash]# grep ""LS_HEAP_SIZE"" bin/logstash.lib.sh LS_HEAP_SIZE = "" ${LS_HEAP_SIZE:=100m} "" # Set memory size to use(1) Write a logstash configuration file[root@logstash1 logstash]# cat conf/messages.conf input { File { # data input using input file plugin, read from messages file path => ""/var/log/messages"" } } output { Elasticsearch { # data output points to ES cluster Hosts => [ "" 10.1.1.243:9200 "" , "" 10.1.1.244:9200 "" ] # ES Node Host IP and Port } } [root@logstash1 logstash]# /usr/local/logstash/bin/logstash -f conf/messages.conf --configtest --verbose Configuration OK [root@logstash1 logstash]# /usr/local/logstash/bin/logstash -f conf/messages.conf Default settings used: Filter workers: 1 Logstash startup completed(2) Write some files to message, we install some software[root@logstash1 log]# yum install httpd -yCheck the changes in the messages file[root@logstash1 log]# tail /var/log/messages Oct 24 13:44:25 localhost kernel: ata2.00: configured for UDMA/33 Oct 24 13:44:25 localhost kernel: ata2: EH complete Oct 24 13:49:34 localhost rz[3229]: [root] logstash-2.0.0.tar.gz/ZMODEM: error: zgethdr returned 16 Oct 24 13:49:34 localhost rz[3229]: [root] logstash-2.0.0.tar.gz/ZMODEM: error Oct 24 13:49:34 localhost rz[3229]: [root] no.name/ZMODEM: got error Nov 8 22:21:25 localhost rz[3245]: [root] logstash-2.0.0.tar.gz/ZMODEM: 80604867 Bytes, 2501800 BPS Nov 8 22:24:27 localhost rz[3248]: [root] jdk-8u25-x64.rpm/ZMODEM: 169983496 Bytes, 1830344 BPS Nov 8 22:50:49 localhost yum[3697]: Installed: apr-util-ldap-1.3.9-3.el6_0.1.x86_64 Nov 8 22:50:50 localhost yum[3697]: Installed: httpd-tools-2.2.15-60.el6.centos.6.x86_64 Nov 8 22:51:07 localhost yum[3697]: Installed: httpd-2.2.15-60.el6.centos.6.x86_64Visit web page for elasticsearch head plugin. It has been seen that logstash can be normally written to the elasticsearch cluster and the logstash configuration is completed (the other nodes are configured the same).When building a kafka cluster, you need to install the zookeeper cluster in advance. You can install it separately or you can use the kafka to install the zookeeper program. Choose the zookeeper program that comes with kafka.[root@kafka1 ~]# tar xf kafka_2.11-0.11.0.1.tgz -C /usr/local/ [root@kafka1 ~]# cd /usr/local/ [root@kafka1 local]# ln -sv kafka_2.11-0.11.0.1 kafka ""kafka"" -> ""kafka_2.11-0.11.0.1"" [root@kafka1 local]# cd kafka[root@kafka1 kafka]# grep ""^[a-Z]"" config/zookeeper.properties dataDir=/Data/zookeeper clientPort=2181 maxClientCnxns=0 tickTime=2000 initLimit=20 syncLimit=10 server.1=10.1.1.247:2888:3888 server.2=10.1.1.248:2888:3888tickTime: This time is used as the interval between heartbeats maintained by ZooKeeper servers or between the client and the server. That is, a heartbeat is sent every tickTime.Port 2888: indicates the port where this server exchanges information with the leader server in the cluster.port: indicates that if the leader server in the cluster is hung up, a port is required to re-election and a new leader is selected. This port is the port through which the servers communicate with each other during the election.[root@kafka1 kafka]# mkdir -pv /Data/zookeeper Mkdir: created directory "" /Data "" mkdir: created directory "" /Data/zookeeper "" [root@kafka1 kafka]# echo "" 1 "" > /Data/zookeeper/myid # myid file, the contents of which are numbers for Identify the host, if this file does not, zookeeper can not start[root@kafka1 kafka]# grep ""^[a-Z]"" config/server.properties broker.id = 1 # unique in the Listeners = PLAINTEXT: // 10.1.1.247:9092 # server IP address and port num.network.threads = 3 num.io.threads = 8 socket.send.buffer.bytes = 102400 Socket.Receive .buffer.bytes = 102400 socket.request.max.bytes = 104857600 log.dirs = / the Data / kafka- logs need to create in advance # Num.partitions = 10 # need to be configured larger, sharding affects write and read speeds num.recovery.threads.per.data.dir = 1 offsets.topic.replication.factor = 1 transaction.state.log.replication.factor = 1 Transaction.state.log.min.isr = 1 log.retention.hours = 168 # expiration time log.segment.bytes = 1073741824 log.retention.check.interval.ms = 300000 zookeeper.connect = 10.1 . 1.247 : 2181 , 10.1 . 1.248 : 2181 # zookeeper server IP and port zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0The other nodes have the same configuration except the following:( 1 ) Zookeeper configuration echo "" x "" > /Data/zookeeper/ myid # Unique ( 2 ) Configuration of Kafka Broker.id =1 # unique host.name = local IP[root@kafka1 kafka]# /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &The following two stations perform the same operation. During the startup process, the following error message appearsjava.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:579) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562) at org.apache.zookeeper.server.quorum.QuorumCnxManager.toSend(QuorumCnxManager.java:538) at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.process(FastLeaderElection.java:452) at org.apache.zookeeper.server.quorum.FastLeaderElection$Messenger$WorkerSender.run(FastLeaderElection.java:433) at java.lang.Thread.run(Thread.java:745) [2018-06-05 23:44:36,351] INFO Resolved hostname: 10.1.1.248 to address: /10.1.1.248 (org.apache.zookeeper.server.quorum.QuorumPeer) [2018-06-05 23:44:36,490] WARN Cannot open channel to 2 at election address /10.1.1.248:3888 (org.apache.zookeeper.server.quorum.QuorumCnxManager) java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:579) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614) at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)Since the zookeeper cluster is started, each node tries to connect to other nodes in the cluster. The first startup is certainly not connected to the following nodes. Therefore, the exceptions in the previous section of the log are negligible. As you can see from the latter part, the cluster is finally stable after selecting a leader. Other nodes may also have similar conditions, which are normal[rootkafka1 ~]# netstat -nlpt | grep -E ""2181|2888|3888"" tcp 0 0 :::2181 :::* LISTEN 33644/java tcp 0 0 ::ffff:10.1.1.247:3888 :::* LISTEN 33644/java[root@kafka2 ~]# netstat -nlpt | grep -E ""2181|2888|3888"" tcp 0 0 :::2181 :::* LISTEN 35016/java Tcp 0 0 ::ffff: 10.1 . 1.248 : 2888 :::* LISTEN 35016 / java # which is the leader, then he has 2888 ports tcp 0 0 ::ffff:10.1.1.248:3888 :::* LISTEN 35016/java/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &If you have the following error[2018-06-05 23:52:30,323] ERROR Processor got uncaught exception. (kafka.network.Processor) java.lang.ExceptionInInitializerError at kafka.network.RequestChannel$Request.(RequestChannel.scala:124) at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:518) at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:511) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at kafka.network.Processor.processCompletedReceives(SocketServer.scala:511) at kafka.network.Processor.run(SocketServer.scala:436) at java.lang.Thread.run(Thread.java:745) Caused by: java.net.UnknownHostException: kafka2.example.com: kafka2.example.com: Unknown name or service at java.net.InetAddress.getLocalHost(InetAddress.java:1473) at kafka.network.RequestChannel$.(RequestChannel.scala:43) at kafka.network.RequestChannel$.(RequestChannel.scala) ... 10 more Caused by: java.net.UnknownHostException: kafka2.example.com: Unknown name or service at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293) at java.net.InetAddress.getLocalHost(InetAddress.java:1469) ... 12 moreEdit the hosts file and add 127.0.0.1 to resolve the current host name [root@kafka1 ~]# cat /etc/hosts 127.0.0.1 kafka1.example.com localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6Start zookeeper and kafka on other nodes After the startup is complete, perform some tests[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost: 2181 --replication-factor 2 --partitions 1 -- topic summer # NOTE :factor size cannot More than the number of brokers, otherwise an error occurs. The current cluster broker value is 2 Created topic ""summer"".[root @ kafka1 ~] /usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 10.1 . 1.247 : 2181 # cluster list all Topic Summer[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper 10.1.1.247:2181 --topic summer Topic:summer PartitionCount:1 ReplicationFactor:2 Configs: Topic: summer Partition: 0 Leader: 2 Replicas: 2,1 Isr: 2,1The # Replicas copy exists on top of the broker id 2, 1 .[rootkafka1 ~]# /bin/bash /usr/local/kafka/bin/kafka-console-producer.sh --broker-list 10.1 . 1.247 : 9092 -- topic summerHello,MR.John #Enter something, Enter[root@kafka2 kafka]# /usr/local/kafka/bin/kafka-console-consumer.sh --zookeeper 10.1.1.247:2181 --topic summer --from-beginning Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of zookeeper. Hello,MR.JohnIf you can receive the message from the producer as above, then the kafka-based zookeeper cluster is successful.[root@log-client1 ~]# rsyslogd -v rsyslogd 5.8.10, compiled with:Rsyslog support for kafka is provided after v8.7.0 release, all need to upgrade the system rsyslog versionWget http://rpms.adiscon.com/v8-stable/rsyslog.repo -O /etc/yum.repos.d/rsyslog.repo # Download the yum source yum　update rsyslog -y 　　 Yum install rsyslog -kafka -y # install rsyslog- kafka module Ll /lib64/rsyslog/ omkafka.so # Check whether the module is installed /etc/init.d/rsyslog restart # Restart the serviceCheck for updated version[root@log-client1 yum.repos.d]# wrsyslogd -v rsyslogd 8.30.0, compiled with:(1) Edit the rsyslog configuration file[root@log-client1 yum.repos.d]# cat /etc/rsyslog.d/nginx_kafka.confmodule(load=""omkafka"") module(load=""imfile"")template(name=""nginxAccessTemplate"" type=""string"" string=""%hostname%<-+>%syslogtag%<-+>%msg%\n"")ruleset(name=""nginx-kafka"") { # Log forwarding kafka action ( type=""omkafka"" template=""nginxAccessTemplate"" confParam=[""compression.codec=snappy"", ""queue.buffering.max.messages=400000""] partitions.number=""4"" topic=""test_nginx"" broker=[""10.1.1.247:9092"",""10.1.1.248:9092""] queue.spoolDirectory=""/tmp"" queue.filename=""test_nginx_kafka"" queue.size=""360000"" queue.maxdiskspace=""2G"" queue.highwatermark=""216000"" queue.discardmark=""350000"" queue.type=""LinkedList"" queue.dequeuebatchsize=""4096"" queue.timeoutenqueue=""0"" queue.maxfilesize=""10M"" queue.saveonshutdown=""on"" queue.workerThreads=""4"" ) }input(type=""imfile"" Tag=""nginx,aws"" File=""/var/log/nginx/access.log"" Ruleset=""nginx-kafka"")Test conf file for syntax error[root@log-client1 yum.repos.d]# rsyslogd -N 1 rsyslogd: version 8.30.0, config validation run (level 1), master config /etc/rsyslog.conf rsyslogd: End of config validation run. Bye.Restart rsyslog after the test is complete. Otherwise, the configuration does not take effect.[root@log-client1 yum.repos.d]# /etc/init.d/rsyslog restartStart nginx, add two test pages, accessSwitch to kafka cluster server and check the topic list[root@localhost ~]# /usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 10.1.1.247:2181 summer Test_nginxYou can see that in addition to the summer created by the previous test, an additional test_nginx topic is configured.[root@logstash1 ~]# cat /usr/local/logstash/conf/test_nginx.conf input { kafka{ zk_connect => ""10.1.1.247:2181,10.1.1.248:2181"" # kafka group_id => ""logstash"" topic_id => ""test_nginx"" reset_beginning => false consumer_threads => 5 decorate_events => true } } output { elasticsearch { hosts => [""10.1.1.243:9200"",""10.1.1.244:9200""] # elasticsearch index => ""test-nginx-%{+YYYY-MM}"" } }Test grammar[root@logstash1 ~]# /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/test_nginx.conf -t Configuration OKStart the service, the remaining nodes also start the service[root@logstash1 log]# /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/test_nginx.conf Default settings used: Filter workers: 1 Logstash startup completedSwitch to the ES cluster node es1.example.com to see:[root@es1 ~]# curl -XGET '10.1.1.243:9200/_cat/indices?v&pretty' health status index pri rep docs.count docs.deleted store.size pri.store.size green open logstash-2017.11.08 5 1 4 0 40.7kb 20.3kb green open test-nginx-2017-11 5 1 1 0 12.5kb 6.2kbAs you can see, the test-nginx index already has. In the web interface to access the head plugin, retry the test, use forced refresh. Visit the head plugin web interface, the latest visit to two records has come out.Install kibana on ES cluster nodes[root@es1 ~]# tar xf kibana-4.2.1-linux-x64.tar.gz -C /usr/local/ [root@es1 ~]# cd /usr/local/ [root@es1 local]# ln -sv kibana-4.2.1-linux-x64/ kibana ""kibana"" -> ""kibana-4.2.1-linux-x64/"" [root@es1 local]# cd kibana [root@es1 kibana]# grep "" ^[aZ] "" config/ kibana.yml # Configure the host port to have the elasticsearch server IP address and port server.port: 5601 server.host: ""0.0.0.0"" elasticsearch.url: ""http://10.1.1.243:9200""Now let’s open the kibana, configure an index and create a view to test the data.Conclusion: That’s all for now, here we learned about how to build a cluster using ELK, Zookeeper, Kafka and Rsyslog. Let’s make your hands dirty, if you have any questions regarding this post please drop your comment in the below comment section. Happy learning.Author Bio: Kiran GuthaThe author has an experience of more than 6 years of corporate experience in various technology platforms such as Big Data, AWS, Data Science, Machine Learning, Linux, Python, SQL, JAVA, Oracle, Digital Marketing etc. He is a technology nerd and loves contributing to various open platforms through blogging. He is currently in association with a leading professional training provider, Mindmajix Technologies INC. and strives to provide knowledge to aspirants and professionals through personal blogs, research, and innovative ideas."
https://dev.to/presto412/hyperledger-fabric-transitioning-from-development-to-production-4dch,Hyperledger Fabric: Transitioning from Development to Production,2018-06-15T15:25:53Z,Priyansh Jain,"You’ve set up your development environment. Designed your chaincode. Set up the client. Made some decent looking UI as well. Everything works fine locally. All of your tests pass, and all the services are up and running.All this, and you still aren’t satisfied. You want to do more. You want to scale the network. Simulate a production level environment. Maybe you even want to deploy to production. You give it an attempt. Add multiple orderers. Add more organizations with their own peers and CAs. You try testing it with multiple machines.This is where everything you’ve done fails, no matter what you try. You somehow debug everything and come up with some nifty hacks you’re not entirely proud of. Some parts work, some don’t. You wind up the day, still not satisfied.I don’t know if you can relate to this. I went through this phase, racking my head and taking multiple cups of coffee trying to clear my head and starting the debugging from scratch. Multiple facepalms and “ah, this is how this works” later, I decided to write this article, and be your friendly neighbourhood developer-man(sorry).Fabric provides two consensus protocols for the network - Solo, and Kafka-Zookeeper. If you’ve only been working with Solo mode(configurable in configtx.yaml), this is where you change it. When we want our network to be shared among more than 3 peers, it makes sense to have multiple orderers, in the case where one particular, cursed node goes down. Now by design, orderers are like the postmen in a Fabric network. They fetch and relay the transactions to the peers. Solo mode requires all our orderers to be up, and if one goes down the entire network goes down. So here, the trick is to use a Kafka based ordering service. I’ve comprehensively explained how it works in Fabric here.Fabric docs provide some great best practices to follow.So here goes our first step - Solo to Kafka. Don’t forget to specify the brokers in your transaction config files. A sample has been provided below.configtx.yamlNow that we have our orderers’ dependencies resolved, let’s dive into some network level tips.All the images that we use for Hyperledger Fabric are docker images, and the services that we deploy are dockerized. To deploy to production, we’ve two choices. Well, we have a lot of choices, but I’ll only describe the ones that might just not get you fired. For now.The two choices, are Kubernetes and Docker Swarm. I decided to stick with Docker Swarm because I didn’t really like the hacky docker-in-docker set up for the former. More on this here.From the Docker docs,“A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services). A given Docker host can be a manager, a worker, or perform both roles”.It is a cluster management and orchestration method, that is featured in Docker Engine 1.12.So consider this - the blockchain administrator can have access to these manager nodes, and each potential organization can be a designated worker node. This way, a worker node will only have access to the allowed resources, and nothing gets in the way. Swarm uses the raft consensus algorithm, and the managers and workers accomodate replication of services, thus providing crash tolerance.Not giving hostnames to your services is a rookie mistake. A hostname for every service is necessary. This is how one service in the Swarm maps the IP address to the host name. This is used extensively for Zookeeper ensembles.Consider 3 Zookeeper instances. We would have these depend on each other for synchronization. When these are deployed to a swarm, the Swarm gives an internal IP address, which is pretty dynamic.Another Zookeeper instance that would depend on this,Usually, inside a Swarm, the services are distributed to any node by default. Docker Swarm will always try to improve the performance on the manager nodes. Therefore, it will try to distribute the services to the worker nodes. When a worker node goes down, the manager can redistribute the services inside the swarm.Now there are some very important services that we would like to keep afloat, like the Kafka-Zookeeper ensemble, since they synchronize transactions among all the orderers. Hence, what we would like to do here is make sure that we don’t suffer any downtime. There also may be a service that holds certificates, and it is important that the certificates don’t leak in the Swarm network. Therefore, we need restrictions on stack deployment in the Swarm.We can constrain the services deployed to nodes in the swarm. A simple example is shown below.Note that the deploy section defines the crash handling as well, so use it to your benefit. When we implement constraints, almost every authentication issue can be resolved. You might observe that this defeats the purpose of docker Swarm, where it is supposed to maintain the state of the Swarm as much as possible. If such is the case, you’ll have to spend more on another node that can handle downtime and probably extend your constraints.Certificates are not supposed to be disclosed to other entities in a network. Ideally, the easiest thing to do, is to supply the certificates of a particular organization to the node hosting it, and install to a location that is common to both a docker container and a simple linux machine, like /var/<network-name>/certs. This way, you mount only the required volumes. Speaking of volumes, when mounting, be sure to have absolute paths. You can only deploy services to a Docker Swarm from a manager node, and hence it needs to have the certificates at the location in a node hosting it, else the service will shut down.An example:docker-compose-peer.ymlThe /var/network/certs/ directory should be copied in the host worker node before deploying the service.I don’t recommend a standalone service floating in the swarm, that anyone can access.Naming the servicesdocker stack deploy doesn’t allow certain characters for service creation. If you worked directly from the tutorials, you would have the service names like peer0.org1.example.com, orderer0.example.com.So it is better to name them as peer0_org1 , and orderer0 and so on.Docker Swarm usually prefixes the stack name to the service, and the suffix is a SHA256 hash. Hence, to execute any commands, we need the name of the services, given to them by the Swarm. So for example, if you’ve named your service peer0_org1, and the stack you’ve deployed it to is deadpool, the name that swarm will give it would look like deadpool_peer0_org1.1.sa213adsdaa…..You can fetch its name by a simple command,PRO TIP: Environment variables are your best friends. Do have a dedicated .env for all your scripts.When you have multiple organizations, and you want custom channels to run amongst these, and install different smart contracts on each channel, this is how it should work.So if you’re able to invoke and query successfully, you should be good to go.Document your configuration well, create some scripts that save time, like ssh-ing into the services and executing the above commands from a manager node itself.Attach your client to the services above, and enjoy your Production-level Fabric Network set up. Try deploying your network to the IBM cloud, or AWS.If you have any queries or suggestions, do comment below."
https://dev.to/goaty92/designing-tinyurl-it-s-more-complicated-than-you-think-2a48,Designing TinyURL: it's more complicated than you think,2020-08-10T10:21:05Z,goaty92,"Recently I came across a Youtube video called: System Design : Design a service like TinyUrl, from the channel Tushar Roy - Coding Made Simple. This video discusses a common developer interview question, namely, how do you design a service like TinyURL, which allows users to turn long URLs into short ones that are just several characters long. Basically, a TinyURL-like service would have 2 main APIs: createShort(longUrl) and getLong(shortUrl). The second one is easy, you simply need to do a lookup and return the long URL (or 404 if none exists). The main problem is the createShort() API: How do you generate a short sequence of characters that is unique among URLs (note that uniqueness is an important property, we don't want different URLs to have the same shortcut).Tushar's proposed solutions are quite good and I think most interviewers would be satisfied with them (please watch the video before continuing to read this post). That being said, they are sort of unsatisfying. To summarize, the most sophisticated solution proposed in the video is to partition all possible short sequences into ranges, and use a set of servers to return a monotonically increasing sequence, which falls within a range. Each server would be in assigned only one particular range to work with, and Apache Zookeeper is used to coordinate the sequence range assignments. If the each server has a unique range, then they are guaranteed to generate unique sequences. The reason I think this answer is unsatisfying is because, while it works, it simply shifts the responsibility of generating the ""unique"" part of the sequence, which, is the hardest part of the problem, to Zookeeper. Instead of answering the question ""how to generate a unique sequence?"" (or sequence range, in this case), this solution simply says ""I'll just ask Zookeeper to give me one"". But how does Zookeeper do that?First of all, why is it so hard to generate a unique sequence? Afterall, I can use a single computer to keep increasing a counter, and that would be unique, right? In fact, that solution is mentioned by Tushar in the video, but later rejected, because the counter-generating server might fail (either the machine itself crashes, or the network might go down etc.), and Zookeeper, somehow, magically provides ""high availability"" (i.e. it is resilient to failures).And that's the gist of the problem. If I had the guarantee that my servers never fails, then I wouldn't need Zookeeper. I probably wouldn't need multiple servers either, one beefy machine might be enough to do the job. Unfortunately, in practice machines do fail, and in fact, they fail all the time. That is why when we design systems, we design for failure. In this case, when one servers in the Zookeeper cluster fails, somehow the system needs to make sure that the others don't return a duplicate range. The only way to do that is to make all servers agree on which ranges have been given, and which have not.So let's try to simplify & generalize the problem: given a set of servers, how do we ensure that all servers agree on a value, even if the servers might fail randomly (the value in this case would be the range assignment). This is know as the distributed consensus problem, which actually is one of the hardest problems in Distributed systems. In fact, it has been mathematically proven that, in an asynchronous system (meaning a system where we don't know how long it takes for messages to travel between servers), there is NO way to guarantee distributed consensus. This is known as the FLP Impossibility.Fortunately, in most of the systems in practice, we can workaround this issue by modelling them as ""partially synchronous systems"", that is, we can apply a boundary on how long it takes to send messages between servers. And in this model, consensus is possible. There are several algorithms that can be used to get consensus, like Paxos or Raft. Zookeeper itself uses a consensus protocol called Zab (which stands for Zookeeper atomic broadcast).I won't get into details on how these algorithms work. Afterall, they are quite complicated and sometimes difficult to understand. However if you ever need to work with those directly, an important thing to pay attention to is that they are not perfect. Raft and Paxos, for example, only works if the number of failed nodes is less than half the total number of nodes in the system. Failure also take different forms, and while Paxos and Raft works well with Fail-stop and Fail-safe types of failure, Byzantine-type failures are a lot harder to deal with."
https://dev.to/dihfahsih1/9-best-python-frameworks-for-building-small-to-enterprise-applications-2jla,9 Best Python Frameworks for Building Small to Enterprise Applications,2020-02-04T17:10:54Z,Mugoya Dihfahsih,"PYTHON IS BOTH A FUN TOY AND A FRIGHTENING FLAMETHROWER. SAME GOES WITH WHAT YOU CAN DO WITH PYTHON.Python is loved by hobbyists, scientists and architects alike.It’s damn easy to get started with, has higher-order abstractions and metaprogramming capabilities to build large and complex systems, and has truck-loads of libraries for doing pretty much anything. Sure, there are limitations when it comes to concurrency and strong typing, but you can work around them.In this article, we’ll cast a look at some of the best Python frameworks when it comes to building web applications large and small.And there’s a good reason for that. Django is, as the tagline says, “a web framework for perfectionists with deadlines.” It’s what is called a “batteries included” framework (much like how Python is a batteries-included language), which provides all common functionality out of the box.With these features baked in, Django massively cuts down on development time:A handy and pleasant ORM, with migrations created and applied automatically by the framework. Scaffolding for automatic generation of admin panel based on your models. Support for cookies, sessions, middleware, templates, etc. Security features like XSS prevention, CRSF prevention, etc., are applied automatically. Works with practically all databases out there (it’s easy to find adapters where official support doesn’t exist) First-class support for Geographical data and spatial queries though GeoDjango And much, much more. Suffice it is to say Django is a full-blown, friendly web framework.Is Django for you?Absolutely yes.Django makes excellent sense for all use cases, whether rapid prototyping or planning an enterprise application. The only rough edge you’ll come across is the framework’s structure. Since Django bends over backward to make development fast and easy for you, it imposes its structure (a concept called “convention over configuration”) on the developer, which you may not agree with. For instance, if you want to replace the Django ORM with something else (say, SQL Alchemy), be prepared for surprises.Interested in becoming full stack developer with Django and Python? – Check out this fantastic online course.As opposed to Django, Flask is a “micro-framework,” which means it focuses on getting a few, bare minimum things right, and leaves the rest to you. This “the rest is up to you” can be a source of frustration or delight, depending on what your goals are. For those that know what they’re doing and want to lovingly craft their web applications by choosing components of their choice, Flask is a godsend.Flask offers the following features:Routing, templating, session management, and other useful features. Full support for unit-testing A minimal, pluggable architecture First-class REST support Support for Blueprints, Flask’s unique take on architecture for tiny web applications Choose your packages for ORM, migrations, etc. Flexible application structure — put your files where they make the most sense to you Static file serving WGSI compliant Is Flask for you?As already said, Flask is a minimal web framework, with everything broken up into individual components that you can swap out. If you’re in a hurry to build a prototype, you’ll spend a lot of time making trivial decisions on the database, folder structure, routing, etc., that can prove counter-productive. Flask works best when you’re on to a stable, serious project of medium- to large-scale, especially REST APIs.Bottle strips out even more, to the point where the only dependency is the Python standard library. This means no pip install this or pip install that, though you’d most likely need to before long. Here’s why Bottle stands out for some people:Single-file deployment. Yes, your entire application lives in a single “.py” file. No external dependencies. If you have the right Python version installed, you’re good to go. Supplies its templating engine, which can be swapped out with Jinja2, Mako, or Cheetah. Support for forms, headers, cookies, and file uploads. Built-in web server, which can be easily replaced. Is Bottle for you?If you’re making a really small app (say, less than 500 lines of code) with no special requirements, Bottle might make a lot of sense to you. It’s a complete no-nonsense approach to creating web apps, but in practice, you’ll find you’re more hindered than helped by Bottle. The reason is that the real world is always changing and before you know it. New requirements will be dropped on your head. At that point, putting everything in a single file would become a chore.Also, if you think Bottle and Flask are almost alike, you’re right. Proposals of merging the two date back to 2012, and even Armin, the creator of Flask, agrees with that. However, Marcel, the creator of Bottle, maintains a strict ideological distinction because of the single-file approach and insists that the two remain separate.Zope has several interesting components and features suitable for enterprise application development:A component registering and discovery architecture to configure a large app. ZODB — (the only) object database for Python for storing objects natively. Full-fledged framework and standards for Content Management Systems A set of web application frameworks — the canonical one is still called Zope, although several new frameworks (like Grok) have been built on top of it. Strong standards for software development, release, and maintenance. Is Zope for you?If you’re after a highly structured environment for building really large apps, Zope is good. That said, you’ll run into your fair share of issues as well. While Zope continues to evolve, the community is really small, to the extent that many Python developers haven’t even heard of it. Finding tutorials and extensive documentation is hard, so be prepared to do a lot of digging around (though the community is really helpful!). Also, the Python developers you come across may not want to learn Zope and “dilute” their skill-set.TurboGears has some elegant features, some of which are either not present in popular frameworks (like Django) or are hard to build:First-class support for multiple databases Multi-database transactions Highly modular — start with a single file and scale out as much as you need A powerful ORM (SQLAlchemy, which is more mature and capable than Django’s ORM) Pluggable architecture based on the WSGI specification Built-in support for database sharding A function-driven interface as opposed to deep, rigid object-oriented hierarchies. Is TurboGears for you?If you want to develop happily and want a tested, mature, and robust framework away from the media noise of “awesome, next-gen” and all that, TurboGears is a great fit. It’s highly respected in the community and has complete, extensive documentation. Sure, TurboGears isn’t opinionated, which means initial setup and configuration time can be more, but it’s the ideal framework for enterprise application development.As a result, Web2py takes the zero-dependency approach to the extreme — it has no requirements, nothing to install, and includes a full-featured Web-based editor for development, database management, as well as deployment.You can almost think of it as Android Studio, which is more of a complete environment than just a framework. Some nice features that Web2py has, are:Virtually no learning curve. Minimal core (only 12 objects), which can even be memorized! Pure-Python templating Protection against XSS, CSRF, and other attacks A pleasant and consistent API Is Web2py for you?Web2py is a stable and fun framework, but it’s hard to recommend it against other options like Django, Flask, etc. There are hardly any jobs, and the unit testing story is not great. That said, you might enjoy the code API and the overall experience the framework offers, especially if you’re building REST APIs.While it’s comparable to other microframeworks like Flask, CherryPy boasts of some distinction:It contains a built-in multi-threaded server (something that remains on the wishlist of Flask) The (single) web server can host multiple applications! Serve your application as a WSGI app (to interface with other WSGI apps) or a plain HTTP server (which performs better) First-class support for profiling and unit-testing Runs on PyPy (for the true performance junkies), Jython, and even Android CherryPy does all this, and then the usual you’d expect from a web framework.Is CherryPy for you?If you’re building RESTful services mostly, CherryPy is a much more serious contender than Flask. It’s a decade-old framework that has matured nicely and is suitable for small and large applications alike.Sanic is heavily inspired by Flask, to the extent that it borrowed the route decorators, Blueprints, and other fundamentals hook line and sinker. And they’re not ashamed to admit it. What Sanic brings to the table, if you’re a Flask fan, is true non-blocking I/O to meet the performance levels of a Node application. In other words, Sanic is Flask with async/await support!When compared to CherryPy, Sanic has an incredible performance advantage (just think of how it would fare against Flask!). Check out the following results tested by DataWeave:As you can see, once the concurrency numbers start exceeding 50 per second, CherryPy practically chokes and throws up a high failure rate.Is Sanic for you?While the performance characteristics of Sanic blow everything else out of the water, it may not be the best choice for your next project. The main reason is the lack of asynchronous libraries. The bulk of existing Python tools and libraries were written for the single-threaded CPython version, with no forethought for high concurrency or asynchronous operations. If, for example, your favorite ORM does not support asynchronous operations, the whole point of using Sanic gets defeated.Because of these maturity and availability reasons, we won’t examine any more async frameworks in Python.9.Masonite I came across this framework a while ago and thought it was a step in the right direction. Since then, version 2.0 has been released, and I feel like the time has finally come to give Masonite some love.Simply put, Masonite is the Python version of Laravel (a famous PHP framework, in case you didn’t know). Why does that matter? It matters because Laravel was built on the principles of Ruby on Rails, and together these two frameworks allow non-Ruby devs to experience the “Rails Way” of doing things.Laravel (and to an extent, Rails) developers will feel right at home and would be up and running in literally no time. When I tried Masonite (and I did submit an issue or two, including a bug!), I was able to build REST APIs with exactly zero thinking because my Laravel muscle memory was doing everything.As a batteries-included, full-stack framework, Masonite brings several interesting things to the table:Active-record style ORM Database migrations (which, unlike Django, need to be created by the developer) A powerful IoC Container for dependency injection Own CLI (called “craft”) for scaffolding and running tasks First-class support for unit testing The biggest “rival” for Masonite is Django, as the community is doing its best to market the framework as easy, delightful, and the next big thing. Whether it will surpass Django is something time will tell (if you ask me, it does have a decent shot), but for a discussion comparing the two, see here and here.Is Masonite for you?Masonite is still a baby when compared to Django, so there’s no way it can be recommended over Django. That said, if you’re into the Rails way (or the Laravel way) of doing things, you’d appreciate what Masonite has to offer. It’s ideal for rapidly building prototypes that need everything pre-configured and easy to switch.Conclusion There’s no shortage of Python web frameworks out there, large and small. While you can pick up pretty much anything for a small project, an enterprise application has demands that not many of these frameworks can fulfill. If you ask me, for enterprise development, Django (to an extent), Zope, and TurboGears are what comes to mind. And even among those, I’m inclined towards TurboGears.That said, any architect worth his salt can pick up a microframework and roll out their architecture. And this is pretty much what happens in practice, which explains the success of Flask and similar ideas.If you are a newbie, then this online course would be helpful to learn Python."
https://dev.to/sandipmavani/nodejs-vs-php-52g4,Node.js vs PHP,2018-09-24T06:12:38Z,Sandip Mavani,"There’s no doubt PHP is the most known and commonly used language for server-side scripting. Before Django and Ruby on Rails gained popularity (2005-2006), there was hardly any more suitable option for back-end than PHP. However, the tech world is fastly evolving in the direction of simplicity (“Javascript everywhere”) what used to be the language of the front-end has successfully expanded to the back-end. So now we are facing the popular back-end dilemma “Node.js vs PHP”. Let’s try to solve it together!PHP PHP (Hypertext Preprocessor) was created as a scripting language for back-end web development in 1994. For almost 10 years, it had been the only option for a back-end developer. Although lately, new technologies emerged, PHP wasn’t at a stop as well (the last stable version was released in January 2018). As of the latest statistics of 2018, more than 80% of websites are built with PHP (though some websites are built with more than one back-end language).Node.js Node.js is an open source run-time environment for back-end scripting in Javascript powered by Google’s V8 JS engine. It was created in 2009 and came up with the main advantage — Node.js allows to perform asynchronous programming. It can handle thousands of requests operating on one thread, which means no waiting until the previous command is completed. Although the percentage of websites that are built with Node.js is comparatively low (0,4%), it’s fastly becoming popular among developers.Node.js vs PHP: DifferencesLanguages Starting our Node.js vs PHP discussion, we can’t avoid talking about the specifics of two programming languages Javascript and PHP. In terms of syntax simplicity, PHP wins. Such an essential operation as an interaction with a database requires less and simpler (quite alike to HTML) code in PHP than in Node.js.However, the simplicity of PHP doesn’t come without certain restrictions. On the other hand, some complexity of Node.js is balanced by more possibilities of using various Javascript libraries for two-way asynchronous scripting of client and server.Also, Javascript syntax of Node.js is obviously a reason to choose it for those who come from the front-end side. So a full-stack developer may take an advantage from the code reusability options as a result of scripting both client and server sides in Javascript.Requirements Although PHP has a built-in web server (which is available since PHP 5), it’s not a full-featured one and can be used only for testing purposes. The most commonly used with PHP servers are Apache and NGINX. One need to get it installed and configure (by installing corresponding PHP components) a server before actually start working with PHP.Node.js doesn’t require any external servers. You can get started by installing Node.js. After that, you just need to install http-server packages via npm (which is just a few words of code) to use built-in web server tools. Also, installation of Express.js a Node web framework may help to do more than just trivial handling HTTP requests.When it comes to libraries, both PHP and Node.js have built tools that allow to install and manage them from the command line (Composer in PHP and npm in Node.js).Scalability Comparing Node.js vs PHP scalability, it turns out that both both technologies are scalable. It’s totally possible to build large scalable applications with either PHP or Node.js. Still there’s a difference that lies in the efficiency of building scalable application architecture.PHP is supported across most popular content management systems (such as Drupal, Joomla, WordPress), which makes it an often choice as a tool for building blogs and e-commerce web applications. In contrast, Node.js efficiently serves as a tool for creating scalable dynamic solutions that deal with numerous I/O operations. It’s also possible to scale Node on multi-cores systems, though with more efforts.Handling data When it comes to working with data, the main arm of Node.js is JSON, which is, on one hand, understandable by NoSQL and, on the other hand, is used for passing data on client side. And if early version of PHP had some troubles with JSON, it’s not an issue any more as JSON support is included in the core since PHP 5.2.However, the main advantage of PHP when it comes to working with data is that it’s compatible with all the hosting services. And the latter is very far from the reality for Node.js, which makes a good fit with only a narrow range of hosting services.Node.js vs PHP: SimilaritiesLanguages Both PHP and Javascript are interpreted languages that can be used not only for web programming but general purpose scripting. They require a certain run-time environment to be used for scripting (in the case of Javascript it may be either browser or server, which depends on whether it’s used for client- or server- side scripting).Performance Although Node.js is always highlighted as a high-performative one because of its asynchronous model, PHP has also evolved in this direction. With such libraries as ReactPHP, it has become possible for PHP to be used in event-driven programming as well.On the other hand, most servers function on multi-threading, which brings difficulties to work with Node.js. In such cases, asynchronous programming becomes more of an obstacle as not every intermediate-level programmer has enough expertise in it.Even though Node.js is faster (some Node.js vs PHP benchmarks can be found here), PHP 7 and Node.js are almost at the same level in terms of pure performance. In other words, handling asynchronous I/O operations isn’t something that can make Node.js a winner in Node.js vs PHP performance competition.Conclusion Both PHP and Node.js obviously have their own advantages and disadvantages. However, it doesn’t mean that you can’t build the same applications using either PHP or Node.js. So how can you pick one of them? I believe that the best way to make a choice of Node.js vs PHP 2018 is to pick the one that is most compatible with other technologies you use for developing web applications. It will save your time and resources.Apparently, if you are about to use most of MEAN (MongoDB, Express.js, AngularJS, Node.js) stack tools, numerous Javascript libraries (Angular, React, Backbone, etc.) or develop SPAs (Single Page Applications), Node.js may be a better fit for your project. On the other hand, if you take advantage of the most of LAMP (Linux, Apache, MySQL, PHP) stack technologies or other servers like SQL, Oracle, Postgresql you may need to consider PHP in the first place.Although discussions around Node.js vs PHP don’t seem to cease any soon, the important thing to remember is that there’s nothing unique that you can do only with one of them they are interchangeable. However, you can always orient at the level of development expertise and stack of technologies that are to be used in the process of development."
https://dev.to/josiehall/import-data-from-s3-to-redshift-in-minutes-using-dataform-55g2,Import data from S3 to Redshift in minutes using Dataform,2019-10-10T15:52:10Z,Josie Hall,"Dataform is a powerful tool for managing data transformations in your warehouse. With Dataform you can automatically manage dependencies, schedule queries and easily adopt engineering best practices with built in version control. Currently Dataform integrates with Google BigQuery, Amazon Redshift, Snowflake and Azure Data Warehouse. However, often the “root” of your data is in another external source e.g. Amazon S3. If this is the case and you’re considering using a tool like Dataform to start building out your data stack, then there are some simple scripts you can run to import this data into your cloud warehouse using Dataform.We’re going to talk about how to import data from Amazon S3 to Amazon Redshift in just a few minutes, using the COPY command. This allows you to load data in parallel from multiple data sources. The COPY command can also be used to load files from other sources e.g. Amazon EMR or an Amazon DynamoDB table.An Amazon Web Services (AWS) account. Signing up is free - click here.Permissions in AWS Identity Access Management (IAM) that allow you to create policies, create roles, and attach policies to roles. This is required to grant Dataform access to your S3 bucket.Verified that column names in CSV files in S3 adhere to your destination’s length limit for column names. If a column name is longer than the destination’s character limit it will be rejected. In Redshift’s case the limit is 115 characters.An Amazon S3 bucket containing the CSV files that you want to import.A Redshift cluster. If you do not already have a cluster set up, see how to launch one here.A Dataform project set up which is connected to your Redshift warehouse. See how to do that here.Ok now you’ve got all that sorted, let’s get started! Once you’re in Dataform, create a new .sqlx file in your project under the definitions/ folder. Using Dataform’s enriched SQL this is what the code should look like:Finally, you can push your changes to GitHub and then publish your table to Redshift. Alternatively, you can run this using the Dataform CLI: dataform run.And Voila! Your S3 data is now ready to use in your Redshift warehouse as a table and can be included in your larger Dataform dependency graph. This means you can now run it alongside all other code, add dependencies on top of it (so any datasets that rely on this will only run if it is successful), you can use the ref() or resolve() functions on this dataset in another script and you can document it's data catalog entry using your own descriptions.For more information about how to get setup on Dataform please see our docs."
https://dev.to/ahmetkucukoglu/couchbase-geosearch-with-asp-net-core-i04,Couchbase GeoSearch with ASP.NET Core,2020-05-20T04:26:53Z,Ahmet Küçükoğlu,"This article was originally published at: https://www.ahmetkucukoglu.com/en/couchbase-geosearch-with-asp-net-core/The subject of this article will be about how to do ""GeoSearch"" by using Couchbase.For this purpose, we create Couchbase cluster with the docker by running the command below.When Couchbase is up, it will start broadcasting at the address below.http://localhost:8091/We create the cluster by clicking the ""Setup New Cluster"" button. We define the password as ""123456"".You can make adjustment according to your current memory status. You can turn off ""Analytics"". We complete the cluster installation by clicking the ""Save & Finish"" button.We go to ""Buckets"" from the left menu, click the ""Add Bucket"" button at the top of the right corner and create a bucket called ""meetup"".http://localhost:8091/ui/index.html#!/bucketsAs an example, we will do the events section in the Meetup application that we all use. We will define events through the API. We will record the locations of these events. We will then query a list of events near by the current location.We create an ASP.NET Core API project. We install the nuget packages below.We add Couchbase connection information to the appsettings.json file.We define Couchbase in the Startup.cs file.In the project, we create a folder named ""Models"" and add a class named ""EventDocument"" into it. This class will be the model of the document that we will add to the bucket named ""events"" in Couchbase.Likewise, we add a class named ""CreateEventRequest"" under the ""Models"" folder. It will be the request model of endpoint, which will allow us to record this event.We add a controller named ""EventsController"".Now we can add events to Couchbase. Let's run the application and make requests from Postman as follows.When we look at the documents of events bucket in Couchbase, we will see that it has been added.http://localhost:8091/ui/index.html#!/doc_editor?bucket=eventsIn Couchbase, we come to ""Search"" from the left menu and click the ""Add Index"" button in the upper right corner.http://localhost:8091/ui/index.html#!/fts_new/?indexType=fulltext-index&sourceType=couchbaseWe write ""eventsgeoindex"" in the Name field. We select ""events"" from the Bucket field.Using the ""+ insert child field"" on the right side of the mapping named ""default"" under ""Type Mappings"", we add mappings as follows.We add mapping to save the ""Subject"" information in the event to ""eventsgeoindex"".We add mapping to save the ""Address"" information in the event to ""eventsgeoindex"".We add mapping to save the ""Date"" information in the event to ""eventsgeoindex"".We add mapping to search according to the ""Location"" information in the event.The final situation will be as in the image below.We create the index by clicking the ""Create Index"" button. When ""Indexing progress"" is 100%, it means that indexing has been finished.http://localhost:8091/ui/index.html#!/fts_list?open=eventsgeoindexWe add a class named ""GetUpcomingEventsRequest"" under the ""Models"" folder. This will be the request model of endpoint, which will return events near by our location.Likewise, we add a class named ""GetUpcomingEventsResponse"" under the ""Models"" file. This will be the endpoint's response model, which returns events that are near by our location.We add a controller named ""UpcomingEventsController"". In the line 30, we indicate that the search will be made on the basis of km. In the line 40, we indicate the name of the ""search index"" that we will create in Couchbase.Now we can list events near by our location. Let's run the application and make requests from Postman as follows. By typing 1 and 2 to ""Radius"", you can see the events near 1 or 2km.You can access the final version of the project from Github.Good luck."
https://dev.to/angular/outputting-json-ld-with-angular-universal-4ia1,Outputting JSON-LD with Angular Universal,2019-06-07T18:39:16Z,Angular,"Originally published at https://samvloeberghs.be on March 12, 2019This article and guide on generating JSON-LD with Angular Universal is targeted to developers that want to generate the JSON that represents the Linked Data object and inject it correctly in the HTML that gets generated on the server. We will re-use and explain some techniques that the TransferState key-value cache service uses to transfer server-state to the application on the client side.JSON-LD is a lightweight Linked Data format. It is easy for humans and machines to read and write. It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale.Linked Data empowers people that publish and use information on the Web. It is a way to create a network of standards-based, machine-readable data across Web sites. It allows an application to start at one piece of Linked Data, and follow embedded links to other pieces of Linked Data that are hosted on different sites across the Web.These 2 definitions of JSON-LD and Linked Data were simply taken from the website of JSON-LD. This article does not go deep into the full specification of JSON-LD and it does not teach you how to properly generate your Linked Data structures. The tools of the JSON-ld website, like for example the playground, give you all you need to validate your Linked Data structures.What you can see below is a typical example of a JSON-LD data object, injected into the HTML. It describes a LocalBusiness by following its definition and providing the properties that identifiy the entity. For example, the address, it's geolocation, opening hours etc are given. As you can see the address has it's own type PostalAddress. Most of the types can specificy an external link and this is how data gets interweaved on the same website or resource and even on other websites.You should definitely care about JSON-LD for several reasons:In general; it ads a lot more semantic value to your HTML markup by providing context to what you show on the page and makes this context machine-readable, allowing it to be parsed more easily by search engines and other crawlers on the web.In the simple case of a blog or not too complex website, generating your JSON Linked Data can be done using the same base data you use to set your meta tags for social sharing and SEO. Your most important concern is building the correct structure. How your structure looks is completely dependent on your usecase. As a basic example we will use this website, and more specifically the about page.Using the router we first define the correct SEO data associated with our route. Just like we did before, setting the correct social share meta tags.Using a service we can subscribe to route changes to extract this data and pass the data to a service to parse the JSON-LD object.The JsonLdService injected above is a simple service that caches and updates the data structure we need to output. A basic implementation of this service can be as follows, where you as a developer are still in charge of correctly structuring your object.Using the approach explained above our current JSON-LD data-object in memory will look like this:This basic example is what we wanted to achieve. The next step is getting this object outputted in our static HTML.Injecting the JSON-LD data object in the DOM is only really required when we generate the HTML on the server. To get to this result I looked into the BrowserTransferStateModule(https://github.com/angular/angular/blob/master/packages/platform-browser/src/browser/transfer_state.ts) exposed via the @angular/platform-browser module.Essentialy this module, and more specifically, the TransferState(https://github.com/angular/angular/blob/master/packages/platform-browser/src/browser/transfer_state.ts) service it provides, does the same thing we want to achieve. It caches data, the result from for example HTTP calls, in an object and holds it in memory during the application lifecycle. Apart from the HTTP calls, that's exactly what our JsonLdService does.The server counter part, ServerTransferStateModule(https://github.com/angular/angular/blob/master/packages/platform-server/src/transfer_state.ts) exposed via the @angular/platform-server, serializes the data and injects it in the DOM before generating the HTML on the server and sending it back over the wire to the browser. Again, that is exactly what we want to achieve. This is achieved by providing an extra factory function to the BEFORE_APP_SERIALIZED token. Right after the application becomes stable all factories attached to the BEFORE_APP_SERIALIZED are executed.When the Angular application bootstraps in the browser, the TransferState service picks up the serialized state in the static HTML and unserializes it, making it available as a direct cache. This way we avoid that the application makes a similar request for the same data to the server, for which the server-side-rendered version already did the call. We don't need this part, but it's good to know.Following the concepts and ideas we learned from the ServerTransferStateModule and the BrowserTransferStateModule we create our own ServerJsonLdModule and BrowserJsonLdModule. The only small differences are limited to changing the type of the <script> element and injecting it in the <head> instead of right before the </body> tag. We also don't need to pickup any state from the static HTML, as is done in the BrowserTransferStateModule.To differentiate the execution of our application on the server versus on the browser, in Angular Universal we typically create a new server module app.server.module.ts next to app.module.ts that will directly import the AppModule exported by app.module.ts. This version of the application is used in the HTML renderer configured at the server side, in most cases as a rendering engine.If we go back to the example of the TransferState service, we see that the app.server.module.ts is importing the ServerTransferStateModule. This will overwrite the provider of the JsonLdService imported in the app.module.ts and provide the extra serialize functionality, next to also providing the TransferState service.We do the same thing with our BrowserJsonLdModule and ServerJsonLdModule. The browser module has to be imported in app.module.ts while the server module has to be imported in app.server.module.ts.Generating JSON-LD using Angular Universal is pretty straight-forward. By borrowing concepts from the Angular source code we can create our own JsonLd modules and services that enable us to output JSON-LD in the statically generated HTML on the server. The result of this code can be found live on this website, just look at the source and do a hard refresh. Why a hard refresh you might ask? Because of the service worker caching the default index.html as the app shell.Originally published at https://samvloeberghs.be on March 12, 2019"
https://dev.to/leonardomso/a-beginners-guide-to-graphql-3kjj,A Beginner’s Guide to GraphQL,2019-01-05T11:45:31Z,Leonardo Maldonado,"One of the most commonly discussed terms today is the API. A lot of people don’t know exactly what an API is. Basically, API stands for Application Programming Interface. It is, as the name says, an interface with which people — developers, users, consumers — can interact with data.You can think of an API as a bartender. You ask the bartender for a drink, and they give you what you wanted. Simple. So why is that a problem?Since the start of the modern web, building APIs has not been as hard as it sounds. But learning and understanding APIs was. Developers form the majority of the people that will use your API to build something or just consume data. So your API should be as clean and as intuitive as possible. A well-designed API is very easy to use and learn. It’s also intuitive, a good point to keep in mind when you’re starting to design your API.We’ve been using REST to build APIs for a long time. Along with that comes some problems. When building an API using REST design, you’ll face some problems like:1) you’ll have a lot of endpoints2) it’ll be much harder for developers to learn and understand your API3) there is over- and under-fetching of informationTo solve these problems, Facebook created GraphQL. Today, I think GraphQL is the best way to build APIs. This article will tell you why you should start to learn it today.In this article, you’re going to learn how GraphQL works. I’m going to show you how to create a very well-designed, efficient, powerful API using GraphQL.You’ve probably already heard about GraphQL, as a lot of people and companies are using it. Since GraphQL is open-source, its community has grown huge.Now, it’s time for you start to learn in practice how GraphQL works and all about its magic.GraphQL is an open-source query language developed by Facebook. It provides us with a more efficient way design, create, and consume our APIs. Basically, it’s the replacement for REST.GraphQL has a lot of features, like:You write the data that you want, and you get exactly the data that you want. No more over-fetching of information as we are used to with REST.It gives us a single endpoint, no more version 2 or version 3 for the same API.GraphQL is strongly-typed, and with that you can validate a query within the GraphQL type system before execution. It helps us build more powerful APIs.This is a basic introduction to GraphQL — why it’s so powerful and why it’s gaining a lot of popularity these days. If you want to learn more about it, I recommend you to go the GraphQL website and check it out.The main objective in this article is not to learn how to set up a GraphQL server, so we’re not getting deep into that for now. The objective is to learn how GraphQL works in practice, so we’re gonna use a zero-configuration GraphQL server called ☄️ Graphpack.To start our project, we’re going to create a new folder and you can name it whatever you want. I’m going to name it graphql-server:Open your terminal and type:Now, you should have npm or yarn installed in your machine. If you don’t know what these are, npm and yarn are package managers for the JavaScript programming language. For Node.js, the default package manager is npm.Inside your created folder type the following command:Or if you use yarn:npm will create a package.json file for you, and all the dependencies that you installed and your commands will be there.So now, we’re going to install the only dependency that we’re going to use.☄️Graphpack lets you create a GraphQL server with zero configuration. Since we’re just starting with GraphQL, this will help us a lot to go on and learn more without getting worried about a server configuration.In your terminal, inside your root folder, install it like this:Or, if you use yarn, you should go like this:After Graphpack is installed, go to our scripts in package.json file, and put the following code there:We’re going to create a folder called src, and it’s going to be the only folder in our entire server.Create a folder called src, after that, inside our folder, we’re going to create three files only.Inside our src folder create a file called schema.graphql. Inside this first file, put the following code:In this schema.graphql file is going to be our entire GraphQL schema. If you don’t know what it is, I’ll explain later — don't worry.Now, inside our src folder, create a second file. Call it resolvers.js and, inside this second file, put the following code:This resolvers.js file is going to be the way we provide the instructions for turning a GraphQL operation into data.And finally, inside your src folder, create a third file. Call this db.js and, inside this third file, put the following code:In this tutorial we’re not using a real-world database. So this db.js file is going to simulate a database, just for learning purposes.Now our src folder should look like this:Now, if you run the command npm run dev or, if you’re using yarn, yarn dev, you should see this output in your terminal:You can now go to localhost:4000. This means that we’re ready to go and start writing our first queries, mutations, and subscriptions in GraphQL.You see the GraphQL Playground, a powerful GraphQL IDE for better development workflows. If you want to learn more about GraphQL Playground, click here.GraphQL has its own type of language that’s used to write schemas. This is a human-readable schema syntax called Schema Definition Language (SDL). The SDL will be the same, no matter what technology you’re using — you can use this with any language or framework that you want.This schema language its very helpful because it’s simple to understand what types your API is going to have. You can understand it just by looking right it.Types are one of the most important features of GraphQL. Types are custom objects that represent how your API is going to look. For example, if you’re building a social media application, your API should have types such as Posts, Users, Likes, Groups.Types have fields, and these fields return a specific type of data. For example, we’re going to create a User type, we should have some name, email, and age fields. Type fields can be anything, and always return a type of data as Int, Float, String, Boolean, ID, a List of Object Types, or Custom Objects Types.So now to write our first Type, go to your schema.graphql file and replace the type Query that is already there with the following:Each User is going to have an ID, so we gave it an ID type. User is also going to have a name and email, so we gave it a String type, and an age, which we gave an Int type. Pretty simple, right?But, what about those ! at the end of every line? The exclamation point means that the fields are non-nullable, which means that every field must return some data in each query. The only nullable field that we’re going to have in our User type will be age.In GraphQL, you will deal with three main concepts:queries — the way you’re going to get data from the server.mutations — the way you’re going to modify data on the server and get updated data back (create, update, delete).subscriptions — the way you’re going to maintain a real-time connection with the server.I’m going to explain all of them to you. Let’s start with Queries.To explain this in a simple way, queries in GraphQL are how you’re going to get data. One of the most beautiful things about queries in GraphQL is that you are just going to get the exact data that you want. No more, no less. This has a huge positive impact in our API — no more over-fetching or under-fetching information as we had with REST APIs.We’re going to create our first type Query in GraphQL. All our queries will end up inside this type. So to start, we’ll go to our schema.graphql and write a new type called Query:It’s very simple: the users query will return to us an array of one or more Users. It will not return null, because we put in the !, which means it’s a non-nullable query. It should always return something.But we could also return a specific user. For that we’re going to create a new query called user. Inside our Query type, put the following code:Now our Query type should look like this:As you see, with queries in GraphQL we can also pass arguments. In this case, to query for a specific user, we’re going to pass its ID.But, you may be wondering: how does GraphQL know where get the data? That’s why we should have a resolvers.js file. That file tells GraphQL how and where it's going to fetch the data.First, go to our resolvers.js file and import the db.js that we just created a few moments ago. Your resolvers.js file should look like this:Now, we’re going to create our first Query. Go to your resolvers.js file and replace the hello function. Now your Query type should look like this:Now, to explain how is it going to work:Each query resolver has four arguments. In the user function, we’re going to pass id as an argument, and then return the specific user that matches the passed id. Pretty simple.In the users function, we’re just going to return the users array that already exists. It’ll always return to us all of our users.Now, we’re going to test if our queries are working fine. Go to localhost:4000 and put in the following code:It should return to you all of our users.Or, if you want to return a specific user:Now, we’re going to start learning about mutations, one of the most important features in GraphQL.In GraphQL, mutations are the way you’re going to modify data on the server and get updated data back. You can think like the CUD (Create, Update, Delete) of REST .We’re going to create our first type mutation in GraphQL, and all our mutations will end up inside this type. So, to start, go to our schema.graphql and write a new type called mutation:As you can see, we’re going to have three mutations:createUser: we should pass an ID, name, email, and age. It should return a new user to us.updateUser: we should pass an ID, and a new name, email, or age. It should return a new user to us.deleteUser: we should pass an ID. It should return the deleted user to us.Now, go to our resolvers.js file and below the Query object, create a new mutation object like this:Now, our resolvers.js file should look like this:Now, we’re going to test if our mutations are working fine. Go to localhost:4000 and put in the following code:It should return a new user to you. If you want to try making new mutations, I recommend you to try for yourself! Try to delete this same user that you created to see if it’s working fine.Finally, we’re going to start learning about subscriptions, and why they are so powerful.As I said before, subscriptions are the way you’re going to maintain a real-time connection with a server. That means that whenever an event occurs in the server and whenever that event is called, the server will send the corresponding data to the client.By working with subscriptions, you can keep your app updated to the latest changes between different users.A basic subscription is like this:You will say it’s very similar to a query, and yes it is. But it works differently.When something is updated in the server, the server will run the GraphQL query specified in the subscription, and send a newly updated result to the client.We’re not going to work with subscriptions in this specific article, but if you want to read more about them click here.As you have seen, GraphQL is a new technology that is really powerful. It gives us real power to build better and well-designed APIs. That’s why I recommend you start to learn it now. For me, it will eventually replace REST.Thanks for reading the article, please give a comment below!🐦 Follow me on Twitter! ⭐ Follow me on GitHub!"
https://dev.to/azure/learn-how-you-can-build-a-serverless-graphql-api-on-top-of-a-microservice-architecture-233g,"Learn how YOU can build a Serverless GraphQL API on top of a Microservice architecture, part I",2019-04-13T12:22:00Z,Microsoft Azure,"Follow me on Twitter, happy to take your suggestions on topics or improvements /ChrisThe idea with this article is to show how we can build microservices, dockerize them and combine them in a GraphQL API and query it from a Serverless function, how's that for a lot of buzzwords in one? ;) Microservices, Docker, GraphQL, ServerlessThis is part of series:So, it's quite ambitious to create Microservices, Serverless and deploy to the Cloud in one article so this is a two-parter. This is part one. This part deals with Microservices and GraphQL. In part two we make it serverless and deploy it.In this article we will cover:We will throw you in head first using Docker, GraphQL and some Serverless with Azure functions. This article is more of a recipe of what you can do with the above techniques so if you feel you need a primer on the above here is a list of posts I've written:A library and paradigm like GraphQL, is the most useful when it is able to combine different data sources into one and serve that up as one unified API. A developer of the Front end app can then query the data they need by using just one request.Today it becomes more common to break down a monolithic architecture into microservices, thereby you get many small APIs that works independently. GraphQL and microservices are two paradigms that go really well together. How you wonder? GraphQL is really good at describing schemas but also stitch together different APIs and the end result is something that's really useful for someone building an app as querying for data will be very simple.Different APIs is exactly what we have when we have a Microservices architecture. Using GraphQL on top of it all means we can reap the benefits from our chosen architecture at the same time as an App can get exactly the data it needs.How you wonder? Stay with me throughout this article and you will see exactly how. Bring up your code editor cause you will build it with me :)Ok, so what are we building? It's always grateful to use an e-commerce company as the target as it contains so many interesting problems for us to solve. We will zoom in on two topics in particular namely products and reviews. When it comes to products we need a way to keep track of what products we sell and all their metadata. For reviews we need a way to offer our customer a way to review our products, give it a grade, a comment and so on and so forth. These two concepts can be seen as two isolated islands that can be maintained and developed independently. If for example, a product gets a new description there is no reason that should affect the review of said product.Ok, so we turn these two concepts into product service and a review service.What does the data look like for these services? Well they are in their infancy so let's assume the product service is a list of products for now with a product looking something like this:The review service would also hold data in a list like so:As you can see from the above data description the review service holds a reference to a product in the product service and it's by querying the product service that you get the full picture of both the review and the product involved.Ok, so we understand what the services need to provide in terms of a schema. The services also need to be containerized so we will describe how to build them using Docker a Dockerfile and Docker Compose.So the GraphQL API serves as this high-level API that is able to combine results from our product service as well as review service. It's schema should look something like this:We assume that when a user of our GraphQL API queries for Reviews they want to see more than just the review but also some extra data on the Product, what's it called, what it is and so on. For that reason, we've added the product property on the Review type in the above schema so that when we drill down in our query, we are able to get both Review and Product information.So where does Serverless come into this? We need a way to host our API. We could be using an App Service but because our GraphQL API doesn't need to hold any state of its own and it only does a computation (it assembles a result) it makes more sense to make it a light-weight on-demand Azure Function. So that's what we are going to do :) As stated in the beginning we are saving this for the second part of our series, we don't want to bore you by a too lengthy article :)We opt for making these services as simple as possible so we create REST APIs using Node.js and Express, like so:The app.js file for /products looks like this:and the app.js for /reviews looks like this:Looks almost the same right? Well, we try to keep things simple for now and return static data but it's quite simple to add database later on.Before we start Dockerizing we need to install our dependency Express like so:This needs to be done for each service.Ok, we showed you in the directory for each service how there was a Dockerfile. It looks like this:Let's go up one level and create a docker-compose.yaml file, so it's easier to create our images and containers. Your file system should now look like this:Your docker-compose.yaml should have the following content:We can now get our service up and running withI always feel like I'm starting up a jet engine when I run this command as all of my containers go up at the same time, so here goes, ignition :)You should be able to find the products service at http://localhost:8000 and the reviews service at http://localhost:8001. That covers the microservices for now, let's build our GraphQL API next.Your products service should look like the following:and your review service should look like this:There are many ways to build a GraphQL server, we could be using the raw graphql NPM library or the express-graphql, this will host our server in a Node.js Express server. Or we could be using the one from Apollo and so on. We opt for the first one graphql as we will ultimately serve it from a Serverless function.So what do we need to do:Now, this is an interesting one, we have two options here for defining a schema, either use the helper function buildSchema() or use the raw approach and construct our schema using primitives. For this case, we will use the raw approach and the reason for that is I simply couldn't find how to resolve things at depth using buildSchema() despite reading through the manual twice. It's strangely enough easily done if we were to use express-graphql or Apollo so sorry if you feel your eyes bleed a little ;)Ok, let's define our schema first:Above we are defining two types Review and Product and we expose two query fields products and reviews.I want you to pay special attention to the variable reviewType and how we resolve the product field. Here we are resolving it like so:Why do we do that? Well, it has to do with how data is stored on a Review. Let's revisit that. A Review stores its data like so:As you can see above the product field is an integer. It's a foreign key pointing to a real product in the product service. So we need to resolve it so the API can be queried like so:If we don't resolve product to a product object instead the above query would error out.In our schema.js we called methods like getProducts(), getReviews() and getProduct() and we need those to exist so we create a file services.js, like so:Ok, we can see above that methods getProducts() and getReviews() makes HTTP requests to URL, at least judging by the names process.env.PRODUCTS_URL and process.env.REVIEW_URL. For now, we have created a .env file in which we create those two env variables like so:Wait, isn't that? Yes, it is. It is the URLs to product service and review service after we used docker-compose to bring them up. This is a great way to test your Microservice architecture locally but also prepare for deployment to the Cloud. Deploying to the Cloud is almost as simple as switching these env variables to Cloud endpoints, as you will see in the next part of this article series :)Ok, so we need to try our code out. To do that let's create an app.js file in which we invoke the graphql() function and let's provide it our schema and query, like so:In the above code, we specify a query and we expect the fields hello, products and reviews to come back to us and finally we invoke graphql() that on the then() callback serves up the result. The result should look like this:We set out on a journey that would eventually lead us to the cloud. We are not there yet but part two will take us all the way. In this first part, we've managed to create microservices and dockerize them. Furthermore, we've managed to construct a GraphQL API that is able to define a schema that merges our two APIs together and serve that up.What remains to do, that will be the job of the second part, is to push our containers to the cloud and create service endpoints. When we have the service endpoints we can replace the value of environment variables to use the Cloud URLs instead of the localhost ones we are using now.Lastly, we need to scaffold a serverless function but that's all in the next part so I hope you look forward to that :)"
https://dev.to/qainsights/performance-testing-neo4j-database-using-bolt-protocol-in-apache-jmeter-1oa9,Performance Testing Neo4j Database using Bolt Protocol in Apache JMeter,2019-11-08T17:09:37Z,NaveenKumar Namachivayam ⚡,"Apache JMeter 5.2 has been released with lots of new features, enhancements, and bug fixes. JMeter 5.2 supports Bolt protocol out of the box. No need to install it via Plugins Manager. This blog post provides you an overview about performance testing Neo4j database using Bolt protocol in Apache JMeter 5.2First, you need to add the basic elements such as Thread Group, Bolt Connection Configuration, and View Results Tree, then you need to add Bolt Request.In Bolt Connection Configuration, you need to add the Bolt URI with the valid credentials. Default credentials of Neo4j is neo4j/neo4j. At first login, it will prompt you to change the password.Once the connection is set up, next you need to load up the demo data in your Neo4j.Detailed instructions are in here.Bolt communicates over 7687 port.Neo4j can be accessed from your favorite browser using http://localhost:7474.Now, head back to JMeter and enter the below command in the Bolt Request.MATCH (n:Movie) RETURN n LIMIT 25Make sure you set the Record Query Results flag set to true, and then hit run.In View Results Tree, you can see the response as shown below.You cannot configure the pool size of Bolt Connection in this inception implementation.You need to set the Record Query Results set to false for better performance.Here is the complete video demo.This repo has the sample JMeter test plan to demonstrate Bolt request. For detailed instruction, please visit my blogBuy me a tea"
https://dev.to/lirantal/taking-a-look-at-cfps-for-nodetlv-i-ll-see-you-there-right-5hd8,"Taking a look at CFPs for NodeTLV. I'll see you there, right?",2019-12-22T08:11:47Z,Liran Tal,"Did you hear about Node.js? What about Tel Aviv? Any reason why you shouldn't enjoy both of them? I agree. No reason indeed :-)In the summer of 2019, the Israeli Node.js community have announced the first international Node.js conference in Tel Aviv, which will happen on 3/3/2020 🎉 This is the result and the work of the amazing local community in Israel for Node.js and the Node.js IL meetup group.As I was part of the content team, I did a little number crunching on the CFP submissions, and I think it would be interesting to look at the result.The CFP opened on August 2019 and ended in November 2019. In total, NodeTLV received 171 talk proposal applications and as is pretty common with human beings, and perhaps programmers in particular, these said creatures have procrastinated their applications to the very last month before the CFP deadline closes. Odd creatures, engineers are.How bad do programmers procrastinate? Let's find out!Following is a plotted chart of talk proposal by day of the month of being submitted.Y’all just had to wait till the last minute to submit, huh? Typical!We have an amazing speaker line-up for you with a mix from both local community heroes as well as internationally recognized speakers and stewards of the Node.js ecosystem. You're not going to be bored, that's for sure!Not convinced yet? Look! It's right there on Tel Aviv beach. How can you pass on that?I'll be there too, and I warmly invite you to grab your ticket and join us.Hoping to see you soon, Liran."
https://dev.to/snyk/don-t-let-security-vulnerabilities-crawl-into-your-node-js-docker-images-4fnn,Don't let security vulnerabilities crawl into your Node.js Docker images,2019-12-09T23:29:16Z,Snyk,"When we choose a base image for our Docker container, we indirectly take upon ourselves the risk of all the security concerns that the base image is bundled with.This can be poorly configured defaults that don’t contribute to the security of the operating system, as well as system libraries that are bundled with the base image we chose.Based on scans performed by Snyk users, it was found that 44% of Docker image scans had known vulnerabilities, and for which there were newer and more secure base images available.A good first step is to make use of as minimal a base image as is possible while still being able to run your application without issues.This helps reduce the attack surface by limiting exposure to vulnerabilities; on the other hand, it doesn’t run any audits on its own, nor does it protect you from future vulnerabilities that may be disclosed for the version of the base image that you are using.Therefore, one way of protecting against vulnerabilities in open source software is to use tools such as Snyk, to add continuous scanning and monitoring of vulnerabilities that may exist across all of the Docker image layers that are in use.In action, a snyk test for a docker image:To practice the same workflow you can use snyk to scan a Docker image for known vulnerabilities with these commands:Monitoring vulnerabilities as they get discovered is not any less important than scanning them to begin with.Monitor a Docker image for known vulnerabilities so that once newly discovered vulnerabilities are found in the image, Snyk can notify and provide remediation advice:Snyk also found that for 20% out of all Docker image scans, only a rebuild of the Docker image would be necessary in order to reduce the number of vulnerabilities.This tip is part of a complete 10 Docker image security best practices you should adopt. Thanks for reading and to Omer Levi Hevroni who worked with me on it.The original blog post includes a high-resolution printable PDF like the snippet you see below. Check it out"
https://dev.to/lirantal/npm-security-tips-to-keep-you-safe-of-malicious-modules-25bp,npm security tips to keep you safe of malicious modules,2019-08-19T19:24:20Z,Liran Tal,"Tip 3: Minimize attack surfaces by ignoring run-scripts (out of 10 npm security best practices)The npm CLI works with package run-scripts. If you’ve ever run npm start or npm test then you’ve used package run-scripts too.The npm CLI builds on scripts that a package can declare, and allows packages to define scripts to run at specific entry points during the package’s installation in a project.For example, some of these script hook entries may be postinstall scripts that a package that is being installed will execute in order to perform housekeeping chores.With this capability, bad actors may create or alter packages to perform malicious acts by running any arbitrary command when their package is installed.A couple of cases where we’ve seen this already happening is the popular eslint-scope incident that harvested npm tokens, and the crossenv incident, along with 36 other packages that abused a typosquatting attack on the npm registry.Apply these best practices in order to minimize the malicious module attack surface:Always vet and perform due-diligence on third-party modules that you install in order to confirm their health and credibility.Hold-off on upgrading blindly to new versions; allow new package versions some time to circulate before trying them out.Before upgrading, make sure to review changelog and release notes for the upgraded version.When installing packages make sure to add the --ignore-scripts suffix to disable the execution of any scripts by third-party packages.Consider adding ignore-scripts to your .npmrc project file, or to your global npm configuration.--I also blogged about a complete 10 npm security best practices you should adopt in a post that includes a high-resolution printable PDF like the snippet you see below.Thanks for reading and to Juan Picado from the Verdaccio team who worked with me on it."
https://dev.to/setevoy/kubernetes-the-krew-plugins-manager-and-useful-kubectl-plugins-list-24df,"Kubernetes: Krew plugins manager, and useful kubectl plugins list",2021-10-14T09:28:55Z,Arseny Zinchenko,"One of the most valuable features of the kubectl utility is its plugins.Of course, there are things like Lens, widely used by developers who don’t like working in the terminal, or tools like k9s, but kubectl's plugins worth for a dedicated post.So, in this post, we will install the Krew  - kubectl's plugins manager, and will take a look at the list of the plugins, that are used by me during work.Recently, kubectl got its own plugins manager, the Krew.It has a plugins repository used to install and upgrade plugins.On the Arch Linux krew can be installed from AUR:Arch Linux:Update its packages list:Look for a plugin, for example the topology:Install it:And check:At the end of this post I’ll add a couple of links with a similar plugins list, and here below let’s take a look at the best, as for me, plugins that I’m using during work.Let’s start from the kubectl-topology plugin that allows to display all cluster's WorkerNodes, their regions, and pods located on each node:The kubectl-resources plugin can display extended information about limits and resources used by pods on WorkerNodes.Install with Go:The kubectl-free plugin is similar to the free utility in Linux - it will display information about used and available resources on WorkerNodes.Install it by downloading an archive from the release page, unzip, and copy to the /usr/local/bin:And run it:kubecolor is very useful to make kubectl's output more readable by colonizing it.Install with Go:Add an alias to the ~/.bashrc:And its result:An improved variant of the kubectl --watch. The project's page is here>>>.Install:Check:And run:An advanced logs viewer. Similar to the kubectl logs -f, but can tail logs from all pods in a namespace. More details here>>>.Install:And check logs from the istio-system namespace:kubectl-who-can is used to display RBAC permissions and accesses.Install with Krew:And check who is able to delete pods in the default namespace:kubectl-rolesum is also used to work with the Kubernetes RBAC to check permissions.Install:And check permissions for the kiali-service-account ServiceAccount in the istio-system namespace:ketall will display indeed all resources including Secrets, ServiceAccount, Roles, Binding, and so on, and not only Pods, Services, Daemonsets, Deployments, and ReplicaSets, as it is when using the kubectl get all.Install:An example of the kubectl get all command's output:And with the ketall - kubectl get-all:kubectl-status can display extended information about pods' statuses, nodes, deployments, services, and so on.Install:And check Kubernetes WorkerNodes status with the role: data-workers label set:Pod-Dive will display information about a Kubernetes Pod  -  its WorkerNode, a namespace, containers in that pod, and other pods, that are running on the same WorkerNode.Install:And check a Pod:kubectl-janitor - a ""cleaner"" for a Kubernetes cluster. Can find problematic resources, such as unscheduled pods, failed jobs, volumes, etc.Install:And find all pods that are not scheduled to a WorkerNode:And the last plugin for today  -  kubectl-cf.Actually, there are a lot of plugins to simply work with kubectl's contexts, but I'm using dedicated files for clusters instead of dedicated contexts:For a faster switch between them, we can use the kubectl-cf plugin, that will work via creating symlinks to the ~/.kube/config file.Pay attention, that all files must have the .kubeconfig extension.Install it — clone its repository, build, and copy to the /usr/local/bin:Originally published at RTFM: Linux, DevOps, and system administration."
https://dev.to/setevoy/kubernetes-helm-x509-certificate-signed-by-unknown-authority-and-serviceaccount-for-pod-3d01,"Kubernetes: Helm  - ""x509: certificate signed by unknown authority"", and ServiceAccount for Pod",2021-10-01T07:53:19Z,Arseny Zinchenko,"We have Github runners in our AWS Elastic Kubernetes service cluster, that are used to build Docker images and deploy them with Helm or ArgoCD.On the first helm install run in a Github runner's Pod, we are getting the "" x509: certificate signed by unknown authority"" error:Or, if not set an API URL, then we can see a permissions error:So, the cause is obvious: Helm in the Pod is trying to access the Kubernetes API server using the default ServiceAccount, that was created during Github runner deployment.As already discussed in the Kubernetes: ServiceAccounts, JWT-tokens, authentication, and RBAC authorization post, to authenticate on an API server we need to have its Certificate Authority key and a token.Connect to the pod:Set variables:And try to access API now:A similar error you’ll get if try to run the kubectl get pod:Okay, and what we can do here?Actually, instead of mounting the default ServiceAccount to the pod, we can create our own with permissions set with Kubernetes RBAC Role amd Kubernetes RBAC RoleBinding.We can use a pre-defined role from the list here — User-facing roles, for example, cluster-admin:Or, we can write our own with strict limitations. For example, let’s grant access to the verbs get and list for the resources pods:Create that role in a necessary Kubernetes Namespace:Check it:Next, create a new ServiceAccount in the same namespace:Apply it:Now, we need to create a binding — a connection between this ServiceAccount and the role, which was created above.Create a RoleBinding:Apply it:And the final thing is to attach this ServiceAccount to our pods.First, let’s do it manually to check, without a Deployment:Create this Pod:Connect to it:And check API access:Now we are able to use the get and list for Pods, but not for the Secrets, as we didn't add this permission:Let’s go back to our Github Runners and helm in there.So, as this Helm will be used to deploy anything and everywhere in the EKS cluster, we can give it admin access by using the cluster-admin ClusterRole.Delete RoleBinding created before:Create a new one, but at this time by using the ClusterRoleBinding type to give access to the whole cluster:Attach this ServiceAccoun to this ClusterRole:Go back to the Pod, and try to access a Kubernetes Secrets now:Nice — now our Helm has full access to the cluster!And let’s update the Github Runners Deployment to use this ServiceAccount.Edit it:Set a new ServiceAccount by adding the serviceAccount:Wait for new pods will be created (check the Kubernetes: ConfigMap and Secrets — data auto-reload in pods), connect and check the Pod’s permissions now:And check access to another namespace, as we’ve created the ClusterRoleBinding, which is applied to the whole cluster:Good — and we have access to the istio-system Namespace too.And check the Helm:Done.Originally published at RTFM: Linux, DevOps, and system administration."
https://dev.to/setevoy/aws-web-application-firewall-overview-configuration-and-its-monitoring-3hl0,"AWS: Web Application Firewall overview, configuration, and its monitoring",2021-07-20T05:18:10Z,Arseny Zinchenko,"AWS WAF (Web Application Firewall) is an AWS service for monitoring incoming traffic to secure a web application for suspicious activity like SQL injections. Can be attached to an AWS Application LoadBalancer, AWS CloudFront distribution, Amazon API Gateway, and AWS AppSync GraphQL API.In case of finding any request that sits WAF’s rules, it will be blocked, and its sender will get a 403 response.AWS WAF consist of four main components:AWS WAF has a capacity for its ACLs: each List can hold up to 1500 WCU (WAF Capacity Unit). We will speak about WAF’s limits in the AWS WAF limitations. Also, check the AWS WAF Web ACL capacity units (WCU).The most inconvenient limit is that one Application Load Balancer can have only one ACL attached.Also, I asked the AWS team about performance — will attaching a WAF ACL to an ALB/CloudFront will affect its response time, but no ACL or number of rules in it will affect a target anyway.So, in this post we will spin up a test application in Kubernetes, will go through main WAF concepts, will see how Rules can be configured for an ACL, will create such an ACL, and will configure its monitoring with CloudWatch and с Prometheus.Let’s use a simple Kubernetes Deployment that will create a Kubernetes Pod with Nginx, Service, and an Ingress resource. This Ingress with AWS Load Balancer Controller will create an AWS Application LoadBalancer.Deploy it, and check the Ingress and its ALB:Check for the response of this ALB:Everything works here, let’s go to the AWS WAF.Go to the AWS Console > WAF, click on the Create web ACL:In this case, we will attach an AWS ALB, so at first (!) chose a necessary AWS Region, then set an ACL’s name which also will be used for CloudWatch metrics, they will be discussed below in the AWS CloudWatch metrics, and Prometheus topic:Next, find an ALB to be protected:And from now, every request sent to this AWS ALB first will be sent over a chain of the ACL’s rules, and then it will be or declined with the 403 error, or will be passed to an application behind this load balancer.We will add rules later, for now just leave the default action set to the Allow:Here, you can also configure additional actions:And again, for now just leave everything by default here, as this can be configured later.Similarly to common firewalls, you can configure rules priority in an ACL: the first rule matching a request will be applied:Skip for now too.CloudWatch will be configured a bit later, so skip it as well:Run curl to the testing ALB to get some traffic and graphs:The IP set allows the creation of a set of IPs or IP ranges that can be used later in rules.For example, we can create a set of our office IPs to make an Allow rule. See Creating an IP set:Pay attention to the AWS Regions here: for a CloudFront, you have to choose the CloudFront (Global), for all others — a region, where a secured resource is located.Get your current IP (my office):Create an IP Set, use the /29 mask to include all our office IPs:And now, let’s go to the Rules.Select the ACL created above, click on the Add Rules, сhoose the Add my own rules:Let’s add a simple rule: block all requests that originated from our office.Choose the Rule type == IP Set as we will use the IP set created before, set the rule’s name, select your IP Set, use the Source IP address, and the default action — Block:If set the action to the Count, WAF will just count requests that matched the rule without aby blocking activities. Can be used for testing before applying a rule:And in a few seconds we will get the 403 responses:Now, let’s try to make something more interesting: allow traffic only from the office, but block all other requests from the world.As there is no way to create an IP Set with the 0.0.0.0/0 CIDR, so let’s make it in a different way: change the Default Action of the ACL to the Block, and use office’ IP set to the Allowed rule.Edit the Default web ACL action:Check the access from something different than the office, for example from a server where is the rtfm.co.ua hosted. In 15–20 new settings will be applied:Go back to the ACL test-office-rule, and change Block to the Allow from the office:Check if it’s working now from the office:Okay, but what if we want to block public access only to a specific URI, let’s say /status, but allow its access from the office?To do so, we need to:Go back to the default action of the ACL, and change it back to the Allow, thus allowing access to the ALB from anywhere:Now, we need two additional rules: one to allow from the office, and one to block from the world. Let’s start with the blocking rule.At this time, use the Rule builder, call the rule as block-status-uri-world, in the Statement choose the URI Path, set a value as /status, in the Action set the Block:Check the access now.From the office to the /:And from an another IP to the /:From the office to the /status:And from the world:Good: now we’ve granted access to the root of the website from anywhere, and the /status is blocked from anywhere. Now, let’s allow the /status access from the office.Go back to the Rules, and add another rule, let’s call it allow-status-uri-office:Build the rule’s condition:By the way, pay attention to the Capacity field: here we can see how many WCUs will be consumed by the rule from the total 1500 available for us in the ACL.So, we’ve added an allow rule, but now it will not work as we have the first rule block-status-uri-world, and it will block all requests to the /status, even from the office:To fix this, move the allow-status-uri-office to the beginning of the list:So now priorities will be the next:Check access from outside of the office:Good, it was blocked. Now, from the office:And here it works.One of the nastiest types of attacks is SQL injection which definitely worth being banned.In the AWS WAF Managed rules we have a list of predefined rules from the AWS itself and its partners, and also you can create such a block on your own.Create a new rule called sqli-test:Define its conditions:In the Actions leave the default Block, and in the Custom response let’s set the 405 — Method not allowed, to confuse an attacker (although it might be even more fun to set the 200 here :-) ):Save the rule, and check it with a request products?category=Gifts'-- googled in here:Yay! We’ve blocked an SQL injection attempt to our application!Let’s take a look at what AWS and its partners suggest to use in their existing rules.Chose the Add managed rule groups:In the AWS managed rule groups you can find a list of rules from the AWS, and the rest is paid rules that can be purchased on the AWS Marketplace.Check the documentation for the rules description — AWS Managed Rules rule groups list.For example, the Known bad inputs rule will inspect the Host header of a request, and if it contains localhost, then such a request from the Internet will be blocked as a valid request can not contain it.Add it to the ACL:Also, here you can configure a default action for the group, or add a filter to select only some of the requests:And again — pay attention to the Capacity, as this group will use the whole 200 WCUs from the 1500 limit of an ACL.Save, check priorities:Check access:And add the non-valid value for the Host header - localhost:And now it’s blocked.There is one thing to be noted: if a rule was created directly from an ACL, and when this ACL will be deleted, the rule will be deleted as well.To save your own rules persistently, use the Rule groups:Then, create your rules in a Group, and attach this Group to your ACL.See all limits — AWS WAF quotas.From the main, as forme:Okay, so we’ve seen how to block requests, created an ACL, tested it. All works.But it’s good to monitor its activities, and even send alerts when something goes weird.The very first place where we can go is CloudWatch and WAFV2 metrics:Choose our ACL here or a specific rule:Here are our blocked requests.Next, we can collect them to a Prometheus instance with the cloudwatch-exporter:Run our test requests with SQL injection, or even run it in a loop:Check graphs in the Prometheus:And now we can see blocked requests here too.Let’s add a test alert: check an average increase per second for the aws_wafv2_blocked_requests_sum metric during last 5 minutes:If its value will be greater than zero, then WAF blocked someone, and we will be notified about it:See Managing logging for a web ACL.To log WAF activity, we need to have an AWS S3 bucket, and an AWS Kinesis Data Firehose delivery stream.Go to the Kinesis, create a new Data Firehouse stream:Its name must be started with the aws-waf-logs- prefix, also set the Direct PUT or other sources, and click Next:Skip everything on the following page, on the next page for the Destination select S3, chose existing, or create a new S3 bucket:Here, you also can set a prefix to keep logs from various ACLs in the same S3 bucket:On the next page, for now, can leave everything with default values:Check settings and click Create delivery stream:Go back to the WAF ACL to the Logging and metrics tab, click on the Enable logging:Select the stream you created above, optionally configure filters for fields:Wait for 5–10 minutes, and you’ll get your logs delivered to the bucket:And its content:Now, you can collect them to services like Logz.io, see Configure Logz.io to fetch logs from an S3 bucket.CloudWatch Logs isn’t supported yet, but it’s planned to be added.So, we have an ACL:And we need to attach this ACL to an ALB, created from a Kubernetes Ingress with the AWS Load Balancer Controller.To do so, we can use its alb.ingress.kubernetes.io/wafv2-acl-arn.Enable ARNs display for your ACLs:Update your Ingress:Apply:Check the ACL — Associated AWS resources:Done.Originally published at RTFM: Linux, DevOps, and system administration."
https://dev.to/zaiste/livestream-building-e-commerce-apps-with-flutter-graphql-1ao3,Livestream: Building E-Commerce Apps with Flutter & GraphQL,2021-07-04T18:19:57Z,Zaiste,"I'll be doing a live stream this week about building mobile apps with Flutter and GraphQL. My plan is to create a simple e-commerce app. For the backend I'll use the Saleor API.If this sounds interesting and/or you're building (or planning to build) something similar, let me know your questions. I'll happily consider them during the coding! This can be anything related to Flutter, GraphQL or e-commerce.The stream will happen on my YouTube channel - subscribe and click the notificaiton bell."
https://dev.to/zaiste/modern-react-js-setup-for-graphql-using-vite-urql-abg,Modern React.js Setup for GraphQL (using Vite & urql),2020-09-29T13:39:40Z,Zaiste,"Let’s start with pnpm, a package manager for JavaScript that is faster and more efficient than npm or yarn. pnpm uses a content-addressable filesystem to store your project dependencies. This way files inside node_modules are linked from a single place on your disk. Thus, you install each dependency only once and this dependency occupies the space on your disk only once. In other words, libraries are not copied over for each new project. This way, on top of being faster than alternatives, pnpm provides huge disk space gains.pnpm comes with two command-line tools: pnpm for installing dependencies and pnpx for invoking commands (a npx equivalent).Let’s use Vite to create the project structure for our React.js project. We need to initialize the project using the vite-app generator with the React as the template. The template must be set explicitly using the --template parameter with react as its value. Finally, gh-explorer is a custom name we are giving to this project.Vite is a build tool for web projects. It serves the code in development using ECMAScript Module imports. In production, vite bundles the code using Rollup. Vite is a lightweight solution that can be 100-150x times faster than alternatives such as Webpack or Parcel. This enormous speed gain is possible thanks to esbuild, a new TypeScript/JavaScript bundler written using the Go programming language.Go inside the gh-explorer directory and install the necessary dependencies using the pnpm install command. Then, start the development server with pnpm dev and head to the localhost:5000 in your browser. You should see a React.js logo along with a counter and a button.When interacting with external APIs, we need to learn the specifics for each new API we are dealing with. This is especially visible at the level of authentication. The methods of authentication are slightly different between one API and another. Even though those APIs are provided either as REST or GraphQL endpoints, it takes time and often much effort to learn how to use them. Luckly, there is OneGraph. The project provides a layer of unification for various GraphQL APIs. Using OneGraph, we can just access one endpoint and gain access to various GraphQL APIs at once. Think, a catalog of APIs. This simplifies and speeds up the development. We will use OneGraph to interact with the GitHub API.Let’s create an application in OneGraph:Then, we can use OneGraph's Explorer to test our GraphQL queries for GitHub before we integrate them with our React.js application. On the left side of the Explorer I have a list of all available APIs. It goes from Airtable, Box to Shopify, Stripe, Zendesk and much more. This catalog is quite impressive on its own.Our goal is to list the repositories of a specific user. I start by selecting the GitHub API. Then, I select theuser branch. I enter the handle of a specific user, e.g. zaiste - in this case, it’s my own username. I go further down the GitHub API tree by selecting the repositories branch. I want to list only the public repositories that are not forks and ordered by the number of stars. For each repository, I want to return its id, name and the number of stars.Just by clicking the fields in the OneGraph Explorer I end up with the following GraphQL query:We can now execute this query from our React.js application. We will use urql, a versatile GraphQL client for React.js, Preact and Svelte. The project is lightweight and highly customizable compared to alternatives such as Apollo or Relay. Its API is simple and the library aims to be easy to use. We need to add urql along with the graphql as dependencies for our project.urql provides the useQuery hook. This function takes the GraphQL query as input, and returns the data along with errors and the fetching status as the result. We will name our component RepositoryList. You can use the regular .jsx extension, or .tsx if you plan to integrate with TypeScript - it will work out-of-the-box with Vite. There is no need for additional TypeScript configuration.Next, in main.jsx let’s configure our GraphQL client. We need the Provider component along with the createClient function from urql, and an instance of OneGraphAuth. For the latter, we need another dependency, i.e. onegraph-auth.Let’s create an instance of OneGraphAuth with the appId of the application we created using the OneGraph dashboard. Then, we create a GraphQL client with the OneGraph endpoint as the url parameter. Finally, we nest the <App/> component inside the <Provider/>.We are almost finished. The last step is to authenticate the user against the OneGraph endpoint. It’s a unified approach for any API from the OneGraph catalog. We will use the .login method from the onegraph-auth with github as the value. Once the user logs in, we will adjust the state accordingly by displaying the <RepositoryList/> component.That’s all. Here’s the final result. You may need to adjust the stylesheets for the same visual effect.We created a React.js application using Hooks. The project has a minimal set of dependencies. It uses the modern ECMASCript Modules approach. It is efficient in disk space as it uses pnpm as the package manager. The JavaScript/TypeScript transpilation is 100-150x faster than Webpack or Parcel. We use a simple and versatile GraphQL client called urql. Finally, we access the GitHub API via OneGraph, a meta API that provides an impressive catalog of GraphQL APIs with the unified access method. The end result is lightweight and modern.I hope you will use some of those elements in your future React.js applications. If you liked the article, follow me on Twitter for more.--If you prefer watching programming tutorials rather than reading them, check this video on YouTube where I code this application step-by-step.If you like it, subscribe to my channel."
https://dev.to/zaiste/my-favorite-privacy-tools-in-2020-be-safer-on-the-internet-4dik,My Favorite Privacy Tools in 2020: Be Safer on the Internet!,2020-08-16T20:27:05Z,Zaiste,"This is a list of tools I use to increase my privacy on the Internet. I'm not a security expert. My goal is to have a good enough protection. Feel free to suggest any additions and improvements.Firefox probably doesn't need an introduction. It is an open-source project. It's run by Mozilla - a non-profit organization. Anyone can go to the Firefox repository to take a look at the code and see how it works.Firefox doesn't gather your personal data to serve you ads as Chrome. While Google seems careful and to an extend trustworthy with handling people's personal data, you don't seem to have much control how your profile is built and used.The browser is constantly being improved. It seems slightly heavier than Chrome or Safari on my computer, but that's a minor issue compared to all the advantages. I use the Developer Edition.Little Snitch is a MacOS application that monitors connections and lets you control inbound and outbound traffic from your computer. It visualizes these connections and shows where applications connect. This way you have information about both familiar and unfamiliar network actions that applications are trying to perform.PiHole & AdGuard are ad blockers. PiHole is open-source while AdGuard is a paid offering. I use one at home and the other one at the office. AdGuard is slightly easier to setup and use. Both work network-wide, but AdGuard also provides standalone applications and browser plugins to protect from ads when being outside of private networks.Nibspace is an alternative to Google Analytics. It provides basic information about the visitors of my websites. It doesn't use cookies and it has a small footprint (~ 1kb) while being affordable ($1/mo per domain). It's not open-source. There are few other, notable competitors such as Fathom, Simple Analytics and Plausible.Bitwarden is an open-source password manager. I used to use 1Password, but I wanted something more affordable yet with similar features and Bitwarden fits the bill perfectly.NordVPN is a personal virtual private network service provider. Their marketing seems aggressive, so I was a bit skeptical at the beginning, but then I decided to give it a try. Overall, NordVPN is one of the most polished and reliable VPNs I've used so far.I also like that they show some innovation efforts, namely their NordLynx technology that is built around WireGuard - this relatively new VPN tunneling protocolLast year NordVPN was hacked, but they handled the whole situation pretty well in my view.Fastmail is a paid e-mail service that focuses on privacy and doesn't display ads. It is a simple service to manage e-mails, calendars and contacts. It costs $5/mo. It is also fast. This is partly related to their work on a more reliable, faster IMAP alternative, called JMAP - so it goes beyond just a snappy UI.Have I missed an interesting tool? Let me know on Twitter."
https://dev.to/ionic/10-reasons-to-choose-ionic-for-mobile-development-213a,10 Reasons to Choose Ionic for Mobile Development,2021-07-22T18:47:57Z,Ionic,"The Ionic stack makes it easy to build iOS, Android, Desktop, and Progressive Web Apps using standard web technology. There are a lot of options for cross-platform mobile and web app development, so when should you or your team consider using Ionic?Here are 10 reasons you might want to choose Ionic:One of the best reasons to choose Ionic is if you’re a web developer and want to stay in the web development world. The Ionic stack is built on standard web development technologies meaning you can build the majority of your app directly in a browser and use standard browser development tools and technologies, while still deploying and building a traditional native app.Other cross-platform toolkits don’t use the web development stack so aren’t the best fit for web developers.Because the Ionic stack is focused on web developers, it’s the easiest cross-platform technology to hire for because there are considerably more web developers in the world than any other mobile technology. Any web developer can easily learn to build apps using the Ionic stack and it will be the most familiar to them, and have the best compatibility with popular web frameworks like React, Angular, and Vue.And hiring web developers means they can then develop for mobile, desktop, and web without the need to hire specialist teams for each platform.Ionic is the only cross-platform development stack that has Enterprise support and integrations for teams building employee and customer-facing apps. Ionic offers dedicated support, security features like Biometrics and Single Sign-on, and cloud services for remote app updates, app builds, and app store distribution.Ionic is the industry leader in enterprise app development with hundreds of Fortune 1000 customers, and customers love it.Because the Ionic stack is based around standard web development, it’s the only cross-platform solution that supports bringing existing web experiences, libraries, and other web code into an app.This is particularly important for enterprises or teams with a heavy web presence that want to bring those experiences into existing apps, such as traditional native apps.Building apps with popular technologies makes it easier to hire for, easier to find partners, and easier to find resources on the web as you build your app.The Ionic developer community is massive, with millions of developers around the world having built apps on the stack. Passionate community members build and share tutorials, add-ons, templates, and more, and developers can find help on the Ionic forum and Stack Overflow.But Ionic is unique in that the community isn’t just limited to Ionic’s bubble. Because Ionic is based on standard web technologies, the addressable community goes beyond Ionic and overlaps with the broader web development community, such as the React, Angular, and Vue communities. This is critical when considering building out app dev teams because an Ionic team can be built with web developers, but other technologies require hiring specialists on languages and technologies that are not widely used.We often hear from teams who spent months building their app using a different technology, failed to hit deadlines, and then switched to Ionic and had the new app working and surpassing the old one within weeks.Ionic is highly productive because the web is highly productive. And one app can run on multiple platforms so development time can be upwards of 3-4x faster than traditional native development!Ionic apps run anywhere the web runs, and with the same code! And Ionic has official support for iOS, Android, Desktop, and Web, so teams can hit the ground running — targeting all these platforms and their respective user bases at launch.Because the Ionic stack is based on web technology, teams can target the web and build a high-performance Progressive Web App (PWA).This means teams can reach users both in traditional app stores but also on the web and through Google search to maximize reach and have the best chance for a successful app project. We’ve talked at length about how the Ionic stack gives you the best distribution optionality of any cross-platform technology in the market.If you already have web developers on your team or at your company, using the Ionic stack is a no-brainer! Web developers can easily become mobile developers by learning to use the Ionic stack, including Ionic’s 100+ mobile-optimized UI components and the native runtime that provides hooks into functionality on each platform. Ionic also supplies the means to build and deploy those apps.Ionic is the most accessible platform for cross-platform app development. If you’re familiar with basic web development you can build real mobile apps (and desktop + web apps) today!Learning Ionic requires learning a few new HTML tags, some new JavaScript APIs for native functionality, and then just building your app using your framework of choice (React/Angular/Vue/etc).And the best thing is you continue to build your web development skills at the same time and don't have to change your technical career focus.There you have it, ten good reasons you and your team might want to adopt Ionic.If you're interested in building cross-platform apps using Ionic and modern web technologies, Get started with Ionic today."
https://dev.to/ionic/app-store-or-web-why-not-both-3fa2,App store or web? Why not both!?,2021-06-09T16:12:23Z,Ionic,"Most app development technologies force teams to make hard decisions about where they will distribute their apps, usually either app stores as a native app or the web as a Progressive Web App.Building a native Swift (iOS) or Kotlin (Android) app, for example, obviously limits teams to just those respective platforms. But others have more subtle restrictions. Xamarin enables you to build for iOS, Android, and Desktop, but not for the web, meaning no access to a major chunk of mobile traffic and app-consuming users. React Native’s View elements can be abstracted to have the same API on the web but you will need to write completely separate view code for it, avoid using certain standard web features, and use different calls for certain APIs, so it’s an additional investment. Flutter can be used to build iOS, Android, and Web apps, but its web support requires multiple MBs of code for users so it’s not appropriate for high-performance Progressive Web Apps and speed-sensitive web sites like those that depend on search engine rankings and SEO.Capacitor, a universal app runtime, avoids these tradeoffs by targeting one universal runtime (the web), and giving you maximum optionality for where you distribute the app that you build. You can deploy your app with one codebase anywhere your users are, even if that happens to change! That might mean the app stores today, but it could mean the web and desktop tomorrow.I was recently helping a team think through an app project they were going to deploy to the app stores on iOS and Android. They were using Capacitor with React, using a home-grown UI based on Tailwind.During development, the team decided that they wanted to have a strong web portion to their app in order to easily acquire users, benefit from instant access and SEO on the web, and avoid app install bounce risk (meaning, the user leaves without ever installing the app, which is very common in the mobile world!)Since their app was built with Capacitor, utilizing Capacitor’s cross-platform APIs, they could simply deploy the same app with the same code as a Progressive Web App hosted on a platform like Vercel or Netlify.And this decision didn’t preclude them from also shipping a native iOS and Android app. They decided that they would unlock some additional features that depended on native functionality missing from the browser environment for users that were “activated” and likely to stick around.Doing so in Capacitor was straightforward because Capacitor APIs are identical on iOS, Android, and the Web, and Capacitor comes with a number of utilities for adding conditional platform-specific code if desired.For new projects, the greatest risk isn’t technical but rather that no one uses your app and it never gains meaningful traction or scale. One of the best ways to avoid that is to reach the broadest possible segment of your target audience on day one.For most companies, their audience isn’t on a single platform, form factor, or environment (such as mobile or web). This is especially true for consumer and enterprise consumer companies. In fact, there’s a good chance a majority of users aren’t even on a platform that you, the developer, actually use primarily (such as Android if you’re an iPhone user or web apps if you’re in the US and used to installed native apps).Having optionality might mean the difference between your app project failing or succeeding, and nothing could matter more than that.One of the things to think about when building a Progressive Web App not with Capacitor, is that it will constrain your app to running only in a traditional browser context. This means it will only be accessible on the web and not have the option to access native APIs that don't have a browser analog since it cannot be distributed in popular mobile app stores.Thus, the decision to build a Progressive Web App can also be limiting. But in this case a very straightforward way to add native iOS and Android support is by installing Capacitor into your Progressive Web App.Capacitor has full Progressive Web App support and apps use the same Capacitor API calls regardless of the platform they are running on. This makes it easy to bring it to iOS and Android with the same code.In that sense, Capacitor is ""web app virtualization layer"" that enables standard web apps to run anywhere with the same APIs, regardless of the ""host"" platform underneath.Capacitor is growing quickly (100% YoY) and is installed over 500k times per month. Teams at companies like Burger King, Tim Hortons, Workgrid (Liberty Mutual), and Fidelity, to name a few, are all using Capacitor to enable their web teams to deploy iOS, Android, and Web apps with the same code.Capacitor was also rated #1 in satisfaction among mobile development tools on the latest State of JS survey.Capacitor was created by the team behind the popular Ionic Framework as a replacement for Cordova for deploying modern Web Native apps on iOS, Android, Desktop, and Web.Getting started with Capacitor and building full distribution optionality into your mobile-enabled web app is as easy as installing the Capacitor library and running a few commands.Get started with Capacitor today and follow us for more resources and updates on the project!"
https://dev.to/ionic/build-mobile-apps-with-tailwind-css-next-js-ionic-framework-and-capacitor-3kij,"Build Mobile Apps with Tailwind CSS, Next.js, Ionic Framework, and Capacitor",2021-02-01T15:24:54Z,Ionic,"A very popular stack for building responsive web apps is Tailwind CSS and Next.js by Vercel.Tailwind, a utility-first CSS framework that replaces the need to write custom class names or even any CSS at all in many cases, makes it easy to design responsive web apps through small CSS building blocks and a flexible design foundation.Next.js, a React framework for building high performance React apps, is one of the leading environments for building production React apps on the web.As these technologies have grown, they are increasingly used together for web app development (in fact, Next.js is working on an RFC for official Tailwind integration). This has prompted many users of these projects to ask whether they can be used to build mobile apps, too.Turns out, they can! And they make a great fit for cross-platform mobile development when paired with Ionic Framework and Capacitor.As I started playing with these technologies, I realized that each had a natural fit in a combined mobile stack, and I wanted to put together a solid starting foundation for others interested in building real iOS and Android apps using these technologies.If you're confused by all the project names and how they work together, don't worry, I'll break down each part of the stack each project is concerned with, along with some visuals and code samples demonstrating how all the projects work together. At the end I'll share a starter project with these technologies installed and working together that can form the foundation of your next app.The above is a live screenshot of a React app built with Next.js that is using Ionic Framework and Tailwind for the UI experience, and Capacitor to natively deploy that app to iOS and provide access to any Native APIs the app uses.There are a lot of projects working in tandem to provide the full experience here. To visualize it, I've tried to overlay the different layers and how they correspond to each project in this diagram above.We can see that Capacitor is concerned with the entire app and device layer of the app, Next.js is concerned with the entire web/React app our code and UI is running in, then Ionic handles the ""platform UI"" including navigation toolbar (including system title and toolbar buttons) as well as the bottom tabs.Finally, Tailwind is used to then style and customize the content of each page, which is where the bulk of the app-specific styling will occur.If your experience building with web technologies is primarily for desktop or responsive sites, you might not be familiar with mobile-focused libraries Ionic Framework and Capacitor.Ionic Framework is a cross-platform, mobile-focused UI library for the web. It provides ~100 components that implement platform UI standards across iOS and Android. Things like toolbars, navigation/transitions, tabs, dialog windows, and more. The big draw is those components work on the web and work in frameworks like React, Angular, Vue, or plain HTML/CSS/JS.Ionic Framework is highly popular and powers upwards of 15% of apps in the app store.Historically, Ionic Framework would be paired with a project like Cordova which provided the native iOS and Android building and runtime capabilities. However, most new Ionic Framework apps use Capacitor for this part of the stack.Capacitor is a project built by the team behind Ionic Framework focused on the native side of a web-focused mobile app.Capacitor provides a plugin layer and runtime that runs web apps natively on iOS, Android, Desktop, and Web, and provides full access to device APIs and features (including extending the web environment by writing additional native Swift/Java code).As such, any popular web technologies and libraries can be used to build mobile apps with Capacitor, and then deploy the same apps with the same code to the web and desktop.And, to top it all off, Capacitor was just rated the second highest in satisfaction among popular Mobile & Desktop Tools on the State of JS 2020 Survey! If your last experience with this mobile development approach was with Cordova, we think you'll find Capacitor to be a big improvement.Now that you have a sense for how these technologies all work together to make it easy for web developers to build mobile apps, let's take a look at a real demo and starter project (GitHub repo):Let's take a look at the main Feed page (seen above in the screenshot) for an example of how the different technologies in use work together:As we can see, we use Ionic Framework controls (IonPage, IonHeader, IonContent, IonToolbar, etc) for the structure of the page (these controls implement iOS and Android platform-specific styles and navigation/transition behavior), then we use Tailwind for the page content that is where our custom design lives (which will tend to be in IonContent).If we look at another page that is just a simple list, we see that we don't use Tailwind at all, because the user would expect this page to be a standard iOS/Android list and toggle button (code here):So, we tend to use Tailwind more for pages with a lot of custom design and assets. That's by design. Generally when building a native mobile app, we want to use platform conventions as much as possible, especially for experience and performance-sensitive elements like Lists, Toolbars, Tabs, and Form inputs. However, for the Feed page, which has a pretty custom UI experience, we end up getting a lot of mileage out of Tailwind.So, in general, the way to think about when to lean more on Ionic Framework and when to lean on Tailwind is when your UI experience will heavily use typical mobile UI elements (prefer Ionic components) or when it will be more custom (prefer Tailwind).Finally, this starter also comes with a few small opinions around folder structure and state management. For state management, the library pullstate is used which is a simple yet powerful state management library with a hooks-based API (I wrote more about it here). If want to use something else, removing it is easy.The app can be easily deployed to iOS and Android using Capacitor and its local CLI tools. After running npm install, you'll have the npx cap command available, which enables a native development workflow:To add an iOS or Android native project:Then, to build the Next.js app, export it, and copy it to the native projects:This command is needed every time the built output changes. However, you can enable livereload during development (see the README for more info).Then, you can launch Xcode and/or Android Studio to build and run the native project:If you've been interested in building mobile apps using popular web dev projects like Next.js or Tailwind, hopefully this starter provides inspiration and a solid foundation for building your next app using web technologies. It's worth mentioning this exact same approach can be used with other UI libraries (like material, bootstrap, or your own!).When you're ready, dig into the starter project, follow the Capacitor and Ionic Framework docs, and get building!"
https://dev.to/jignesh_simform/how-to-select-the-right-database-for-your-androidios-application-2aca,How to select the right Database for your Android/iOS Application,2018-01-17T15:00:53Z,Jignesh Solanki,"Imagine you’re developing a simple app with a list of different objects which should be searchable. Your app has basic requirements like when an user click on an object, it must shows information like name, age, income, or anything else.The Database selection for simple application like above won’t have drastic impact on the functionality and performance. But, it gets complicated when your application have demanding requirements.Consider these use-cases for example:When data synchronization and offline features are at the core of your mobile app, the users might have access to the same data and have the same CRUD rights.This basically means that you would end in a situation where multiple users are editing the same data block – concurrently!In other situations, we have an app that uses API to communicate with the server and perform CRUD operations. When a mobile app is offline, there’s no API connecting and dictating the read-write operations.When a user now edits a shared doc that others are editing as well, the writes may or may not be accepted by the server when the app goes online again.Let’s imagine this challenge better. Assume that we have the following situation:Some users are editing records while they’re offline. So, how does the app responds to their changes? Let’ see which edits will be sent to the server first.Assessing how complex the situation can get is a bit tricky, and many fail to accommodate for edge cases here. The solution here would require a locking mechanism to prevent data loss while synchronization.When any SQL databases lose network connection with the client side storage, it typically generates an error message instead of transferring the data as needed. If this problem occurs frequently, you may need to reconfigure your database.Apart from that, there are other challenges to deal with MySQL and network interruptions:Persisting local data including UN-synchronized transactions and app state Getting mobile transactions and operations back to the main server-side database Above listed issues occur on behalf of network interrupts, keeping a database that offers better reliability and resists connection loss is a better option in this case.Scaling = replacing all components of a car while driving it at 100mph – Mike Krieger, Instagram Co-founderWhen you think of scaling your application, you think of adding more resources in form of servers and making the database engine more efficient.The Database should be able to utilize the resources and handle parallel processing, that means the Database must be multi-threaded.Multi threading allows a database to schedule parallel tasks on the available resources and minimize the workload on the server-side.Apart from multi threading, Distributed Design of a database is significantly important for scalability.In a distributed designed database, you can split up the services on different threads to minimize the workload on the main database. This drastically improves the parallel processing of databases.A support for Multi-Version Concurrency Control (MVCC) allows simultaneous access without blocking the threads or processes involved.MVCC allows a reader to view a snapshot of the data before the writer’s changes, allowing the read and write operations to continue in parallel.For instance, look at this table to see which databases have MVCC implementation:Many startups have failed their product and some have migrated on-time to avoid scalability issues like Crisp and Uber.Crisp had to abandon Firebase, and Uber migrated from PostgreSQL to MySQL.Developers and organizations should not ignore the importance of a database in their application as it can bring huge challenges in the future.I agree that there are a wide number of database choices available to support different features of an application. Thus, it has become difficult to make a fair decision.But, if you follow the right principles and map your app requirements correctly, then your application will not suffer from database problems.This article is just a sneak peak of a more comprehensive guide on Database selection for mobile app developers.Please feel free to share your thoughts and past experiences with database selection."
https://dev.to/nipeshkc7/how-i-ve-been-misusing-async-await-1f5j,How I've been misusing Async Await,2020-05-18T03:27:27Z,Arpan Kc,"Async-await is very intuitive and apparently very easy to misuse. Recently I've found out how I've been misusing this feature because of which my JS executes a lot slower than it should.Let's look at the example to demonstrate what the problem was:See the core difference is how I was previously waiting for itemA to resolve before fetching itemB. On the other hand, when I wait for both the promises to resolve concurrently I speed up the execution by not having itemA to resolve before itemB.(Note: However this is only recommended when the itemB does not somehow depend upon the promise from itemA being resolved first.)P.S. Please follow me on twitter, I'd really appreciate it. @Nipeshkc"
https://dev.to/nipeshkc7/deploying-vuejs-front-end-to-netlify-322c,Deploying VueJS Front-end to Netlify,2020-04-27T06:24:45Z,Arpan Kc,"To preface, I'm currently working on a project whose backend API service I've recently published to heroku. So next step is to deploy my front-end.Considering all the free options, I settled on Netlify. In addition to the Continuous deployments which let me deploy on every pull request, I also did not have to deal with any server side stuff. And unlike the Heroku free servers, Netlify (static)sites never sleep.Here's how I did it in 3 simple steps:Because the front-end vuejs code is inside the 'client' subdirectory, I have to specify in this .toml file that the subdirectory to use is 'client' and the directory to publish after building is ./dist.Include this file so that directly going to a different url (like https://yoururl.netlify.app/home) won't return a '404')And voila, netlify should build and deploy the site.Thanks for reading.Project Repo: https://github.com/nipeshkc7/BeatingBookies Live project: https://beatingbookies.netlify.app/P.S. Please follow me on twitter, I'd really appreciate it. @Nipeshkc"
https://dev.to/nipeshkc7/organizing-my-nodejs-code-and-folder-structure-1060,Organizing my NodeJS code and folder structure,2020-02-02T13:18:25Z,Arpan Kc,"As I look at other people's nodeJS repository, I see neatly organized code and folder structure by people who seem like they really know what they're doing. Meanwhile I look at mine, it's a mess. As convenient as it might be to not having keeping everything in a single file, it's not the best practice. So I set on mission to perfectly organize my folder structure.To do that, I went through a bunch of other people's repositories, read a bunch of articles and basically what I learned was there is no one-size-fits-all approach. However in my case, I did end up with a folder structure that I think will make my code somewhat maintainable.In case of my project, I'm doing a full stack javascript application with vuejs at the front-end and nodejs at the backend.Here I'm only focusing on the server side nodejs. The main components are :So this is how you can organize you Nodejs folder structure for your project.Thanks for reading and would would love to hear any insights into this matter.Follow me on Twitter: @Nipeshkc*Visit my blog: blog.arpankc.com"
https://dev.to/heroku/properly-managing-django-media-static-files-on-heroku-o2l,Managing Django Media & Static Files on Heroku with Bucketeer,2022-01-13T18:22:54Z,Heroku,"This article will walk through how we correctly persist static & media files for a Django application hosted on Heroku. As a bonus, it will also explain how we can satisfy the additional constraint of specifying private versus public media files based on model definitions.Before I begin, this post extends from this TestDriven.io article that was written awhile back. I frequent it often when setting up my projects, and have built some extra functionality on top of it over the years. I decided to create a more focused post that references Heroku & Bucketeer with these extra features after helping an individual on StackOverflow.I think it's because I turn off a PC, where I took these imagesThis probably is not it, because Heroku doesn't have access to the files on your computer.When you upload a file to the Django admin, it looks at the DEFAULT_FILE_STORAGE settings configuration to determine how to…So without further ado, let's first dive into what static & media files are and how Heroku dynos manage their filesystem?If you are working with a Django project, then you inevitably have all of your Python application code written around a bunch of .py files. These are the code paths of your application, and the end-user - hopefully - never actually sees these files or their contents.Outside of these business-logic files, it is common to serve users directly from your server's file system. For these static files, Django doesn't need to run any code for them; the framework looks up the file and returns the contents for the requesting user to view.Some examples of static files include:Media files in Django are a particular variant of static files. Media files are read from the server's file system as well. Unlike static files, though, they are usually generated files uploaded by users or generated by your application and are associated with a model's FileField or ImageField. In the examples above, user profile pictures and generated PDFs are typical examples of media files.When a new media file is uploaded to a Django web application, the framework looks at the DEFAULT_FILE_STORAGE settings configuration to determine how to store that file. By default, it uses the django.core.files.storage.FileSystemStorage class, which is what most projects start off as having configured. This implementation looks at the MEDIA_ROOT configuration that is defined in the settings.py file and copies the uploaded file contents to a deterministically-created file path under that given MEDIA_ROOT.For example, if the MEDIA_ROOT is set as /var/www/media, all uploaded files will be copied and written to a location under /var/www/media/.Storing these static files on your server's disk file system is okay until you start to work with a containerization platform such as Heroku. To explain why this is the case, it helps to take a step back.When downloading files on your personal computer, it's okay that these get written to the file system - usually under ~/Downloads or somewhere similar. This download is because you expect your computer's file system to persist across restarts and shutdowns; if you download a file and restart your computer, that downloaded file should still be there once the laptop is finished restarting.Heroku uses containerization to execute customer workloads. One fact of this environment is that the associated file systems do not persist across restarts and reschedules. Heroku dynos are ephemeral, and they can be destroyed, restarted, and moved without any warning, which replaces the associated filesystem. This situation means that any uploaded files referenced by FileField's andImageField's are just deleted without a trace every time the dyno is restarted, moved, or scaled.I will be stepping through the process of configuring the Django application for Heroku & S3-compatible storage, but feel free to reference the repository below for the complete code to browse through.Used in my blog post of detailing private & public static files for a Heroku-served Django application.Note: This does include a $5.00 / month Bucketeer add-on as a part of the one-click deployment.This tutorial aims to help you retrofit an existing Django project with S3-compatible storage, but I'll quickly go through the steps I used to set up the example Django application. It may help those new to Django & Heroku or those who encounter bugs following the rest of the setup process.You can view the tagged project before the storage change at commit 299bbe2.Before we can start managing static and media files, the Django application needs a persistent place to store the files. Again, we can look to Heroku's extensive list of Add-Ons for s3-compatible storage. Ours of choice will be one called Bucketeer.Heroku's Bucketeer add-on provides an AWS S3 storage bucket to upload and download files for our application. The Django application will use this configured bucket to store files uploaded by the server and download them from the S3 when a user requests the files.If you'd like to learn more about AWS S3, the widely-popular data storage solution that Bucketeer is built upon, you can read the S3 user documentation.It is worth mentioning that the base plan for Bucketeer - Hobbyist - is $5 per month. If you plan on spinning up the one-click example posted above, it should only cost a few cents if you proactively destroy the application when you are done using it.To include the Bucketeer add-on in our application, we can configure it through the Heroku CLI, web dashboard, or via the project's app.json file. We will use the third method of including the add-on in an app.json file.If the project does not have one already, we can create the basic structure listed below, with the critical part being the addition of the ""add-ons"" configuration. This array defines the ""bucketeer:hobbyist"" resource that our application will use, and Heroku will install the add-on into our application if it does not already exist. We also include the "" as"" keyword, which will preface the associated configuration variables with the term BUCKETEER. This prefacing is helpful to keep the generated configuration value names deterministic because, by default, Heroku will generate the prefix as a random color.With the required resources being defined, we can start integrating with our storage add-on.The django-storages package is a collection of custom, reuseable storage backends for Django. It aids immensely in saving static and media files to different cloud & storage provider options. One of the supported storage providers is S3, which our Bucketeer add-on is built on. We will leverage the S3 django-storages backend to handle different file types.Begin by installing the django-storages package and the related boto3 package used to interface with AWS's S3. We will also lock our dependencies to ensure poetry and our Heroku deployment continue to work as expected.Then, just like most Django-related packages, django-storages will need to be added to the project's INSTALLED_APPS in the projects settings.py file. This will allow Django to load the appropriate code flows as the application starts up.We will return to the settings.py file later to configure the usage of django-storages, but before that can be done, we will implement three custom storage backends:We can extend from django-storages 's S3Boto3Storage storage backend to create these. The following code can be directly ""copy and paste ""'d into your project. The different settings attributes read in the module will be written shortly, so do not expect this code to work if you import it right now.The attributes listed in each storage backend class perform the following:With our storage backends defined, we can configure them to be used in different situations via the settings.py file. However, it is challenging to use S3 and these different cloud storage backends while in development, and I've always been a proponent of keeping all resources and files ""local"" to the development machine, so we will create a logic path that will:First, we will assume that you have a relatively vanilla settings.py file concerning the static- & media-related variables. For reference, a new project should have a block that looks similar to the following:We will design a slightly advanced control flow that will seamlessly handle the two cases defined above. In addition, it will provide enough control to override each part of the configuration as needed.Since there are already default values for the static file usage, we can add default values for media file usage. These will be used when serving files locally from the server while in development mode.To begin the process of including S3, let's create the controls to manage if we should serve static & media files from the local server or through the S3 storage backend. We will create three variablesIn the example above, we are using the python-decouple package to make it easier to read and cast environment variables to Python variables. I highly recommend this package when working with settings.py configurations. We also include a value check to ensure consistency across these three variables. If all three variables are defined in the environment but conflict with one another, the program will throw an error.We can now start configuring the different configuration variables required by our file storage backends based on those control variables' value(s). We begin by including some S3 configurations required whether we are serving static, media, or both types of files.The above defines some of the variables required by the django-storages S3 backend and sets the values to environment configurations that are provided by the Bucketeer add-on. As previously mentioned, all of the add-on environment variables are prefixed with BUCKETEER_. The S3_SIGNATURE_VERSION environment variable is not required and most likely does not need to be included.With the S3 configuration together, we can reference the LOCAL_SERVE_MEDIA_FILES and LOCAL_SERVE_STATIC_FILES control variables to override the default static and media file settings if they are desired to be served via S3.Notice the last line where STATICFILES_STORAGE is set to the custom Backend we created. That ensures it follows the location & ACL (Access Control List) policies that we configured initially. With this configuration, all static files will be placed under /static/ in the bucket, but feel free to update STATIC_LOCATION if desired.We can configure a very similar situation for media files.The big difference here is that we have configured two different storage backends for media files; one for publicly accessible objects and one for objects that require an access token. When the file is requested, this token will be generated internally by django-storages so you do not have to worry about anonymous public access.Since we will have S3_ENABLED set to False in our local development environment, it will serve static and media files locally through the Django server instead of from S3. We will need to configure the URL routing to handle this scenario. We can configure our urls.py file to serve the appropriate files like so:This will locally serve the static or media files based on the values of the LOCAL_SERVE_STATIC_FILES and LOCAL_SERVE_MEDIA_FILES settings variables we defined.We can enable these storages and our add-on in the app.json file to start using these storage backends. This will effectively disable LOCAL_SERVE_STATIC_FILES and LOCAL_SERVE_MEDIA_FILES to start serving both via S3 when deployed to Heroku.By default, Django will use the PublicMediaStorage class for uploading media files, meaning the contents will be publicly accessible to anyone with the link. However, a model can utilize the PrivateMediaStorage backend when desired, which will create short-lived access tokens that prevent the public from viewing the associated object.The below is an example of using public and private media files on the same model.You can see the code for this complete example at commit 265becc. This configuration will allow your project to scale efficiently using Django on Heroku using Bucketeer.In a future post, we will discuss how to upload and set these files using vanilla Django & Django REST Framework.As always, if you find any bugs, issues, or unclear explanations, please reach out to me so I can improve the tutorial & experience for future readers.Take care everyone"
https://dev.to/heroku/deploying-a-kotlin-app-to-heroku-5d0g,Deploying a Kotlin App to Heroku,2021-10-13T11:24:52Z,Heroku,"Since its earliest release, Java has touted itself as a ""write once, run everywhere"" programming language. The idea was that a programmer could develop an app in Java, have it compiled down to bytecode, and become an executable that can run on any platform, regardless of operating system or platform. It was able to do so in part by a runtime known as the Java Virtual Machine, or JVM.To Java's credit, the JVM was (and still is!) an incredibly fine-tuned runtime that abstracted away a computer's underlying hardware. While Java as a programming language survives to this day, it is often viewed as cumbersome, stodgy, and representative of an outdated approach to implementing solutions.In the last 10 years, more and more languages that run on the JVM have developed, but look and feel nothing like Java. One such language is Kotlin. Because of the JVM, it has no real performance advantages over regular Java. Still, its strength lies in the fact that it prioritizes legibility in a way that Java does not. Consider, for example, printing a substring in Java:You must first get the index of the character you want to be in the substring, add one (since strings are zero-indexed), and call System.out.println to write to stdout.In Kotlin, this is much shorter:Kotlin has garnered so much interest that Google even recommends it over Java for developing Android apps.In this post, we'll take a quick look at how to develop an app in Kotlin. We'll build a simple API with a PostgreSQL database and deploy it to Heroku to see it live.Before we begin, you'll need to make sure you've got the following software installed on your machine:You will also need to be a little familiar with Git and have it installed on your machine.We’re going to be using the IntelliJ IDE for this Kotlin app. Their documentation provides some guidance on how to create a new project. Make sure you select the following options:After the IDE sets everything up, you should have a directory structure that looks roughly like this:Our Kotlin files will be kept in src/main/kotlin, and our build logic will be in build.gradle.kts.Gradle is a build tool for a variety of languages. It also acts as a dependency management tool, similar to Maven. You’ll already have some lines in your build.gradle.kts file, which the IDE automatically added to be helpful. You can delete all of that, and paste in these lines instead:These lines specify our project's dependencies and where to find them. For example, we want to use [org.springframework.boot](https://plugins.gradle.org/plugin/org.springframework.boot) at version 2.4.3, which is why it's defined within the plugins block. We point out the repositories where the packages can be found—at mavenCentral()—and which exposed classes we want to use (implementation( ""org.springframework.boot:spring-boot-starter-web"")).Let's create two small files to test our setup. Create a file called Application.kt in the src/main/kotlin folder and paste in the following:This starts a default app using the Spring framework. The real magic happens in this next file, Controller.kt, which you should create alongside Application.kt in src/main/kotlin:Here, we define a route (@GetMapping(""/{name}"")), where {name} is a dynamic value. By placing this decorator over a Kotlin method (fun get, or ""a function named get""), we're able to match the route to whatever behavior we want—in this case, returning a greeting with the path name for a GET request.In order for the IDE to know how to launch our application, we need to create a run configuration. At the top of the IDE menu, click the button that says Add Configuration…. Select Add new run configuration, then choose Gradle from the list. For the Gradle project name, enter kotlin-api. In the Tasks field, type bootRun. bootRun is a Gradle task provided by the Spring framework which will compile our code and start the server. Click Ok; you should now have a green Play button in your IDE menu bar. When you click on this, the IDE will execute gradle bootRun to build this Kotlin app and start the server. When that finishes, navigate to http://localhost:8080/world. You should see a nice greeting.Now, let's get to the (somewhat) serious stuff. Suppose we wanted to attach a database to this project. In a Maven/Java world, we'd need to update an XML file and add a reference to a JAR file. In Gradle, we can get by with just adding a few lines to our build.gradle.kts file:Here, we've included HikariCP in our project, which is a popular database connection driver. We also indicate that we want to ""load"" the org.postgresql library during runtime. With just these two lines, we've let our configuration know that we want to interact with a PostgreSQL database. If you already have a PostgreSQL database running locally, that's great. You'll be able to continue the rest of this guide locally and see the results when browsing localhost. If you don't have PostgreSQL, don't fret—we'll show you just how easy it is to deploy this app on Heroku, which will take care of the infrastructure for you.Head back to Controller.kt, and replace it with the contents below. This takes some of what we had from before but adds to it. We'll go over the changes shortly.There's quite a lot going on here! Let's start from the bottom. We define a function called dataSource which provides a connection to our database. Because we're building a 12-Factor app, our database credentials are stored in an environment variable called DATABASE_URL. We fetch that URL and pull out the username and password from it if one exists. If not, we check another two environment variables for that information—DATABASE_USERNAME and DATABASE_PASSWORD. We then put all that information together into a format that the database connector needs. The initDb function creates a table called names, with a single text column called name. The /everyone endpoint has a @GetMapping decorator just like before. This defines a GET /everyone route, which gets all the names from the database.Finally, we've added something rather new: a @PostMapping decorator. Here, we need to define what types of content this POST route can accept. In this case, it consumes a TEXT_PLAIN_VALUE media type (in other words, ""Content-Type: text/plain""). Whatever string of information we put in the request body will be added to the database. In just a few lines, we've built a small API that we can add to and query.If you start this server now—and if you have PostgreSQL running locally—you should be able to interact with it. Try making the following request:If you navigate to http://localhost:8080/everyone, you'll see that Frank was included.It's time to see just how easy it is to get Kotlin running on Heroku. First, we need to create a file that's specific to Heroku: the Procfile. This text file defines how our application should boot and run.Create a file named Procfile in the root level directory, right next to your build.gradle.kts file. Copy-paste the following lines into it:Here, we're saying that we want Heroku to run java -jar build/libs/kotlin-api.jar. That JAR is packaged and built during the deployment process; Heroku will create it automatically for us because it knows how to execute the Gradle task to do so. We are also binding the $PORT environment variable so that Heroku knows which port the server is listening to.Next, run the following Git commands:Since we have the Heroku CLI installed, we can call heroku create on the command line to generate an app:Your app will be assigned a random name—in this example, it's desolate-plains-67007—as well as a publicly accessible URL.In order to provision a database, we use the heroku addons command. Calling it without arguments will let us know if any exist:No add-ons exist for our app, which makes sense—we just created it! To add a PostgreSQL database, we can use the addons:create command like this:Heroku offers several tiers of PostgreSQL databases. hobby-dev is the free tier, so we can play around with this without paying a dime.Your code is ready, your Heroku app is configured—you’re ready to deploy. This is the easy part! Just type out the following command:Your code will be pushed to Heroku. From that point on, Heroku will take over. You'll see your build logs scrolling through your terminal. This will show you what Heroku is installing on your behalf and where you are in the build process. After it’s complete, you can visit your special URL in the browser (in this case, https://desolate-plains-67007.herokuapp.com) and interact with the API on the internet!Kotlin's performant design and legible syntax—combined with the ease of Gradle—make it ideal for enterprises that need to rely on battle-tested Java packages. Because of its interoperability with Java, Kotlin is ideal as a transitional language; vast swaths of Java code can be converted into Kotlin while still maintaining a functional app. Deploying to Heroku is smooth, and I didn't even take advantage of the different ways to fine-tune Java and JVM-based apps for deployment."
https://dev.to/heroku/azure-heroku-service-bus-e3p,Azure/Heroku Service Bus,2021-02-04T15:29:33Z,Heroku,"Web applications are optimized for throughput and latency to service a high number of HTTP requests as quickly as possible. For improved performance, web applications defer the CPU intensive, IO intensive, time-intensive, and scheduled processing workloads to background jobs that run independently of the user interface. These background jobs must function without intervention from the user interface and should not block a synchronous user and system interaction. Offloading slow and compute or memory-intensive activity to background jobs improves web applications' performance and throughput.For example, consider an eCommerce web application that captures a customer’s orders and triggers the background jobs to process the orders further. The application’s background jobs work with the operational data (all orders placed by customers) and the contextual data (orders for a single customer) to update the inventory and shipping systems.Heroku supports several queue services as add-ons such as RabbitMQ, Kafka, and IronMQ. However, you are not limited to using add-ons for integrating with cloud queue services. In this example, we will build a background job that processes messages from an Azure Service Bus queue. AWS, Azure, and GCP offer message queues as a service that you can use to extend the capabilities of your Heroku applications.Azure Service Bus offers a rich set of features including support for At-Least-Once and At-Most-Once delivery guarantee. Azure Service Bus also offers First In, First Out (FIFO) messages for both point-to-point (queue) and publish/subscribe communication. While Heroku's application platform is simple, easy to scale, and supports low ceremony DevOps integration, Azure supports an array of enterprise grade services of Azure that can be easily integrated. For complex scenarios, you will find it easy to build applications by integrating the right services across the cloud.Heroku allows you to compose your application from various process types such as web and worker processes. In this demo, we will deploy a simple background worker process that processes messages from a work queue. Heroku allows you to scale the processes in an application independently, which gives you the ability to scale worker instances in proportion to the workload.Apart from the worker, a feature-rich queue is the next crucial component of an event-driven worker process. Azure Service Bus queue service allows consumer processes to lock and process messages independently, enabling you to scale the number of worker dynos and achieve high throughput. Let’s discuss the Azure Service Bus queue service in detail next.The Azure Service Bus service includes a reliable queue service and a durable publish/subscribe messaging service, any of which you can choose based on your needs. Let’s focus on the Azure Service Bus queue service, which offers FIFO message delivery. The message receivers of an Azure Service Bus queue receive the messages in the same sequence in which they were added to the queue by the producer.Service Bus queues act as a buffer between the producer and the consumer of the messages. During the peak load period, the producer can enqueue several additional messages to the queue, which the message consumers can keep processing at the same scale as during an average load period. You can create an Azure Service Bus queue using the Azure CLI and the Azure Portal, among other options. The Azure Service Bus SDK is available in many popular programming languages such as C#, Java, Node, and Go.I will use Go and the Azure Service Bus Go package to build a sample application to demonstrate how we can develop and deploy a background service that reads messages off a work queue, processes them, and prints the results. The following link will take you to the GitHub repository of the application.Source CodeThe application itself is straightforward. It receives messages from the configured Service Bus queue and prints the message body to the console after a small delay. The deliberate processing delay will help me demonstrate that each worker dyno instance receives a different message from the queue and can process the messages independently and thus scale out if required.Start your favorite Go code editor such as VSCode, create a folder for your project, and create a module named sbworker using the following command:To work with Azure Service Bus, let’s install the Azure Service Bus Go package and the Godotenv package to load environment variables from a .env file. The Godotenv package makes it easier to work with applications on development machines and CI servers where several applications might run with each requiring their own set of environment variables. You can read more about this package in the README of its GitHub repository.Create a file named main.go and create the main method in it as follows:Let’s read the code together. The loadEnvFromFileIfExists function loads the environment variables from the .env file present in the folder. We will create and populate the .env file later. Also, remember that this feature is only for our convenience. We will use actual environment variables for configuring our application in Heroku.Next, we instantiated the message handler that will receive and process the messages we receive from the Service Bus queue. We will discuss the message handler in detail later. Since we intend to build a background application, we created a background context with support for cancellation.Next, we fetched the connection string for the Service Bus namespace and the name of the queue from environment variables. We then created a client to communicate with the service bus namespace, and we also created a client to communicate with the queue. A namespace is a container for all messaging components; in this case, the queue.Finally, we started the message receiver with the ReceiveOne function. We handled the particular case of a timeout error, in which case we recurse the loop and reattach the message receiver to the queue. Note that we passed the handler object to the ReceiveOne function, which implements the Handler interface. This interface only requires defining the Handle function that is invoked whenever the receiver can lock a message for processing on the service bus. Let’s define the struct MessageHandler next.The implementation of the function Handle is straightforward. We log the message data, wait five seconds, and mark the message as complete. Note that you must mark a message as complete after processing; otherwise it will reappear on the queue.Finally, let’s define the loadEnvFromFileIfExists function to help us read and load environment variables from a file.Add a file named .env to the folder. We will add the Service Bus connection string and the name of the queue to this file shortly. The last artifact that you need to add to the project is a Procfile. A Heroku Procfile specifies the processes in your application and the commands executed by the applications on startup. Our application is of the worker process type. To start the application, we need to run the command sbworker to launch the module executable generated after Go compiles our application.Let’s spin up an Azure namespace and a queue. I prefer to use the Azure CLI, but you can also use any supported means, such as the Azure portal. The following commands will create a resource group named azsb-heroku-worker-rg, an Azure Service Bus namespace named worker-ns, and a queue named messages in the namespace.Let’s now navigate to the Azure portal to fetch the connection strings of the namespace. Visit the Azure portal quickstart guide for creating Azure Service Bus namespace and queue if you face difficulty navigating through the portal.We will create an access policy that grants only the listen permission (receive messages) to the client. Open the Service Bus namespace that you created and click on Shared access policies. In the next blade, click on the Add button, and on the next panel, provide a name for the policy and select Listen from the list of permissions. Finally, click on the Create button to finish creating the policy.Create listen only policyAfter creating the policy, click on it, and copy the connection string value from the next panel.Copy the connection stringLet’s now apply this value to the .env file that we created earlier as follows:Add a .gitignore file to the project and add the pattern .env to avoid committing this file to the Git repository.You can try running the program on your system with the command go run main.go and debug any errors if the application fails to start. Create a GitHub repository and push the code to it. We will connect this repository to Heroku next.Navigate to the Heroku dashboard and create an app using the Common Runtime as follows:Create app in HerokuAfter creating your app, add the config vars to it, which Heroku will surface as environment variables to our application. Click on the gears icon, and click on the Reveal Config Vars button as follows:Show config varsCreate two config vars SERVICEBUS_CONNECTION_STRING and QUEUE_NAME and set the same value of the variables you set in the .env file earlier.Set config varsIt is now time to connect our GitHub repository to the application. Navigate to the Deployment tab and click on the Connect to GitHub button. You will be asked to log into GitHub and grant access to Heroku to your repositories, which you must accept. Search for your repository and connect it as shown below.Connect app to GitHubIn the expanded panel of the deployment view, select the branch you want to deploy to Heroku and click on the Enable Automatic Deploys button. Any subsequent commit to your repository now will trigger a build and deployment on Heroku.Select the repository branch to deploySince we have already committed our code and do not intend to make any changes to our application, click on the Deploy Branch button to immediately kick off a deployment.Heroku does not automatically create worker dyno instances upon the first deployment. You must use the Heroku CLI or the portal to select the type and the number of dyno instances that you require. Click on the Dynos tab and click on the Edit button, as shown below:Edit dyno configurationIn the dyno edit view, you can select the compute configuration and the instance count of dynos. Set the count of dyno instances to 2 and click the Confirm button.Set dyno instance countIt’s now time to run the application by submitting some messages to it from the Azure portal.Launch the Azure portal in your browser and navigate to the queue that you created in the namespace. Click on the Service Bus Explorer option, which will launch the Service Bus Explorer tool that you can use to send, receive, and peek (see without lock or delete) messages in your queue. Send a few messages to your queue successively after changing the message text. Remember to keep the Content-type to Text/Plain, which is what our receiver expects.Send messages to queueOpen the logs view of your application in the Heroku portal, as shown below:View application logsIn the logs, you can see the two instances processing the messages independently. Also, each receiver instance is independently locking a different message to process, and hence the messages are not duplicating between them.Worker dynos processing the queue messagesInstead of the intentional delay, you can try adding an actual operation to your application and store the result of processing in a persistent data store. You can also try to add a front end to the application that submits messages to the Service Bus, which will convert this simple background job to a complete application.This article presented you with the procedure to integrate Azure Service Bus queues with Heroku worker process to build an event-driven background job. Background services are a critical component of event driven architecture which enables building microservices that are decoupled and iterate independently. Since messages placed on the Azure Service Bus are immutable, they can be treated as the source of truth of business events that can be audited.Thanks to Rahul Rai for his kind permission to publish this article."
https://dev.to/dmfay/ectomigo-safer-schema-migrations-5g6b,ectomigo: Safer Schema Migrations,2022-03-29T14:44:51Z,Dian Fay,"The team I work with at my day job maintains many applications and processes interacting across a smaller number of databases. This is hardly exceptional. We are also constantly adding, subtracting, and refining not only the client programs but also the database schemas themselves. This too is hardly exceptional: business requirements change, external systems expose new information and deprecate old interfaces, von Moltke's Law (""no plan of operations remains certain once the armies have met"") comes calling. Every now and again we just make a modeling or implementation mistake that manages to sneak through review and up to production. Sic semper startups.So our database schemas are continually evolving. Each of those many applications and processes has to evolve along with them, or we get paged when the renamed column or dropped table breaks something we hadn't accounted for, and instant breakage is the best case. We've had schema incompatibilities lie in wait for over a month to catch us completely flatfooted. The complexities of even a single moderately-sized codebase are beyond the grasp of human memory. What hope do we have of recalling which relevant subset of database interactions appear where across two or ten or more?What we need is a distinctly inhuman memory, one for which summoning up each and every reference to a changing table or view takes a moment's effort, and which cannot forget. A memory which operates at the level of the organization, rather than that of the project or of the individual developer/reviewer, only able to focus on a single target at a time. A memory we can consult when, or better yet before, code is ready to deploy -- ""shifting left"", as they say.We need a database.I built one.ectomigo is a continuous integration module (initially a GitHub action) which parses your source files using tree-sitter to find data access code: SQL scripts and inline SQL in Java, JavaScript, and Python; MassiveJS calls; SQLAlchemy definitions; and more languages, data access patterns, analysis features, and platform support on the way after launch. Everything it finds it indexes, storing database object names and the file row-column positions of each reference.When you submit schema changes for review, it parses that code as well, and matches the targets you're altering or dropping against every codebase your organization has enabled. If it does find any matches -- in other words, you still have live references to an affected database object, in this or another repository -- it leaves review comments alerting you to each potential problem.ectomigo is launching on GitHub free for public and up to two private projects, with pricing available beyond that. The action code and the core code analysis library it integrates are open under the AGPL should you be interested in that.We've been using early ectomigo builds at my workplace for a couple of months now, and it's already saved our bacon a few times with reports on database object usage in places we'd forgotten. If you're faced with migration risk yourself, I hope it can help you."
https://dev.to/dmfay/exploring-databases-visually-41j5,Exploring Databases Visually,2021-04-05T01:46:52Z,Dian Fay,"In ""things you can do with a terminal emulator that renders images"":One way to look at a database's structure is as a graph of foreign key relationships among tables. Two styles of visual representation predominate: models or entity-relationship diagrams (ERDs) created as part of requirements negotiation and design, and descriptive diagrams of an extant database. The former are drawn by hand on a whiteboard or in diagramming software; the latter are often generated by database management tools with some manual cleanup and organization. Both styles usually take the complete database as their object, and whether descriptive or prescriptive, their role in the software development process is as reference material, or documentation.Documentation isn't disposable. Even though these diagrams are out of date practically as soon as they're saved off, they take effort to make, or at least to make legible -- automated tools are only so good at layout, especially as table and relationship counts grow. That effort isn't lightly discarded, and anyway a diagram that's still mostly accurate remains a useful reference.Documentation isn't disposable. But documentation isn't the only tool we have for orienting ourselves in a system: we can also explore, view the system in parts and from different angles, follow individual paths through the model from concept to concept. Exploration depends on adopting a partial, mobile perspective from the inside of the model, with rapid feedback and enough context to navigate but not so much as to be overwhelmed. The view from a single point is more or less important depending on the point itself, but in order to facilitate exploration that view has to be generated and discarded on demand. Look, move, look, move.This is a partial perspective of the pagila sample database, from the table film:It's generated by this fks zsh function which queries Postgres' catalog of foreign keys using a recursive common table expression to identify and visualize everything connected in a straight line to the target. The query output is passed to the Graphviz suite's dot with a template, rendered to png, and the png displayed with wezterm imgcat. No files are created or harmed at any point in the process.Why only a straight line, though? The graph above has obvious gaps: film_actor implies an actor, and film_category its own table on the other side of the junction. inventory probably wants a store, and rental and the payment tables aren't much use without a customer. The view from rental is markedly different, with half a dozen tables that weren't visible at all from film:This graph is familiar in part: there's rental itself, the payment tables, inventory, film -- the last shorn of the junctions to the still-missing actor and category tables. Those have passed around a metaphorical corner, since in order to get from rental to film_actor you must travel first up foreign keys into film (via rental.inventory_id and inventory.film_id), then down by way of film_actor.film_id. language, meanwhile, is ""upwards"" of film and therefore remains visible from rental.The reason fks restricts its search to straight lines from the target table is to keep context narrow. You can get a fuller picture of the table structure by navigating and viewing the graph from multiple perspectives; what fks shows is the set of tables which can affect the target, or which will be affected by changes in the target. If you delete a store or a film, rentals from that store or of that film are invalidated (and, unless the intermediary foreign keys are set to cascade, the delete fails). But deleting a film_actor has nothing to do with rental, and vice versa.There's an actual, serious problem with unrestricted traversal, too. If you recurse through all relationships, you wind up mapping entire subgraphs, or clusters of related tables. And clusters grow quickly. Stuart Kauffman has a great illustration of the principle in his book At Home in the Universe: The Search for the Laws of Self-Organization and Complexity.Imagine 10,000 buttons scattered on a hardwood floor. Randomly choose two buttons and connect them with a thread. Now put this pair down and randomly choose two more buttons, pick them up, and connect them with a thread. As you continue to do this, at first you will almost certainly pick up buttons that you have not picked up before. After a while, however, you are more likely to pick at random a pair of buttons and find that you have already chosen one of the pair. So when you tie a thread between the two newly chosen buttons, you will find three buttons tied together. In short, as you continue to choose random pairs of buttons to connect with a thread, after a while the buttons start becoming interconnected into larger clusters.When the ratio of threads to buttons, or relationships to tables, passes 0.5, there's a phase transition. Enough clusters exist that the next thread or relationship will likely connect one cluster to another, and the next, and the next. A supercluster emerges, nearly the size of the entire relationship graph. We can see what the relationship:table ratio looks like in a database by querying the system catalogs:The lowest ratio I have in a real working database is 0.56, and it's a small one, with f=14 and t=25. Others range from 0.61 (f=78, t=126) all the way up to 1.96 (f=2171, t=1107 thanks to a heavily partitioned table with multiple foreign keys); pagila itself is in the middle at 1.08 (f=27, t=25). I don't have enough data to back this up, but I think it's reasonable to expect that the number of relationships tends to increase faster than the number of tables. Without restrictions on traversal, you might as well draw a regular ERD: superclusters are inevitable.fks will draw a regular ERD if passed only the database name, but like I said earlier, automated tools are only so good at layout (and in a terminal of limited width, even a smallish database is liable to produce an illegibly zoomed-out model). With no way to add universal render hints, Graphviz does a lot better with the smaller, more restricted graphs from local perspectives inside the database -- and so do humans. Reading a full-scale data model is hard! Tens or hundreds of nodes have to be sorted by relevance to the problem at hand; nodes and relationships which matter have to be mapped, the irrelevant actively ignored, others tagged with a mental question mark. Often a given problem involves more relevant entities than the human mind can track unaided. fks doesn't resolve the issue completely, but making a database spatial and navigating that space visually goes some way to meet our limitations and those of our tools."
https://dev.to/dmfay/join-semiotics-and-massivejs-v6-44he,JOIN Semiotics and MassiveJS v6,2019-08-13T13:52:56Z,Dian Fay,"MassiveJS version 6 is imminent. This next release closes the widest remaining gap between Massive-generated APIs and everyday SQL, not to mention other higher-level data access libraries: JOINs.This is something of a reversal for Massive, which until now has had very limited functionality for working with multiple database entities at once. I've even written about this as a constraint not without benefits (and, for the record, I think that still -- ad-hoc joins are a tool to be used judiciously in application code!).But the main reason for this lack was always that I'd never come up with any solution that didn't fit awkwardly into an already-awkward options object. Deep insert and resultset decomposition were quite enough to keep track of. I am naturally loath to concede any inherent advantages to constructing models, but this really seemed like one for the longest time.There are, however, ways. Here's what Massive joins look like, if we invade the imaginary privacy of an imaginary library system's imaginary patrons:(relation in this sense indicates a table or view.)And the output:Or in other words, exactly what you'd hope it would look like -- and what, if you use Massive, you may previously have been dealing with a view and decomposition schema to achieve. This is a moderately complex example, and between defaults (e.g. type to INNER) and introspection, declaring a join can be as simple as naming the target: db.libraries.join('books').The join schema is something of an evolution on the decomposition schema, sharing the same structure but inferring column lists, table primary keys, and even some on conditions where unambiguous foreign key relationships exist. It's more concise, less fragile, and still only defined exactly when and where it's needed. Even better, compound entities created from tables can use persistence methods, meaning that join() can replace many if not most existing usages of deep insert and resultset decomposition.It might seem a little unconventional to just invent ersatz database entities out of whole cloth. There's some precedent -- Massive already treats scripts like database functions -- but the compound entities created by Readable.join() are a good bit more complex than that. There's a method to this madness though, and its origins date back to before Ted Codd came up with the idea of the relational database itself.Semiotics is, briefly, the study of meaning-making, with 19th-century roots in both linguistics and formal logic. It's also a sprawling intellectual tradition in dialogue with multifarious other sprawling intellectual traditions, so I am not remotely going to do it justice here. The foundational idea is credited on the linguistics side to Ferdinand de Saussure: meaning is produced in the relation of a signifier to a signified, or taken together a sign. Smoke to fire, letter to sound, and so forth. Everything else proceeds from that relationship. There is, of course, a lot more of that everything else, and like so many other foundational ideas the original Saussurean dyad is something of a museum piece.But the idea of theorizing meaning itself in almost algebraic terms would outlive de Saussure. The logician Charles Sanders Peirce had already come to similar conclusions, and had realized to boot that the interpreted value of the signifier's relationship to its signified is as important as the other two. Peirce, following this line of reasoning, understood this ""interpretant"" itself to be a sign comprising its own signifier and signified which in turn yield their own interpretant, in infinite chains of signification. Louis Hjelmslev, meanwhile, reimagined de Saussure's dyad as a relation of expression to content, and added a second dimension of form and substance. To Hjelmslev, a sign is a function, in the mathematical sense, mapping the ""form of expression"" to the ""form of content"", naming as the ""substance of expression"" and ""substance of content"" the raw materials formed into the sign.The use of the term ""substance"" sounds kind of like some sort of philosophically-détourned jargon, but there are no tricks here: it's just stuff. There's no more specific designation than the likes of ""substance"" for ""that which has been made into a sign""; the category includes everything from physical materials to light, gesture, positioning, electricity, more, in endless combinations. A sign is created by these matters being selected and formed into content and expression: fuel, oxygen, and heat organized into fire and smoke, or sounds uttered in an order corresponding to a known linguistic quantity. It should be said also that consciousness need not enter into it: anything can make a sign, and even a plant can interpret one.This all is to say: there's stuff out there, and what it has in common is that it is made to mean things. Most stuff, in fact, is constantly meaning many things at the same time, as long as there's an interpreting process -- and there's always something. The philosopher-psychologist tag team of Gilles Deleuze and Felix Guattari envisioned the primordial soup of matters-awaiting-further-formation as a spatial dimension: the plane of consistency or plane of immanence. Signification, as they proposed in 1000 Plateaus, happens on and above the plane of consistency, as matters are selected and drawn up from it to become substance and sign. The recursive nature of signification means that these signs are then selected into the substance of yet other signs, becoming layers or strata on the plane in a fashion they compare to the formation of sedimentary rock.A database management system, like any other program, is an immensely complex system of signs. However, what sets DBMSs (and some other categories of software, like ledgers and version control systems) apart is that they're designed to manage other systems of signs. Thanks to this recursive aspect, a database can be imagined as a plane of consistency, a space from which any combination of unformed bytes might be drawn up into column-signs and row-signs which in turn are gathered into table-signs and view-signs and query-signs.And if tables and views and queries are all still signs at base, where exactly do the differences come in? Tables store persistent data and are therefore mutable, while views and queries do not and are not, and must be constituted from tables themselves and (in the case of views) from each other. Tables constitute a lower stratum of signs, with views forming table- and view-substance into signs on higher strata, and queries higher still, at a sufficient remove from the plane of consistency that they're no longer stored in the database itself.This is, of course, arriving at inheritance the long way around. In Massive terms, database entities are first instances of a base Entity class, after which they inherit a second prototype: one of Sequence, Executable, or Readable. Some of the latter may be further articulated as Writables, as well; there are no Writables which are not also Readables.But there's more than one thing happening here, and the ordering of tables, views, and database functions into class-strata is the second step -- matters must be chosen before they can be formed into signs. It's in this first step of stratification that Massive adds script files to the API system of signs, treating them (almost) identically to functions and procedures.Readable.join() takes the same idea further to expand on the database's relations: before, a Readable mapped one-to-one with a single table or view. But as long as SQL can be generated to suit, there's no reason one Readable couldn't map to multiple relations. Writables too, for that matter:The first v6 prerelease is available now: npm i massive@next. There's now a prerelease section of the docs going over what's new and different in detail. But to sum up the other changes:This is a feature I've been wishing I could make happen somehow ever since I first published the original resultset decomposition Gist more than two years ago. It's involved extensive changes to table loading, criteria parsing, and statement generation. I've endeavored not to break these areas, and have informally experimented by dropping pre-prerelease versions into an existing codebase. Results have been good, but should you find an issue with this or any other Massive functionality, please let me know!I'm really excited to see just how far joins expand Massive's capabilities, but in truth there's just one thing I think I and most other Massive users will get the most mileage out of: plain old query predicate generation with criteria objects, without having to define and manage a plethora of views to cover basic JOINs. Stratification is a useful way to think about the production of meaning -- but strata themselves can also be dead weight."
https://dev.to/condenastitaly/when-food-meets-ai-the-smart-recipe-project-21pg,When food meets AI: the Smart Recipe Project,2020-07-27T06:35:35Z,Condé Nast Italy,"Mussel soupIn the previous article, we extracted food entities (ingredients, quantities and units of measurement) from recipes. In this post, we classify the ingredient taxonomic class using the BERT model. In plain words, this means to classify Emmental as cheese, orange as a fruit, peas as a vegetable, and so on for each ingredient in recipes.Since its release in late 2018, BERT has positively changed the way to face NLP tasks, solving many challenging problems in the NLP field. Under this view, one of the main problems in NLP consists of a lack of training data. To cope with this lack, the idea is to exploit a large amount of unannotated data for training general-purpose language representation models, a process known as pre-training, and then fine-tuning these models on a smaller task-specific dataset. Though this technique is not new (see word2vec and GloVE embeddings), we can say, BERT exploits it better. Why? Let’s find it out in five points:To carry out the task, we designed a taxonomy, a model of classification for defining macro-categories and classifying the ingredients within them:Such categorization is then used to tag the dataset that trains the model.For our task (ingredient taxonomic classification), the pre-trained BERT models have optimal performance. We chose the bert-base-multilingual-cased model and divided the classifier into two modules:A training module. We used Bert For Sequence Classification a basic Bert with a single linear layer at the top for classification. Both the pre-trained model and the untrained layer were trained on our data.An applying module. The applier takes the trained model and uses it to determine the taxonomic class of the ingredient in the recipe. You can find a more detailed version of the post on Medium.When Food meets AI: the Smart Recipe Project a series of 6 amazing articlesTable of contentPart 1: Cleaning and manipulating food data Part 1: A smart method for tagging your datasets Part 2: NER for all tastes: extracting information from cooking recipes Part 2: Neither fish nor fowl? Classify it with the Smart Ingredient Classifier Part 3: FoodGraph: a graph database to connect recipes and food data Part 3. FoodGraph: Loading data and Querying the graph with SPARQL"
https://dev.to/condenastitaly/when-food-meets-ai-the-smart-recipe-project-2d6e,When Food meets AI: the Smart Recipe Project,2020-07-20T08:11:59Z,Condé Nast Italy,"In the previous articles, we constructed two label datasets to train machine learning models and develop systems able to interpret cooking recipes.This post dives into the extractor system, a system able to extract ingredients, quantities, time of preparation, and other useful information from recipes. To develop the service, we tried different Named Entity Recognition (NER) approaches.ER is a two-step process consisting of a) identifying entities (a token or a group of tokens) in documents and b) categorizing them into some predetermined categories such as Person, City, Company... For the task, we created our own categories, which are INGREDIENT, QUANTIFIER and UNIT.NER is a very useful NLP application to group and categorize a great amount of data which share similarities and relevance. For this, it can be applied to many business use cases like Human resources, Customer support,Search and recommendation engines,Content classification, and much more.For the Smart Recipe Project, we trained four models: a CRF model, a BiLSTM model, a combination of the previous two (BiLSTM-CRF) and the NER Flair NLP model.Linear-chain Conditional Random Fields - (https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541)[CRFs] - are a very popular way to control sequence prediction. CRFs are discriminative models able to solve some shortcomings of the generative counterpart. Indeed while an HHM output is modeled on the joint probability distribution, a CRF output is computed on the conditional probability distribution.In poor words, while a generative classifier tries to learn how the data was generated, a discriminative one tries to model just observing the dataIn addition to this, CRFs take into account the features of the current and previous labels in sequence. This increases the amount of information the model can rely on to make a good prediction.Fig.1 CRF NetworkFor the task, we used the Stanford NER algorithm, which is an implementation of a CRF classifier. This model outperforms the other models in accuracy, though it cannot understand the context of the forward labels (a pivotal feature for sequential tasks like NER) and requires extra feature engineering.Going neural... we trained a Long Short-Term Memory (LSTM) model. LSTM networks are a type of Recurrent Neural Networks (RNNs), except that the hidden layer updates are replaced by purpose-built memory cells. As a result, they find and exploit better long-range dependencies in the data.To benefit from both past and future context, we used a bidirectional LSTM model (BiLSTM), which processes the text in two directions: both forward (left to right) and backward (right to left). This allows the model to uncover more patterns as the amount of input information increases.Fig.2 BiLSTM architectureMoreover, we incorporated character-based word representation as the input of the model. Character-level representation exploits explicit sub-word-level information, infers features for unseen words and shares information of morpheme-level regularities.This model belongs to the (https://github.com/flairNLP/flair)[Flair NLP library] developed and open-sourced by (https://research.zalando.com/)[Zalando Research]. The strength of the model lies in a) the use of state-of-the-art character, word and context string embeddings (like (https://nlp.stanford.edu/projects/glove/)[GloVe], (https://arxiv.org/abs/1810.04805)[BERT], (https://arxiv.org/pdf/1802.05365.pdf)[ELMo]...), b) the possibility to easier combine these embeddings.In particular, (https://www.aclweb.org/anthology/C18-1139/)[Contextual string embedding] helps to contextualize words producing different embeddings for polysemous words (same words with different meanings):Fig.3 Context String Embedding networkLast but not least, we tried a hybrid approach. We added a layer of CRF to a BiLSTM model. The advantages (well explained here) of such a combo is that this model can efficiently use both 1) past and future input features, thanks to the bidirectional LSTM component, and 2) sentence level tag information, thanks to a CRF layer. The role of the last layer is to impose some other constraints on the final output.Fig. 4 BiLSTM-CRF: general architecture(https://medium.com/@condenastitaly/when-food-meets-ai-the-smart-recipe-project-8dd1f5e727b5)[Read the complete article on medium], to discover that and more about this step of the Smart Recipe Project.When Food meets AI: the Smart Recipe Project a series of 6 amazing articlesTable of contentPart 1: Cleaning and manipulating food data Part 1: A smart method for tagging your datasets Part 2: NER for all tastes: extracting information from cooking recipes Part 2: Neither fish nor fowl? Classify it with the Smart Ingredient Classifier Part 3: FoodGraph: a graph database to connect recipes and food data Part 3. FoodGraph: Loading data and Querying the graph with SPARQL"
https://dev.to/condenastitaly/when-food-meets-ai-the-smart-recipe-project-29bf,When Food meets AI: the Smart Recipe Project,2020-07-13T07:25:28Z,Condé Nast Italy,"A classic TiramisùRaise your hand if you have never come across the “lack of data” problem while working on ML projects.The unavailability or scarcity of training data is indeed one of the most serious challenges in ML and specifically in NLP. A problem that gets harder when the data you need has to be labeled. When no other shortcut works for you, the only alternative is to tag your data... At this point, we imagine the enthusiasm on your face!But don’t put you off! Read the post and discover how we impressively reduced the time and cost of the tagging process.DISCLAIMER: We worked within the food context, but the approach can be easily extended to many different cases.The entities we want to tag are: INGREDIENT: apples, cheese, yogurt, hot peppers… QUANTIFIER: one, 2, ¾, a couple of…. UNIT of measurements: oz, g, lb, liter, cups, tbsp...We used a variant of the IOB schema to tag the entities, where B-, I- tags indicate the beginning and intermediate positions of entities. O is the default tag.We speeded up the ingredient tagging process with TagINGR, a semi-automatic tool which works: 1. matching items in the recipes with those in a list of ingredients; 2. adding the tag INGREDIENT when an item is both in the list and in the recipe.In part 1, the recipe_tagger function tokenizes words and declares some variables:In part 2, it tags the ingredients:Once ingredients were tagged, we can easily tag quantities and units. We first individuated some entity patterns and then tagged them using a set of regex:All very well, but… how did we build the list? what assures us it is complete? what does NN mean in the code? these and other questions will be answered in the medium. Go read it!When Food meets AI: the Smart Recipe Project a series of 6 amazing articlesTable of contentPart 1: Cleaning and manipulating food data Part 1: A smart method for tagging your datasets Part 2: NER for all tastes: extracting information from cooking recipes Part 2: Neither fish nor fowl? Classify it with the Smart Ingredient Classifier Part 3: FoodGraph: a graph database to connect recipes and food data Part 3. FoodGraph: Loading data and Querying the graph with SPARQL"
https://dev.to/neo4j/how-to-build-a-neo4j-application-with-go-1e3o,How to build a Neo4j Application with Go,2022-03-23T14:00:21Z,"Neo4j, Inc.","Are you a Go developer tasked with learning Neo4j, or are you just interested in learning something new?If this sounds like you (or even if it doesn't), then the Building Neo4j Applications with Go course on GraphAcademy is for you!This is a course that I have been working hard on for the past month, and I'm happy to share it with you now.Neo4j GraphAcademy is our free, self-paced, hands-on online learning platform.We on the Neo4j DevRel team have been working hard to build a brand new website and course curriculum that provides a fun, engaging and hands-on learning experience. You can read more about the changes that we have made on the Neo4j Developer blog.In the course, you will learn all about the Neo4j Go Driver by adding it into an existing project, and then modifying a set of services to interact with Neo4j.You will learn all about database sessions, read and write transactions, how to execute a Cypher query and handle results.At the end of the course, the accompanying UI will be populated with data held in a Neo4j Sandbox](https://sandbox.neo4j.com) instance.We assume that you have a prior working knowledge of Go. We also assume that you have some previous experience of working with Neo4j.If you have no prior experience with Neo4j, you can follow the Beginners Learning Path. There are four courses designed to teach you the basics in approximately six hours.The course is split into three modules which will guide you from complete beginner to expert.In the first module, Project Setup, you are guided through setting up the project and registering configuration variables so that the API can communicate with the Neo4j Sandbox instance created during the enrolment process.In the second module, The Neo4j Go Driver, you will learn all about the Neo4j Driver and how it should be used within an application. This included installing the github.com/neo4j/neo4j-go-driver/v4 dependency using go get, building a connection string, creating a Driver instance using the NewDriver() method and verifying that the credentials used to create the driver instance were correct.The third module, Interacting with Neo4j teaches you about the Driver lifecycle; how to open new database sessions, execute read and write transactions and how to consume the results. The module also teaches you how to handle potential errors thrown by the driver. Throughout this module, you will be challenged to modify the existing project and run tests to verify that the code has been written correctly.The final module allows you to practise the skills learnt in the previous three modules by implementing the remaining features.At the end of the course, you will have a working API that serves data to the SPA included in the project. The API will allow you to register a new user and sign in, browse the movie catalogue, rate movies and create a list of favourite movies.The course is free of charge and there is no time limit to complete the course!You can enrol now, for free, by registering for GraphAcademy and clicking Enroll Now on the Building Neo4j Applications with Go page.If you have any comments or feedback you can get in touch with me on Twitter or use the feedback widgets on each page.Good luck!"
https://dev.to/adamcowley/how-to-build-a-neo4j-application-with-nodejs-56h4,How to build a Neo4j Application with Node.js,2021-12-01T12:39:58Z,Adam Cowley,"Are you a Node.js developer tasked with learning Neo4j, or are you just interested in learning something new?If this sounds like you (or even if it doesn't), then the Building Neo4j Applications with Node.js course on GraphAcademy is for you!This is a course that I have been working hard on for the past month, and I'm happy to share it with you now.Neo4j GraphAcademy is our free, self-paced, hands-on online learning platform.We on the Neo4j DevRel team have been working hard to build a brand new website and course curriculum that provides a fun, engaging and hands-on learning experience. You can read more about the changes that we have made on the Neo4j Developer blog.In the course, you will learn all about the Neo4j JavaScript Driver by adding it into an existing project, and then modifying a set of services to interact with Neo4j.You will learn all about database sessions, read and write transactions, how to execute a Cypher query and handle results.At the end of the course, the accompanying UI will be populated with data held in a Neo4j Sandbox](https://sandbox.neo4j.com) instance.We assume that you have a prior working knowledge of JavaScript, NPM and the Node.js ecosystem. We also assume that you have some previous experience of working with Neo4j.If you have no prior experience with Neo4j, you can follow the Beginners Learning Path. There are four courses designed to teach you the basics in approximately six hours.The course is split into three modules which will guide you from complete beginner to expert.In the first module, Project Setup, you are guided through setting up the project and registering environment variables so that the API can communicate with the Neo4j Sandbox instance created during the enrolment process.In the second module, The Neo4j JavaScript Driver, you will learn all about the Neo4j Driver and how it should be used within an application. This included installing the neo4j-driver dependency using npm, building a connection string, creating a Driver instance using the driver() method and verifying that the credentials used to create the driver instance were correct.The third module, Interacting with Neo4j teaches you about the Driver lifecycle; how to open new database sessions, execute read and write transactions and how to consume the results. The module also teaches you how to handle potential errors thrown by the driver. Throughout this module, you will be challenged to modify the existing project and run tests to verify that the code has been written correctly.The final module allows you to practise the skills learnt in the previous three modules by implementing the remaining features.At the end of the course, you will have a working API that serves data to the SPA included in the project. The API will allow you to register a new user and sign in, browse the movie catalogue, rate movies and create a list of favourite movies.The course is free of charge and there is no time limit to complete the course!You can enrol now, for free, by registering for GraphAcademy and clicking Enroll Now on the Building Neo4j Applications with Node.js page.If you have any comments or feedback you can get in touch with me on Twitter or use the feedback widgets on each page.Good luck!"
https://dev.to/adamcowley/how-to-build-an-authentication-into-a-vue3-application-200b,How to Build Authentication into a Vue3 Application,2020-09-28T13:40:45Z,Adam Cowley,"I've recently started a livestream on the Neo4j Twitch Channel about building Web Applications with Neo4j and TypeScript, working on an example project for Neoflix - a fictional streaming service.I've been a long time user of Vue.js, but without proper TypeScript support, I was finding it hard to justify building a Vue-based front end as part of the Stream, after all Vue2's TypeScript support seemed to be lacking. My only real option was Angular, and I got frustrated by that pretty quickly.With Vue v3's official release last week, along with improved TypeScript support, it gave me a good excuse to experiment and see how I could encorporate this into the Neoflix project.One drawback to Vue 2 was the increased complexity as an application grew, the re-use of functionality and readability of components becomes a problem. One example I've seen mentioned a few times is the problem of Sorting results or Pagination. In a Vue2 application, your options were either to duplicate the functionality across components or use a Mixin. The drawback of a Mixin is that it's still not clear what data and methods are bound to a component.The new Composition API allows us to extract repeatable elements into their own files which can be used across components in a more logical way.The new setup function on each component gives you a convenient way to import and reuse functionality. Anything returned from the setup function will be bound to the component. For the search & pagination example, you could write a composition function to perform the specific logic for retrieving search results, while another composition function would provide more generic functionality required to implement previous and next buttons in the UI:Compared to Vue 2's Mixins, the setup function allows you to quickly see which properties and methods are bound to the component without opening multiple files.The official documentation has [a great write up on the Composition API and there is a great Vue Mastery video on the Composition API which explain the problem and solution well.I will assume that you've watched the video and read the docs and will jump straight into a concrete example - Authentication.Authentication is a problem that many apps will have to overcome. A User may be required to provide their login credentials in order to view certain pages on a site or subscribe to access certain features.In the case of Neoflix, Users are required to register and purchase a subscription before they can view or stream the catalogue of Films and TV Shows. A HTTP POST request to /auth/register will create a new account, and a POST request to /auth/login will issue the user with a JWT token which will be passed to each request.As the Users details will be required across multiple components, we will need to save this to the application's global state. On researching the differences between versions 2 and 3, I came across an article that explains that Vuex may not be required for global state management in Vue 3 which will cut down the number of dependencies.This pattern feels a lot like React Hooks where you call a function to create a reference and a setter function, then use reference within the render function.The article provides this code example to explain how it works:You can use the inject function to register a state object using a symbol, then use the provide function to recall the state later on.Or more simply, you can just create a reactive variable and then return it within a function along with any methods required to manipulate the state:The whole use[Something] pattern feels a little React Hook-like, which at the start made me feel a little like ""If I wanted to use Hooks then I could just use React"" - but that thought has faded over time and now it makes sense.In order to interact with API, we will use the axois package.We can create an API instance with some basic config which will be used across the application.Better yet, to avoid duplicating the code required to call the API, we could create a composition function that we could use for all API interactions across the application. To do this we can create a provider function that exposes some useful variables that will be useful to handle loading state inside any component:In order for a component update on the change of a variable, we need to create a reference to a reactive variable. We can do this by importing the ref function. The function takes a single optional argument which is the initial state.For example, when we use this hook, the loading state should be true by default and set to false once the API call succeeds. The data and error variables will be undefined until the request completes.We can then return those variables in an object in order to deconstruct them within the component's setup function.To update these variables, you set .value on the reactive object - for example loading.value = false.We can then create some computed variables to use within the component using the computed function exported from the Vue. For example, if the API returns an error we can use a computed errorMessage property to extract the message or details from the API response.On validation error, Neoflix's Nest.js API returns a 400 Bad Request which includes the individual errors in an array. These can be extracted and converted into an object using Array.reduce:Finally, we can create a method to wrap a GET or POST request and update the reactive variables on success or error:Putting it all together, the function will look like this:Now we have a hook that can be used across the application when we need to send a request to the API.The POST /auth/register endpoint requires an email, password, date of birth and optionally accepts a first name and last name. As we're building a TypeScript application we can define this as an interface which will ensure the code is consistent:In Vue 3, you cann the defineComponent rather than returning a plain Object. In this case, we have one function, setup which uses the composition function to create an API.As part of the setup function, we can call useApi to interact with the API. In this case we want to send a POST request to /auth/register so we can use the useApi function above to extract the variables required in the component.The post method from our useApi hook requires a payload, so we can initialise these in the setup function. Previously, we used the ref function to create individual reactive properties but this can get a little unweildy when deconstructing.Instead, we can use the reactive function exported from vue - this will save us the trouble of calling .value on each property when passing it to the post function. When passing these to the component, we can turn them back into reactive properties using the toRefs function.We can then create a submit method which can be used within component to trigger the request to the API. This will call the post method exported from useApi , which under the hood fires the request and updates error , loading and post .I will omit the entire <template> portion of this query but the variables are used in the same way as a Vue 2 application. For example, the email and password are assigned to inputs using v-model and the submit function can be assigned to the @submit event on the <form> tag.View the component code here...In order to use the user's authentication details across the application, we can create another hook which references a global state object. Again, this is typescript so we should create interfaces to represent the state:The next step is to create an initial state for the module:We can then create a useAuth function which will provide the current state and methods for setting the current user once successfully authenticated or unsetting the user on logout.We can then piece the component together using these functions:The auth module above uses window.localStorage to save the user's access token (AUTH_TOKEN) - if the user returns to the site, we can use that value when the user next visits the site to re-authenticate them.In order to watch for a change of a reactive variable, we can use the watch function. This accepts two arguments; an array of reactive variables and a callback function. We can use this to call the /auth/user endpoint to verify the token. If the API returns a valid response, we should set the user in the global state, otherwise remove the token from local storage.The setup function for the login component is almost identical, except we are calling a different API endpoint:To use the User's information inside a component we can import the same useAuth function and access the user value.For example, we may want to add a personalised welcome message to the top navigation.The user's first name is not required during the Neoflix registration, so we can use the computed function to return a conditional property. If the user has a firstName we will display a Hey, {firstName} message, otherwise fall back to a generic Welcome back! message.We've already added a logout method to the return of useAuth. This can be called from the setup method of a new component to clear the user's information and redirect them back to the login page.In this application, the user should be restricted to the login or register routes unless they are logged in. As we are using vue-router in this application we can use Route Meta Fields to define which routes should be protected:If requiresAuth is set to true, we should check the user provided by useAuth. If the user has not been set, we should return redirect the user to the login page.We can work out whether the user is logged in by accessing the user object returned by useAuth. If the current route's meta data indicates that the route is restricted, we should redirect them to the login page.Conversely, if a user is on the login or register page but has already logged in we should redirect them back to the home page.The more I get used to the new Composition API, the more I like it. It is still early days and the aren't a lot of examples around for Vue 3, so it may emerge at some point that the content in this post is not the best way to do things. If you are doing things differently, let me know in the comments.I will be building out the application as part of my livestream on the Neo4j Twitch Channel. Join me every Tuesday at 13:00 BST, 14:00 CEST or catch up with the videos on the on the Neo4j YouTube Channel.All of the code built during the stream is available on Github."
https://dev.to/javinpaul/how-to-use-redux-for-state-management-in-react-app-2gmg,How to use Redux for state management in React App,2022-05-17T08:15:01Z,javinpaul,"Managing state efficiently is very important in a react application. A React application depends on its state. React provides in-built support to manage the state but as the application grows, this in-built support becomes inefficient.So to manage the state of a large and complex react application, we have to use third-party state management libraries. Redux is by far the most popular state management library. It is heavily used with react and if you are planning to work in a real-time react project, you will encounter redux.It is always complicated for beginners to understand and implement redux. It is a complex library. But it is essential to understand redux properly because its implementation is not easy either. In this article, we will discuss more redux.As mentioned earlier, Redux is a state management library for React. In simple words, it is a store that stores the global state of the application.React provides in-built support for managing the state of a component. It is okay to use the in-built support if the application is small. But with larger and complicated applications that have several components and state is required to be shared among them, we need something powerful like redux.To understand the working of redux, we need to understand the redux flow. Let's understand the redux flow step by step.The state of the entire application resides at one place and that place is called Store. If you are familiar with React, then definitely know what state is.The state controls the behavior of react components. The store provides three powerful methods that are very useful while working with the state. These methods are:The dispatch method is used to dispatch actions (We will discuss actions later) while the getState() method gives us access to the global state of the application. The subscribe method registers listeners. The first two methods are more used in the application because dispatching actions and accessing the state are required frequently.The user interface is not related to redux but it is where a change takes place. The state of the application defines the UI.From the UI, actions are dispatched using the dispatch method. Now, the actions define the change.An action is a plain JavaScript object. It has one mandatory property called ""type"". This type property has information regarding the change. Moreover, an action can also have a ""payload"" property to hold data.A reducer is responsible for changing the state according to the dispatched action. The action will contain its type and optional payload and it is the reducer that defines how the state is going to change.Let's understand the working of redux with the help of a simple example.To install redux, use the following command.Now, we will create a very basic react + redux application to understand the basic concepts. Observe the following code.The UI looks like the following:There are two components - Button and Display. Whenever one of the buttons is clicked, the value in the Display components changes. We will create a global state for this to work.Observe the reducer.The reducer has two parameters - state and action. We have to use the switch loop for iterating over the action and according to the type, the state is updated. Remember, the reducer should only calculate the new state without modifying the already existing state.Next, observe the store.To create a store, we have to use the ""createStore"" method. The first argument of this method is the reducer and the second is the initial state. In our case, the initial state is ""count"".Now, we are ready to dispatch actions.To dispatch an action, we have the dispatch method. But in the Button component, we will use the ""useDispatch"" hook for dispatching actions.We are directly providing the actions inside the onClick handler like this.Remember we have to provide the type while dispatching an action.Now, when the button is clicked, an action is dispatched and according to its type, the reducer will do its work. In the end, the state will be updated in the store.Now, we have to access the state. The best way to access the state is by using yet another hook called useSelector.So now, the store has the initial state (count = 0). When the action is dispatched from the Button component, the reducer updates the state according to the type of action. Then, we can access the state wherever we want.Wrapping it upYou may find redux complicated in the starting but once you understand the redux flow, you can implement redux easily. The main parts of redux are store, action, and reducer. You have to understand their working and then with the help of react hooks, you can implement redux easily in your application."
https://dev.to/javinpaul/understanding-redux-thunk-in-reactjs-with-example-48fd,Understanding Redux Thunk in React.js with Example,2022-04-10T05:56:15Z,javinpaul,"If you are a react developer, then you must have used state in your project. There is no react project without a state. The state is defined as an instance of a component. It is an object that controls the behavior of a component.In this article, we will learn about Redux Thunk, a useful concept when it comes to statement in React.js. State management is one of the most important concepts in react.This is part 3 of my React for Java developer series and in the past two articles we have been focusing on statement management in React and we have seen that how to use manage state using Redux and useState hooks. If you haven't checked them yet then you can check them now to learn React core concepts better.React has in-built support for the state. But as the application grows, managing the state using the in-built features becomes difficult.So in bigger applications, third-party libraries are used to manage the global state. Redux is the most popular and commonly used state management library. But the focus of this article is not redux. It is ""redux-thunk"".Redux-thunk is used to manage the state of a react application. In this article, we will discuss what redux-thunk is and why we should use it.If you're a complete beginner on React then don't worry, have also shared both the best React courses and books, as well as The React Developer RoadMap, which provides all the tools, libraries, and resources you need to become a proficient React Developer. You can go through those resources to learn React.js from scratch.A basic rule of writing reducers in React + Redux application is that it should not have side effects. A side effect is something that can change something which is not in the scope of the function. Making HTTP calls is one example of the side effects.HTTP calls are a side effect but it is an essential part of a web application. We cannot make HTTP calls in a reducer but we can create middleware for it. To handle such situations, we need to create async function middlewares.We can write async function middleware but react already provides them. Redux thunk is nothing but async function middleware.Basically, redux-thunk lets us write functions that can handle async logic such as HTTP calls. In such a function, we can dispatch an action and also access the state of the application. But how? A thunk middleware has two arguments - dispatch and getState.To use redux-thunk, it needs to be installed using the Node Package Manager (NPM).After installing, we need to import ""thunkMiddleware"" from ""redux-thunk"" in the store file. Remember, the store should have the ability to pass the thunk middleware to the dispatch function. For this, we need to use ""applyMiddleware"" like the following.""getData"" is a thunk middleware function. It has two parameters - dispatch and getState. In the last line of the function, dispatch is used to dispatch the required action.So what happened here? The middleware function we created above is handling the asynchronous HTTP call that is providing the data.If we do not have redux-thunk, then where should we write this logic? We cannot write it directly in the reducer and neither inside the action because they cannot have side effects. So we have redux-thunk middleware where we can write async logic.Remember earlier we gave the ability to the store to pass the middleware function to the dispatch function? Observe the following line of code.javascript store.dispatch(getData); `When the above code is executed somewhere in the application, the ""getData"" function is invoked. At this point, no action is dispatched. So it is absolutely fine to make HTTP calls in the ""getData"" function.When the call is made and a response is received, the ""dispatch"" function provided by redux-thunk can dispatch the required action. From here, the normal redux flow can continue to update the state.So this is how redux-thunk is useful in a react application that is using Redux to manage the state. The thunk middleware is neither an action nor a reducer so it can have side effects. Moreover, it provides the dispatch and getState function that let us dispatch actions and access state respectively.That's all about Redux Thunk. If you are going to use redux in your react application, then you should understand the working of redux-thunk. It is a very useful concept that will help you manage side effects efficiently. So go through this article and create your own react application with redux and implement redux-thunk in it to understand the concept more clearly.Other React Tutorials and ResourcesYou May like to exploreThanks for reading this article so far. If you like these free React framework courses, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a comment.P. S. - If you are a complete beginner on React and JavaScript then I also suggest you check out these free JavaScrpt courses first before jumping into React.js. It will save you a lot of time particularly when it comes to reading and understanding code. Once you have done that you can use these free React.js courses to start learning React from scratch."
https://dev.to/javinpaul/what-is-react-lifecycle-methods-and-how-to-use-them-example-4dbm,What is React Lifecycle methods and how to use them? Example,2022-03-19T05:40:08Z,javinpaul,"Hello Devs, if you have been following my blogs then you know that I have launched a new series - React for Java developers and this is the 4th article on the series.Earlier, we have seen state management in React, Redux, and useState hooks example, and in this article, you will learn about how to use lifecycle methods in functional React components.Lifecycle methods are powerful features provided by React.js in the class-based components. Each of the lifecycle methods executes at a particular time during the lifecycle of a component.For example, the componentDidMount method executes only when the component is mounted. Similarly, we have other lifecycle methods that execute at a particular time.Lifecycle methods are very useful but the problem is that they are only available in class components. With the introduction of React hooks, developers are preferring functional components over class components.So this means we cannot use the lifecycle methods in functional components? No, we can use lifecycle methods in functional components with the help of the useEffect() hook.In this article, we will understand how we can achieve functionality similar to lifecycle methods in the functional components using react hooks.By the way, if you are new to the react world then you can also start learning React by exploring these best React Courses and books or you can also go through this React Developer RoadMap to find your path to become a professional React Developer from scratch.The useEffect() in react is used to manage the side effects. It is one of the most commonly used react hooks. The syntax of the useEffect hook is as follows:The useEffect() hook has two arguments - a callback function and a dependency array. The callback function is mandatory while the dependency array is optional.By using this hook, we can create functionality similar to componentDidMount, componentDidUpdate, and componentWillUnmount. If you want to learn more bout React hooks then you can also check out these free React hooks tutorials and courses.The componentDidMount lifecycle method executes when the component is mounted. It means when the component is added to the DOM tree, this method should execute.The following code is equivalent to the componentDidMount method.The empty dependency array means that the hook will execute only once and it will be during the component mounting.The componentDidUpdate method executes only when the component updates.The following code is equivalent to the componentDidUpdate method.Now, the useEffect hook will execute only when the component re-renders. The only difference from the previous example is that no dependency array is provided.The componentDidUpdate method also works differently. We can provide an argument to it and it will execute only when the value of that argument is changed. We can do this in the functional component by adding a value(s) in the dependency array.The useEffect() hook will execute only when the value of ""counter"" is changed.The componentWillUnmount() method executes when the component is unmounted, or removed from the DOM tree.The following code is equivalent to the componentWillUnmount lifecycle method.This time, we have to return a function from the useEffect() hook. This function will execute only when the component is removed from the DOM tree, thus making it similar to the componentWillUnmount lifecycle method.That's all about how to use lifecycle methods in functional React components. As mentioned earlier, the lifecycle methods are very useful in react application development.Even though they are not available in functional components, we can achieve the functionality of componentDidMount, componentDidUpdate, and componentWillUnmount using the useEffect() hook.The useEffect hook will execute only during the component mounting when an empty dependency array is provided. Thus behaving like componentDidMount.The useEffect hook will execute after every re-render when no dependency is provided, meaning no second argument is given to the hook. Thus behaving like componentDidUpdate. If we want to execute the hook when a particular state is changed, we can add that state to the dependency array. So in such cases, the useEffect hook will execute only when that state is changed.If a function is returned from the useEffect hook, it will execute when the component is unmounted, thus behaving like componentWillUnmount.So these are the different ways using which we can use lifecycle methods like functionalities in functional components.Other React and Web Development Articles and resources You May like to exploreThanks for reading this article so far. If you like this React tutorial and useEffect hook example, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a comment.P. S. - If you are a React beginner and looking for free React courses to better learn React fundamental concepts like components, virtual DOM, etc then I also recommend you to join these free REact.js courses for beginners. It contains free courses from Udmey, Coursera, edX to learn to React online for FREE."
https://dev.to/subhransu/nevertheless-subhransu-maharana-coded-5eam,"Dockerizing ReactJS, NodeJS, NGINX using Docker",2020-03-07T16:52:50Z,Subhransu Maharana,"Before starting the project install Docker and Docker-compose. You can install it from here.Now, let's create a simple React application using the create-react-appCreate a file named Dockerfile at the root of the project directory.The first stage is using Node to build the app. we are using Alpine version here as its lightest version. The build would generate a build directory which contains chunk file.Create a .dockerignore file and add node_modules directory to itThis would speed up the process of building images as our local dependencies will not be sent to the Docker daemon.Now time to Build the Docker imageThen, run the container using the Image which we just created.Open http://localhost:3000 in the browser and you should be able to see homepage of the react appNGINX acts a reverse proxy like a middleman between a client making a request to that proxy and that proxy making requests and retrieving its results from other servers.To add nginx as a server to our app we need to create a nginx.conf in the project root folder.Then, add the below lines to the DockerfileHere we are copying the build in the previous step and pasting into the nginx folder and exposing the port 80 - that's going to be the port on which the container will be listening for connections.This produces, production ready imageFinally, the whole Dockerfile should look like this:Docker compose is a basically tool to run multiple container from a single service. It uses a yaml file which contains the configuration to run the containersTo start the containersTo stop the containersWith that , you should be able to add React to a larger Docker powered project for both development and production environments."